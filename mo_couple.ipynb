{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65900bf3-c182-48a6-b6a0-efd08e340f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-26 09:03:04.402196: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-26 09:03:04.402322: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='export TF_INTRA_OP_PARALLELISM_THREADS=12', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mo_descriptor as md\n",
    "import nn_frame as nn\n",
    "import numpy as np\n",
    "import subprocess\n",
    "subprocess.run('export TF_INTRA_OP_PARALLELISM_THREADS=12', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3819da32-6769-405e-8d3f-1fa9c336c9b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 start clustering\n",
      "2 start getting center\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgg95223/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 start clustering\n",
      "2 start getting center\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgg95223/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "prepare data_set\n",
    "1. make mo_pair descriptor\n",
    "'''\n",
    "x_shift = np.arange(0, 4.1, 0.1)\n",
    "y_shift = np.arange(0, 4.1, 0.1)\n",
    "z_shift = np.zeros(x_shift.shape)\n",
    "# the original mo, e.g. homo\n",
    "homo = md.MO_descriptor('data/homo-s0.cube').make()\n",
    "lumo = md.MO_descriptor('data/lumo-s0.cube').make()\n",
    "\n",
    "# for the original pair of one mo and itself\n",
    "homo_pair = md.MO_pair_descriptor(homo, homo).make()\n",
    "lumo_pair = md.MO_pair_descriptor(lumo, lumo).make()\n",
    "\n",
    "homo_pairs = np.zeros((len(x_shift)*len(y_shift),) + homo_pair.shape)\n",
    "lumo_pairs = np.zeros((len(x_shift)*len(y_shift),) + lumo_pair.shape)\n",
    "\n",
    "homo_ = np.zeros(homo.shape)\n",
    "\n",
    "for ii, i in enumerate(x_shift):\n",
    "    for jj, j in enumerate(y_shift):\n",
    "        idx = ii * len(y_shift) + jj\n",
    "        homo_[:,0] = np.add(homo[:,0],0)\n",
    "        homo_[:,1] = np.add(homo[:,1],i)\n",
    "        homo_[:,2] = np.add(homo[:,2],j)\n",
    "        homo_[:,3] = np.add(homo[:,3],0)\n",
    "        \n",
    "        homo_pair_ = md.MO_pair_descriptor(homo, homo_).make()\n",
    "        homo_pairs[idx] = homo_pair_\n",
    "\n",
    "        \n",
    "# def dir_mat(mat):\n",
    "#     mat_shape = mat.shape\n",
    "#     mat_ = mat.flatten()\n",
    "#     for ii, i in enumerate(mat_):\n",
    "#         if i > 1e-6:\n",
    "#             mat_[ii] = 1\n",
    "#         elif (i < 1e-6) and (i > -1e-6):\n",
    "#             mat_[ii] = -1\n",
    "#         elif i < -1e-6:\n",
    "#             mat_[ii] = -1\n",
    "#     return mat_.reshape(mat_shape)\n",
    "\n",
    "# direct = dir_mat(homo_pair)\n",
    "\n",
    "# # for the shifted pair\n",
    "# homo_pairs = np.zeros((len(x_shift)*len(y_shift),) + homo_pair.shape)\n",
    "# lumo_pairs = np.zeros((len(x_shift)*len(y_shift),) + lumo_pair.shape)\n",
    "# for ii, i in enumerate(x_shift):\n",
    "#     for jj, j in enumerate(y_shift):\n",
    "#         idx = ii * len(y_shift) + jj\n",
    "#         homo_pairs[idx][0] = homo_pair[0]\n",
    "#         homo_pairs[idx][1] = np.add(homo_pair[1],i*direct[1])\n",
    "#         homo_pairs[idx][2] = np.add(homo_pair[2],j*direct[2])\n",
    "#         homo_pairs[idx][3] = homo_pair[3]\n",
    "#         lumo_pairs[idx][0] = lumo_pair[0]\n",
    "#         lumo_pairs[idx][1] = np.add(lumo_pair[1],i)\n",
    "#         lumo_pairs[idx][2] = np.add(lumo_pair[2],j)\n",
    "#         lumo_pairs[idx][3] = lumo_pair[3]\n",
    "# np.save('homo_homo_pair.npy', homo_pairs)\n",
    "# np.save('lumo_lumo_pair.npy', lumo_pairs)\n",
    "# homo_pairs = np.load('homo_homo_pair.npy')\n",
    "# lumo_pairs = np.load('lumo_lumo_pair.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b17be6b-d152-4b5d-9405-dec3f617c082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1      -0.1      -7.167328 -7.167328 -0.1      -0.1      -7.167328\n",
      "  -7.167328]\n",
      " [-0.1      -0.1      -7.167328 -7.167328 -0.1      -0.1      -7.167328\n",
      "  -7.167328]\n",
      " [ 6.967328  6.967328 -0.1      -0.1       6.967328  6.967328 -0.1\n",
      "  -0.1     ]\n",
      " [ 6.967328  6.967328 -0.1      -0.1       6.967328  6.967328 -0.1\n",
      "  -0.1     ]\n",
      " [-0.1      -0.1      -7.167328 -7.167328 -0.1      -0.1      -7.167328\n",
      "  -7.167328]\n",
      " [-0.1      -0.1      -7.167328 -7.167328 -0.1      -0.1      -7.167328\n",
      "  -7.167328]\n",
      " [ 6.967328  6.967328 -0.1      -0.1       6.967328  6.967328 -0.1\n",
      "  -0.1     ]\n",
      " [ 6.967328  6.967328 -0.1      -0.1       6.967328  6.967328 -0.1\n",
      "  -0.1     ]]\n"
     ]
    }
   ],
   "source": [
    "print(homo_pairs[:,1,:,:][81])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91a92a2-f076-46c8-835f-958569393703",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. read coupling\n",
    "'''\n",
    "raw_data = np.loadtxt('data/cdft-V1V2.dat')\n",
    "c_homo = np.add(raw_data[:,2], raw_data[:,3]) * 1/2\n",
    "c_lumo = np.add(raw_data[:,4], raw_data[:,5]) * 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda7ef17-1e21-42a4-9daf-27feba4fc51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x1 = homo_pairs[:,0,:,:]\n",
    "# x2 = homo_pairs[:,1,:,:]\n",
    "# x3 = homo_pairs[:,2,:,:]\n",
    "# x4 = homo_pairs[:,3,:,:]\n",
    "# x = np.einsum('aij,aij,aij,aij->aij', x1, x2, x3, x4)\n",
    "\n",
    "train_homo_pairs = homo_pairs\n",
    "train_lumo_pairs = lumo_pairs[0:1200]\n",
    "\n",
    "train_c_homo = c_homo\n",
    "train_c_lumo = c_lumo[0:1200]\n",
    "\n",
    "test_homo_pairs = homo_pairs[1200:]\n",
    "test_lumo_pairs = lumo_pairs[1200:]\n",
    "\n",
    "test_c_homo = c_homo[1200:].reshape((len(c_homo[1200:]),1))\n",
    "test_c_lumo = c_lumo[1200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "938d30ba-e044-469f-b2eb-25f05497f346",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step:     0, loss:  2.492963075638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-26 09:03:27.879459: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-08-26 09:03:27.879580: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Deng-PC): /proc/driver/nvidia/version does not exist\n",
      "2022-08-26 09:03:27.880044: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step:    41, loss:  1.396472334862\n",
      "training step:    82, loss:  0.697279512882\n",
      "training step:   123, loss:  0.541781008244\n",
      "training step:   164, loss:  0.173969164491\n",
      "training step:   205, loss:  0.153603047132\n",
      "training step:   246, loss:  0.550437569618\n",
      "training step:   287, loss:  0.159436628222\n",
      "training step:   328, loss:  0.108560398221\n",
      "training step:   369, loss:  0.070697344840\n",
      "training step:   410, loss:  0.025775521994\n",
      "training step:   451, loss:  0.038534946740\n",
      "training step:   492, loss:  0.010736959055\n",
      "training step:   533, loss:  0.023971691728\n",
      "training step:   574, loss:  0.037656389177\n",
      "training step:   615, loss:  0.028809087351\n",
      "training step:   656, loss:  0.014072754420\n",
      "training step:   697, loss:  0.007926405407\n",
      "training step:   738, loss:  0.013543140143\n",
      "training step:   779, loss:  0.030931644142\n",
      "training step:   820, loss:  0.019215432927\n",
      "training step:   861, loss:  0.008409325965\n",
      "training step:   902, loss:  0.021738750860\n",
      "training step:   943, loss:  0.008324794471\n",
      "training step:   984, loss:  0.020165499300\n",
      "training step:  1025, loss:  0.029291560873\n",
      "training step:  1066, loss:  0.009644601494\n",
      "training step:  1107, loss:  0.021376725286\n",
      "training step:  1148, loss:  0.009604038671\n",
      "training step:  1189, loss:  0.006341312081\n",
      "training step:  1230, loss:  0.008670369163\n",
      "training step:  1271, loss:  0.006402195431\n",
      "training step:  1312, loss:  0.007589928806\n",
      "training step:  1353, loss:  0.006411135197\n",
      "training step:  1394, loss:  0.008754519746\n",
      "training step:  1435, loss:  0.010286962613\n",
      "training step:  1476, loss:  0.007369308732\n",
      "training step:  1517, loss:  0.006864382885\n",
      "training step:  1558, loss:  0.008959391154\n",
      "training step:  1599, loss:  0.008488427848\n",
      "training step:  1640, loss:  0.011054964736\n",
      "training step:  1681, loss:  0.007195463870\n",
      "training step:  1722, loss:  0.006875019986\n",
      "training step:  1763, loss:  0.007789035328\n",
      "training step:  1804, loss:  0.006792085711\n",
      "training step:  1845, loss:  0.005784634035\n",
      "training step:  1886, loss:  0.007379246410\n",
      "training step:  1927, loss:  0.010616322048\n",
      "training step:  1968, loss:  0.007975553162\n",
      "training step:  2009, loss:  0.005334507208\n",
      "training step:  2050, loss:  0.007671953645\n",
      "training step:  2091, loss:  0.018430314958\n",
      "training step:  2132, loss:  0.007333545480\n",
      "training step:  2173, loss:  0.009335686453\n",
      "training step:  2214, loss:  0.006570480298\n",
      "training step:  2255, loss:  0.010741199367\n",
      "training step:  2296, loss:  0.006218062714\n",
      "training step:  2337, loss:  0.015378856100\n",
      "training step:  2378, loss:  0.007911712863\n",
      "training step:  2419, loss:  0.004402840976\n",
      "training step:  2460, loss:  0.004694720730\n",
      "training step:  2501, loss:  0.010860376060\n",
      "training step:  2542, loss:  0.011637393385\n",
      "training step:  2583, loss:  0.025272317231\n",
      "training step:  2624, loss:  0.004600183107\n",
      "training step:  2665, loss:  0.004975659307\n",
      "training step:  2706, loss:  0.006147488020\n",
      "training step:  2747, loss:  0.006005439907\n",
      "training step:  2788, loss:  0.013323275372\n",
      "training step:  2829, loss:  0.007186692674\n",
      "training step:  2870, loss:  0.005889439024\n",
      "training step:  2911, loss:  0.006480144337\n",
      "training step:  2952, loss:  0.021205857396\n",
      "training step:  2993, loss:  0.005065044388\n",
      "training step:  3034, loss:  0.008269659244\n",
      "training step:  3075, loss:  0.008093744516\n",
      "training step:  3116, loss:  0.004966652486\n",
      "training step:  3157, loss:  0.005006310996\n",
      "training step:  3198, loss:  0.004098157864\n",
      "training step:  3239, loss:  0.005223644432\n",
      "training step:  3280, loss:  0.005239143036\n",
      "training step:  3321, loss:  0.005158520769\n",
      "training step:  3362, loss:  0.024799736217\n",
      "training step:  3403, loss:  0.005032230634\n",
      "training step:  3444, loss:  0.015146828257\n",
      "training step:  3485, loss:  0.021665411070\n",
      "training step:  3526, loss:  0.005101554561\n",
      "training step:  3567, loss:  0.003756985534\n",
      "training step:  3608, loss:  0.026055637747\n",
      "training step:  3649, loss:  0.003843685146\n",
      "training step:  3690, loss:  0.003723510075\n",
      "training step:  3731, loss:  0.003262770595\n",
      "training step:  3772, loss:  0.004791458603\n",
      "training step:  3813, loss:  0.012818342075\n",
      "training step:  3854, loss:  0.008994029835\n",
      "training step:  3895, loss:  0.004250791389\n",
      "training step:  3936, loss:  0.004071386997\n",
      "training step:  3977, loss:  0.008405392990\n",
      "training step:  4018, loss:  0.006306959316\n",
      "training step:  4059, loss:  0.004887012299\n",
      "training step:  4100, loss:  0.003738525324\n",
      "training step:  4141, loss:  0.003038250376\n",
      "training step:  4182, loss:  0.003911883105\n",
      "training step:  4223, loss:  0.008050682954\n",
      "training step:  4264, loss:  0.009812204167\n",
      "training step:  4305, loss:  0.007077748422\n",
      "training step:  4346, loss:  0.006041665096\n",
      "training step:  4387, loss:  0.023284148425\n",
      "training step:  4428, loss:  0.011874740943\n",
      "training step:  4469, loss:  0.005041141529\n",
      "training step:  4510, loss:  0.003310922766\n",
      "training step:  4551, loss:  0.004206552636\n",
      "training step:  4592, loss:  0.009385146201\n",
      "training step:  4633, loss:  0.006357266568\n",
      "training step:  4674, loss:  0.004544866737\n",
      "training step:  4715, loss:  0.008183888160\n",
      "training step:  4756, loss:  0.003124193754\n",
      "training step:  4797, loss:  0.003613817273\n",
      "training step:  4838, loss:  0.016117401421\n",
      "training step:  4879, loss:  0.004346349277\n",
      "training step:  4920, loss:  0.003783816937\n",
      "training step:  4961, loss:  0.002903073560\n",
      "training step:  5002, loss:  0.017582908273\n",
      "training step:  5043, loss:  0.013019878417\n",
      "training step:  5084, loss:  0.004379902966\n",
      "training step:  5125, loss:  0.004410292488\n",
      "training step:  5166, loss:  0.004335616715\n",
      "training step:  5207, loss:  0.015992183238\n",
      "training step:  5248, loss:  0.008626624942\n",
      "training step:  5289, loss:  0.004485258367\n",
      "training step:  5330, loss:  0.003320749849\n",
      "training step:  5371, loss:  0.004571076017\n",
      "training step:  5412, loss:  0.006384037435\n",
      "training step:  5453, loss:  0.005499640480\n",
      "training step:  5494, loss:  0.003174730111\n",
      "training step:  5535, loss:  0.004104914609\n",
      "training step:  5576, loss:  0.003633734770\n",
      "training step:  5617, loss:  0.008056681603\n",
      "training step:  5658, loss:  0.007017695345\n",
      "training step:  5699, loss:  0.004367580172\n",
      "training step:  5740, loss:  0.003458571155\n",
      "training step:  5781, loss:  0.003203500062\n",
      "training step:  5822, loss:  0.014317537658\n",
      "training step:  5863, loss:  0.025100035593\n",
      "training step:  5904, loss:  0.017271086574\n",
      "training step:  5945, loss:  0.023355083540\n",
      "training step:  5986, loss:  0.008510759100\n",
      "training step:  6027, loss:  0.009423636831\n",
      "training step:  6068, loss:  0.008247525431\n",
      "training step:  6109, loss:  0.003643331118\n",
      "training step:  6150, loss:  0.008726012893\n",
      "training step:  6191, loss:  0.014491543174\n",
      "training step:  6232, loss:  0.003124276409\n",
      "training step:  6273, loss:  0.009708367288\n",
      "training step:  6314, loss:  0.026235848665\n",
      "training step:  6355, loss:  0.003326838603\n",
      "training step:  6396, loss:  0.022387553006\n",
      "training step:  6437, loss:  0.003989243880\n",
      "training step:  6478, loss:  0.004907944705\n",
      "training step:  6519, loss:  0.002785181627\n",
      "training step:  6560, loss:  0.010024204850\n",
      "training step:  6601, loss:  0.003725014394\n",
      "training step:  6642, loss:  0.020094577223\n",
      "training step:  6683, loss:  0.007361187600\n",
      "training step:  6724, loss:  0.003251684597\n",
      "training step:  6765, loss:  0.002761935350\n",
      "training step:  6806, loss:  0.025466578081\n",
      "training step:  6847, loss:  0.003576449119\n",
      "training step:  6888, loss:  0.022501727566\n",
      "training step:  6929, loss:  0.004583506845\n",
      "training step:  6970, loss:  0.007758441381\n",
      "training step:  7011, loss:  0.002426232444\n",
      "training step:  7052, loss:  0.007583954837\n",
      "training step:  7093, loss:  0.003469388233\n",
      "training step:  7134, loss:  0.007727042306\n",
      "training step:  7175, loss:  0.003754277714\n",
      "training step:  7216, loss:  0.024867935106\n",
      "training step:  7257, loss:  0.026669112965\n",
      "training step:  7298, loss:  0.003166040173\n",
      "training step:  7339, loss:  0.005690266844\n",
      "training step:  7380, loss:  0.004203120247\n",
      "training step:  7421, loss:  0.025943046436\n",
      "training step:  7462, loss:  0.003934369422\n",
      "training step:  7503, loss:  0.025078173727\n",
      "training step:  7544, loss:  0.002301634522\n",
      "training step:  7585, loss:  0.003912007902\n",
      "training step:  7626, loss:  0.003835337237\n",
      "training step:  7667, loss:  0.023327929899\n",
      "training step:  7708, loss:  0.007521169726\n",
      "training step:  7749, loss:  0.021541196853\n",
      "training step:  7790, loss:  0.021286729723\n",
      "training step:  7831, loss:  0.022742923349\n",
      "training step:  7872, loss:  0.003932059743\n",
      "training step:  7913, loss:  0.003138463246\n",
      "training step:  7954, loss:  0.005809789989\n",
      "training step:  7995, loss:  0.004018315580\n",
      "training step:  8036, loss:  0.025712454692\n",
      "training step:  8077, loss:  0.008049599826\n",
      "training step:  8118, loss:  0.003883713624\n",
      "training step:  8159, loss:  0.025229744613\n",
      "training step:  8200, loss:  0.002470996464\n",
      "training step:  8241, loss:  0.002423069207\n",
      "training step:  8282, loss:  0.003076804103\n",
      "training step:  8323, loss:  0.007962844335\n",
      "training step:  8364, loss:  0.003019253025\n",
      "training step:  8405, loss:  0.022844584659\n",
      "training step:  8446, loss:  0.005020448007\n",
      "training step:  8487, loss:  0.009725515731\n",
      "training step:  8528, loss:  0.016649855301\n",
      "training step:  8569, loss:  0.025706287473\n",
      "training step:  8610, loss:  0.003008160973\n",
      "training step:  8651, loss:  0.005842845421\n",
      "training step:  8692, loss:  0.003169029485\n",
      "training step:  8733, loss:  0.003968078177\n",
      "training step:  8774, loss:  0.002296050079\n",
      "training step:  8815, loss:  0.003943139687\n",
      "training step:  8856, loss:  0.016662029549\n",
      "training step:  8897, loss:  0.003970291931\n",
      "training step:  8938, loss:  0.003863111837\n",
      "training step:  8979, loss:  0.004372742027\n",
      "training step:  9020, loss:  0.003148555756\n",
      "training step:  9061, loss:  0.003776168684\n",
      "training step:  9102, loss:  0.003747740062\n",
      "training step:  9143, loss:  0.003309606109\n",
      "training step:  9184, loss:  0.014121070504\n",
      "training step:  9225, loss:  0.007212801836\n",
      "training step:  9266, loss:  0.003769608913\n",
      "training step:  9307, loss:  0.016577504575\n",
      "training step:  9348, loss:  0.003417781321\n",
      "training step:  9389, loss:  0.002974724630\n",
      "training step:  9430, loss:  0.003373584012\n",
      "training step:  9471, loss:  0.022162256762\n",
      "training step:  9512, loss:  0.003846342675\n",
      "training step:  9553, loss:  0.009559960105\n",
      "training step:  9594, loss:  0.014391241595\n",
      "training step:  9635, loss:  0.016870746389\n",
      "training step:  9676, loss:  0.003511245362\n",
      "training step:  9717, loss:  0.003754963167\n",
      "training step:  9758, loss:  0.020368076861\n",
      "training step:  9799, loss:  0.008947098628\n",
      "training step:  9840, loss:  0.003917047754\n",
      "training step:  9881, loss:  0.003291894682\n",
      "training step:  9922, loss:  0.024766026065\n",
      "training step:  9963, loss:  0.007954895496\n",
      "training step: 10004, loss:  0.022403271869\n",
      "training step: 10045, loss:  0.022539718077\n",
      "training step: 10086, loss:  0.025379078463\n",
      "training step: 10127, loss:  0.004036528524\n",
      "training step: 10168, loss:  0.006331895012\n",
      "training step: 10209, loss:  0.002924323780\n",
      "training step: 10250, loss:  0.003163858783\n",
      "training step: 10291, loss:  0.020595580339\n",
      "training step: 10332, loss:  0.003897037823\n",
      "training step: 10373, loss:  0.021472994238\n",
      "training step: 10414, loss:  0.026433821768\n",
      "training step: 10455, loss:  0.003274502233\n",
      "training step: 10496, loss:  0.002984055551\n",
      "training step: 10537, loss:  0.008575599641\n",
      "training step: 10578, loss:  0.004804035649\n",
      "training step: 10619, loss:  0.005545540247\n",
      "training step: 10660, loss:  0.003927063663\n",
      "training step: 10701, loss:  0.002912986092\n",
      "training step: 10742, loss:  0.005729617085\n",
      "training step: 10783, loss:  0.006603276357\n",
      "training step: 10824, loss:  0.005014643073\n",
      "training step: 10865, loss:  0.003058987670\n",
      "training step: 10906, loss:  0.003888932290\n",
      "training step: 10947, loss:  0.003905887250\n",
      "training step: 10988, loss:  0.004825173412\n",
      "training step: 11029, loss:  0.009583796374\n",
      "training step: 11070, loss:  0.002401175909\n",
      "training step: 11111, loss:  0.003040154465\n",
      "training step: 11152, loss:  0.003562819213\n",
      "training step: 11193, loss:  0.003417909844\n",
      "training step: 11234, loss:  0.003827004461\n",
      "training step: 11275, loss:  0.005608928390\n",
      "training step: 11316, loss:  0.025892853737\n",
      "training step: 11357, loss:  0.009078304283\n",
      "training step: 11398, loss:  0.002966843313\n",
      "training step: 11439, loss:  0.003918101545\n",
      "training step: 11480, loss:  0.003910113592\n",
      "training step: 11521, loss:  0.005594341550\n",
      "training step: 11562, loss:  0.025667030364\n",
      "training step: 11603, loss:  0.016636064276\n",
      "training step: 11644, loss:  0.003658818780\n",
      "training step: 11685, loss:  0.003571416251\n",
      "training step: 11726, loss:  0.009616190568\n",
      "training step: 11767, loss:  0.005618278868\n",
      "training step: 11808, loss:  0.003127192846\n",
      "training step: 11849, loss:  0.005614943337\n",
      "training step: 11890, loss:  0.023830356076\n",
      "training step: 11931, loss:  0.004005169496\n",
      "training step: 11972, loss:  0.011832353659\n",
      "training step: 12013, loss:  0.009563967586\n",
      "training step: 12054, loss:  0.024887273088\n",
      "training step: 12095, loss:  0.005546732340\n",
      "training step: 12136, loss:  0.008893646300\n",
      "training step: 12177, loss:  0.003382754512\n",
      "training step: 12218, loss:  0.004015883431\n",
      "training step: 12259, loss:  0.009506006725\n",
      "training step: 12300, loss:  0.003431691322\n",
      "training step: 12341, loss:  0.003559955396\n",
      "training step: 12382, loss:  0.003552182345\n",
      "training step: 12423, loss:  0.023975806311\n",
      "training step: 12464, loss:  0.020972235128\n",
      "training step: 12505, loss:  0.002332747914\n",
      "training step: 12546, loss:  0.003708120203\n",
      "training step: 12587, loss:  0.003921579104\n",
      "training step: 12628, loss:  0.003551237052\n",
      "training step: 12669, loss:  0.003915216308\n",
      "training step: 12710, loss:  0.003469941206\n",
      "training step: 12751, loss:  0.003392206039\n",
      "training step: 12792, loss:  0.007658420596\n",
      "training step: 12833, loss:  0.020738083869\n",
      "training step: 12874, loss:  0.002936706413\n",
      "training step: 12915, loss:  0.011903819628\n",
      "training step: 12956, loss:  0.020527087152\n",
      "training step: 12997, loss:  0.002384803025\n",
      "training step: 13038, loss:  0.003814408788\n",
      "training step: 13079, loss:  0.003908772953\n",
      "training step: 13120, loss:  0.003128573764\n",
      "training step: 13161, loss:  0.022334024310\n",
      "training step: 13202, loss:  0.020729400218\n",
      "training step: 13243, loss:  0.003092259867\n",
      "training step: 13284, loss:  0.003085325472\n",
      "training step: 13325, loss:  0.022488281131\n",
      "training step: 13366, loss:  0.004011541605\n",
      "training step: 13407, loss:  0.004854258150\n",
      "training step: 13448, loss:  0.002962261671\n",
      "training step: 13489, loss:  0.003280860605\n",
      "training step: 13530, loss:  0.003122392576\n",
      "training step: 13571, loss:  0.018792714924\n",
      "training step: 13612, loss:  0.023799246177\n",
      "training step: 13653, loss:  0.018769459799\n",
      "training step: 13694, loss:  0.009495412000\n",
      "training step: 13735, loss:  0.014136597514\n",
      "training step: 13776, loss:  0.025539891794\n",
      "training step: 13817, loss:  0.002369373804\n",
      "training step: 13858, loss:  0.002904579043\n",
      "training step: 13899, loss:  0.003459992120\n",
      "training step: 13940, loss:  0.003783338936\n",
      "training step: 13981, loss:  0.005516833626\n",
      "training step: 14022, loss:  0.002385903616\n",
      "training step: 14063, loss:  0.014147356153\n",
      "training step: 14104, loss:  0.025302886963\n",
      "training step: 14145, loss:  0.025575825945\n",
      "training step: 14186, loss:  0.003986500204\n",
      "training step: 14227, loss:  0.003992460668\n",
      "training step: 14268, loss:  0.023871198297\n",
      "training step: 14309, loss:  0.002314712619\n",
      "training step: 14350, loss:  0.003061804920\n",
      "training step: 14391, loss:  0.003099549562\n",
      "training step: 14432, loss:  0.004025600385\n",
      "training step: 14473, loss:  0.016534702852\n",
      "training step: 14514, loss:  0.003780684434\n",
      "training step: 14555, loss:  0.009047572501\n",
      "training step: 14596, loss:  0.005787036382\n",
      "training step: 14637, loss:  0.003698983463\n",
      "training step: 14678, loss:  0.002930169227\n",
      "training step: 14719, loss:  0.003814929165\n",
      "training step: 14760, loss:  0.003780688392\n",
      "training step: 14801, loss:  0.006159281358\n",
      "training step: 14842, loss:  0.014232011512\n",
      "training step: 14883, loss:  0.024933943525\n",
      "training step: 14924, loss:  0.023882206529\n",
      "training step: 14965, loss:  0.007370713167\n",
      "training step: 15006, loss:  0.011796691455\n",
      "training step: 15047, loss:  0.007887433283\n",
      "training step: 15088, loss:  0.003883830272\n",
      "training step: 15129, loss:  0.003698308719\n",
      "training step: 15170, loss:  0.003117747139\n",
      "training step: 15211, loss:  0.003007827327\n",
      "training step: 15252, loss:  0.018738314509\n",
      "training step: 15293, loss:  0.004967449233\n",
      "training step: 15334, loss:  0.008991654962\n",
      "training step: 15375, loss:  0.014136372134\n",
      "training step: 15416, loss:  0.007384686265\n",
      "training step: 15457, loss:  0.003996109124\n",
      "training step: 15498, loss:  0.002929048613\n",
      "training step: 15539, loss:  0.005661965348\n",
      "training step: 15580, loss:  0.003779342165\n",
      "training step: 15621, loss:  0.020834416151\n",
      "training step: 15662, loss:  0.003545449581\n",
      "training step: 15703, loss:  0.003465096699\n",
      "training step: 15744, loss:  0.002360587474\n",
      "training step: 15785, loss:  0.020769432187\n",
      "training step: 15826, loss:  0.002377868863\n",
      "training step: 15867, loss:  0.003904447658\n",
      "training step: 15908, loss:  0.003809717018\n",
      "training step: 15949, loss:  0.023715971038\n",
      "training step: 15990, loss:  0.007755487226\n",
      "training step: 16031, loss:  0.025328511372\n",
      "training step: 16072, loss:  0.003463869682\n",
      "training step: 16113, loss:  0.006290590391\n",
      "training step: 16154, loss:  0.009042009711\n",
      "training step: 16195, loss:  0.008940341882\n",
      "training step: 16236, loss:  0.009492167272\n",
      "training step: 16277, loss:  0.014149492607\n",
      "training step: 16318, loss:  0.003275134601\n",
      "training step: 16359, loss:  0.003278489923\n",
      "training step: 16400, loss:  0.009495466948\n",
      "training step: 16441, loss:  0.002928518690\n",
      "training step: 16482, loss:  0.002382610459\n",
      "training step: 16523, loss:  0.003778372891\n",
      "training step: 16564, loss:  0.003881513607\n",
      "training step: 16605, loss:  0.014126404189\n",
      "training step: 16646, loss:  0.023763138801\n",
      "training step: 16687, loss:  0.014171170071\n",
      "training step: 16728, loss:  0.004044362810\n",
      "training step: 16769, loss:  0.003695544321\n",
      "training step: 16810, loss:  0.011783368886\n",
      "training step: 16851, loss:  0.003809761722\n",
      "training step: 16892, loss:  0.003544640262\n",
      "training step: 16933, loss:  0.003545905463\n",
      "training step: 16974, loss:  0.011786079034\n",
      "training step: 17015, loss:  0.023747019470\n",
      "training step: 17056, loss:  0.020702492446\n",
      "training step: 17097, loss:  0.003545411397\n",
      "training step: 17138, loss:  0.022406104952\n",
      "training step: 17179, loss:  0.003477899125\n",
      "training step: 17220, loss:  0.002929379931\n",
      "training step: 17261, loss:  0.003123799106\n",
      "training step: 17302, loss:  0.003284484148\n",
      "training step: 17343, loss:  0.003003893420\n",
      "training step: 17384, loss:  0.022408481687\n",
      "training step: 17425, loss:  0.024749284610\n",
      "training step: 17466, loss:  0.007779293694\n",
      "training step: 17507, loss:  0.007873539813\n",
      "training step: 17548, loss:  0.003986362834\n",
      "training step: 17589, loss:  0.003279067343\n",
      "training step: 17630, loss:  0.003281532554\n",
      "training step: 17671, loss:  0.011815166101\n",
      "training step: 17712, loss:  0.002957627876\n",
      "training step: 17753, loss:  0.016532633454\n",
      "training step: 17794, loss:  0.009491839446\n",
      "training step: 17835, loss:  0.025379823521\n",
      "training step: 17876, loss:  0.003881092416\n",
      "training step: 17917, loss:  0.005492100958\n",
      "training step: 17958, loss:  0.014158238657\n",
      "training step: 17999, loss:  0.016517568380\n",
      "training step: 18040, loss:  0.009506423958\n",
      "training step: 18081, loss:  0.022480683401\n",
      "training step: 18122, loss:  0.003905202961\n",
      "training step: 18163, loss:  0.007833805867\n",
      "training step: 18204, loss:  0.003115821863\n",
      "training step: 18245, loss:  0.024767834693\n",
      "training step: 18286, loss:  0.003988886252\n",
      "training step: 18327, loss:  0.006288275588\n",
      "training step: 18368, loss:  0.002984888386\n",
      "training step: 18409, loss:  0.018673973158\n",
      "training step: 18450, loss:  0.005693077110\n",
      "training step: 18491, loss:  0.003000039607\n",
      "training step: 18532, loss:  0.007725314237\n",
      "training step: 18573, loss:  0.002384341555\n",
      "training step: 18614, loss:  0.003124466864\n",
      "training step: 18655, loss:  0.009481262416\n",
      "training step: 18696, loss:  0.003903106553\n",
      "training step: 18737, loss:  0.005678032525\n",
      "training step: 18778, loss:  0.025402767584\n",
      "training step: 18819, loss:  0.014158067293\n",
      "training step: 18860, loss:  0.007841974497\n",
      "training step: 18901, loss:  0.002930973656\n",
      "training step: 18942, loss:  0.002930111019\n",
      "training step: 18983, loss:  0.007856158540\n",
      "training step: 19024, loss:  0.004997308832\n",
      "training step: 19065, loss:  0.025562815368\n",
      "training step: 19106, loss:  0.003882083111\n",
      "training step: 19147, loss:  0.018693119287\n",
      "training step: 19188, loss:  0.003114266088\n",
      "training step: 19229, loss:  0.005700625945\n",
      "training step: 19270, loss:  0.016493681818\n",
      "training step: 19311, loss:  0.005493339617\n",
      "training step: 19352, loss:  0.005706434604\n",
      "training step: 19393, loss:  0.007340753917\n",
      "training step: 19434, loss:  0.022377016023\n",
      "training step: 19475, loss:  0.003136589425\n",
      "training step: 19516, loss:  0.007347458974\n",
      "training step: 19557, loss:  0.018700826913\n",
      "training step: 19598, loss:  0.016475627199\n",
      "training step: 19639, loss:  0.016488131136\n",
      "training step: 19680, loss:  0.007753585000\n",
      "training step: 19721, loss:  0.002979677869\n",
      "training step: 19762, loss:  0.009000374004\n",
      "training step: 19803, loss:  0.003138926812\n",
      "training step: 19844, loss:  0.003808736801\n",
      "training step: 19885, loss:  0.003880299395\n",
      "training step: 19926, loss:  0.003879886586\n",
      "training step: 19967, loss:  0.025330502540\n",
      "training step: 20008, loss:  0.004088725429\n",
      "training step: 20049, loss:  0.025351256132\n",
      "training step: 20090, loss:  0.004988976289\n",
      "training step: 20131, loss:  0.006276092958\n",
      "training step: 20172, loss:  0.003543897299\n",
      "training step: 20213, loss:  0.022413708270\n",
      "training step: 20254, loss:  0.004079963546\n",
      "training step: 20295, loss:  0.002930289600\n",
      "training step: 20336, loss:  0.003282933030\n",
      "training step: 20377, loss:  0.003693603212\n",
      "training step: 20418, loss:  0.004989950918\n",
      "training step: 20459, loss:  0.003638394643\n",
      "training step: 20500, loss:  0.007342399564\n",
      "training step: 20541, loss:  0.014147415757\n",
      "training step: 20582, loss:  0.018707929179\n",
      "training step: 20623, loss:  0.003989129793\n",
      "training step: 20664, loss:  0.004072580487\n",
      "training step: 20705, loss:  0.025577338412\n",
      "training step: 20746, loss:  0.003808674403\n",
      "training step: 20787, loss:  0.004997203127\n",
      "training step: 20828, loss:  0.003776684403\n",
      "training step: 20869, loss:  0.005695482716\n",
      "training step: 20910, loss:  0.003281901823\n",
      "training step: 20951, loss:  0.009481861256\n",
      "training step: 20992, loss:  0.003987724893\n",
      "training step: 21033, loss:  0.011788073927\n",
      "training step: 21074, loss:  0.003467794508\n",
      "training step: 21115, loss:  0.003809079994\n",
      "training step: 21156, loss:  0.005489664152\n",
      "training step: 21197, loss:  0.003000450786\n",
      "training step: 21238, loss:  0.002390592126\n",
      "training step: 21279, loss:  0.025370888412\n",
      "training step: 21320, loss:  0.005486447830\n",
      "training step: 21361, loss:  0.003118887078\n",
      "training step: 21402, loss:  0.003002278507\n",
      "training step: 21443, loss:  0.006290610414\n",
      "training step: 21484, loss:  0.003776990343\n",
      "training step: 21525, loss:  0.004999463912\n",
      "training step: 21566, loss:  0.003466215450\n",
      "training step: 21607, loss:  0.002930029761\n",
      "training step: 21648, loss:  0.003693393897\n",
      "training step: 21689, loss:  0.002970271278\n",
      "training step: 21730, loss:  0.022399516776\n",
      "training step: 21771, loss:  0.006278213114\n",
      "training step: 21812, loss:  0.023761652410\n",
      "training step: 21853, loss:  0.022392785177\n",
      "training step: 21894, loss:  0.003283543512\n",
      "training step: 21935, loss:  0.009000526741\n",
      "training step: 21976, loss:  0.020700324327\n",
      "training step: 22017, loss:  0.003283841303\n",
      "training step: 22058, loss:  0.020701708272\n",
      "training step: 22099, loss:  0.025381704792\n",
      "training step: 22140, loss:  0.003638480790\n",
      "training step: 22181, loss:  0.007875557058\n",
      "training step: 22222, loss:  0.002387026791\n",
      "training step: 22263, loss:  0.003776744707\n",
      "training step: 22304, loss:  0.007344377693\n",
      "training step: 22345, loss:  0.003693070030\n",
      "training step: 22386, loss:  0.004080238286\n",
      "training step: 22427, loss:  0.009468759410\n",
      "training step: 22468, loss:  0.003122466384\n",
      "training step: 22509, loss:  0.003638855647\n",
      "training step: 22550, loss:  0.018703145906\n",
      "training step: 22591, loss:  0.003120251233\n",
      "training step: 22632, loss:  0.011776976287\n",
      "training step: 22673, loss:  0.007340281270\n",
      "training step: 22714, loss:  0.002930380171\n",
      "training step: 22755, loss:  0.005713854916\n",
      "training step: 22796, loss:  0.002976536984\n",
      "training step: 22837, loss:  0.003476502607\n",
      "training step: 22878, loss:  0.005484079011\n",
      "training step: 22919, loss:  0.003692730796\n",
      "training step: 22960, loss:  0.003986350726\n",
      "training step: 23001, loss:  0.016485584900\n",
      "training step: 23042, loss:  0.003986408934\n",
      "training step: 23083, loss:  0.007342018187\n",
      "training step: 23124, loss:  0.005485851318\n",
      "training step: 23165, loss:  0.003776715370\n",
      "training step: 23206, loss:  0.003467087634\n",
      "training step: 23247, loss:  0.002930196235\n",
      "training step: 23288, loss:  0.011780114844\n",
      "training step: 23329, loss:  0.008991863579\n",
      "training step: 23370, loss:  0.003283337457\n",
      "training step: 23411, loss:  0.014156299643\n",
      "training step: 23452, loss:  0.005488418508\n",
      "training step: 23493, loss:  0.009477106854\n",
      "training step: 23534, loss:  0.003119558096\n",
      "training step: 23575, loss:  0.005486198701\n",
      "training step: 23616, loss:  0.003986577038\n",
      "training step: 23657, loss:  0.003134570550\n",
      "training step: 23698, loss:  0.005714405794\n",
      "training step: 23739, loss:  0.004999147728\n",
      "training step: 23780, loss:  0.002392421244\n",
      "training step: 23821, loss:  0.003131753765\n",
      "training step: 23862, loss:  0.025576421991\n",
      "training step: 23903, loss:  0.003543975065\n",
      "training step: 23944, loss:  0.018700357527\n",
      "training step: 23985, loss:  0.003880424425\n",
      "training step: 24026, loss:  0.003692934522\n",
      "training step: 24067, loss:  0.003543945728\n",
      "training step: 24108, loss:  0.003866119077\n",
      "training step: 24149, loss:  0.005005539395\n",
      "training step: 24190, loss:  0.003808819922\n",
      "training step: 24231, loss:  0.005000507925\n",
      "training step: 24272, loss:  0.003880519653\n",
      "training step: 24313, loss:  0.003466638736\n",
      "training step: 24354, loss:  0.009475667030\n",
      "training step: 24395, loss:  0.020695317537\n",
      "training step: 24436, loss:  0.002385467058\n",
      "training step: 24477, loss:  0.003987635951\n",
      "training step: 24518, loss:  0.002929919632\n",
      "training step: 24559, loss:  0.003808768699\n",
      "training step: 24600, loss:  0.014151176438\n",
      "training step: 24641, loss:  0.003987286240\n",
      "training step: 24682, loss:  0.006279366091\n",
      "training step: 24723, loss:  0.003466787748\n",
      "training step: 24764, loss:  0.007342936005\n",
      "training step: 24805, loss:  0.003638697555\n",
      "training step: 24846, loss:  0.003284274368\n",
      "training step: 24887, loss:  0.003987187985\n",
      "training step: 24928, loss:  0.003543955507\n",
      "training step: 24969, loss:  0.003001031466\n",
      "training step: 25010, loss:  0.025572191924\n",
      "training step: 25051, loss:  0.002974902513\n",
      "training step: 25092, loss:  0.004086981062\n",
      "training step: 25133, loss:  0.003471901873\n",
      "training step: 25174, loss:  0.002386072185\n",
      "training step: 25215, loss:  0.023760646582\n",
      "training step: 25256, loss:  0.018699763343\n",
      "training step: 25297, loss:  0.003473138204\n",
      "training step: 25338, loss:  0.003880604636\n",
      "training step: 25379, loss:  0.003866222687\n",
      "training step: 25420, loss:  0.003880636767\n",
      "training step: 25461, loss:  0.007750260178\n",
      "training step: 25502, loss:  0.005486499052\n",
      "training step: 25543, loss:  0.006284750998\n",
      "training step: 25584, loss:  0.003808692563\n",
      "training step: 25625, loss:  0.002385873115\n",
      "training step: 25666, loss:  0.004087060690\n",
      "training step: 25707, loss:  0.023755818605\n",
      "training step: 25748, loss:  0.005714847706\n",
      "training step: 25789, loss:  0.024754177779\n",
      "training step: 25830, loss:  0.004088566639\n",
      "training step: 25871, loss:  0.003003772115\n",
      "training step: 25912, loss:  0.003284438280\n",
      "training step: 25953, loss:  0.002975619864\n",
      "training step: 25994, loss:  0.009472964332\n",
      "training step: 26035, loss:  0.003120016074\n",
      "training step: 26076, loss:  0.007874016650\n",
      "training step: 26117, loss:  0.007342300843\n",
      "training step: 26158, loss:  0.003471129574\n",
      "training step: 26199, loss:  0.006282439921\n",
      "training step: 26240, loss:  0.009002113715\n",
      "training step: 26281, loss:  0.003692963393\n",
      "training step: 26322, loss:  0.002974299481\n",
      "training step: 26363, loss:  0.004086529836\n",
      "training step: 26404, loss:  0.003638638649\n",
      "training step: 26445, loss:  0.020694017410\n",
      "training step: 26486, loss:  0.003693052568\n",
      "training step: 26527, loss:  0.003808820853\n",
      "training step: 26568, loss:  0.003133866936\n",
      "training step: 26609, loss:  0.003638572758\n",
      "training step: 26650, loss:  0.003543937812\n",
      "training step: 26691, loss:  0.007343113422\n",
      "training step: 26732, loss:  0.009475000203\n",
      "training step: 26773, loss:  0.018701018766\n",
      "training step: 26814, loss:  0.024756666273\n",
      "training step: 26855, loss:  0.003901019227\n",
      "training step: 26896, loss:  0.014147803187\n",
      "training step: 26937, loss:  0.003880571341\n",
      "training step: 26978, loss:  0.003467077157\n",
      "training step: 27019, loss:  0.016487088054\n",
      "training step: 27060, loss:  0.002395902527\n",
      "training step: 27101, loss:  0.003776648780\n",
      "training step: 27142, loss:  0.002385495231\n",
      "training step: 27183, loss:  0.007342458703\n",
      "training step: 27224, loss:  0.020693276078\n",
      "training step: 27265, loss:  0.005002636928\n",
      "training step: 27306, loss:  0.025367699564\n",
      "training step: 27347, loss:  0.005000960082\n",
      "training step: 27388, loss:  0.003866179613\n",
      "training step: 27429, loss:  0.002385776257\n",
      "training step: 27470, loss:  0.002974873642\n",
      "training step: 27511, loss:  0.024755932391\n",
      "training step: 27552, loss:  0.003880616743\n",
      "training step: 27593, loss:  0.003119764850\n",
      "training step: 27634, loss:  0.025366047397\n",
      "training step: 27675, loss:  0.003880624427\n",
      "training step: 27716, loss:  0.022395636886\n",
      "training step: 27757, loss:  0.016487987712\n",
      "training step: 27798, loss:  0.003986984026\n",
      "training step: 27839, loss:  0.003134627827\n",
      "training step: 27880, loss:  0.003466879018\n",
      "training step: 27921, loss:  0.011777847074\n",
      "training step: 27962, loss:  0.003776643658\n",
      "training step: 28003, loss:  0.003472963581\n",
      "training step: 28044, loss:  0.003120332956\n",
      "training step: 28085, loss:  0.003136194078\n",
      "training step: 28126, loss:  0.003467105562\n",
      "training step: 28167, loss:  0.002930190414\n",
      "training step: 28208, loss:  0.003986645956\n",
      "training step: 28249, loss:  0.004087508190\n",
      "training step: 28290, loss:  0.009002859704\n",
      "training step: 28331, loss:  0.005715210922\n",
      "training step: 28372, loss:  0.003986804746\n",
      "training step: 28413, loss:  0.023756749928\n",
      "training step: 28454, loss:  0.009473577142\n",
      "training step: 28495, loss:  0.003900963813\n",
      "training step: 28536, loss:  0.007751235738\n",
      "training step: 28577, loss:  0.003986903932\n",
      "training step: 28618, loss:  0.002386162989\n",
      "training step: 28659, loss:  0.002396257827\n",
      "training step: 28700, loss:  0.003986797761\n",
      "training step: 28741, loss:  0.003880507778\n",
      "training step: 28782, loss:  0.003866105806\n",
      "training step: 28823, loss:  0.022393846884\n",
      "training step: 28864, loss:  0.003284273436\n",
      "training step: 28905, loss:  0.003776644822\n",
      "training step: 28946, loss:  0.003466927446\n",
      "training step: 28987, loss:  0.016487540677\n",
      "training step: 29028, loss:  0.007342491765\n",
      "training step: 29069, loss:  0.002395309974\n",
      "training step: 29110, loss:  0.016487760469\n",
      "training step: 29151, loss:  0.003472398734\n",
      "training step: 29192, loss:  0.023760326207\n",
      "training step: 29233, loss:  0.002974665025\n",
      "training step: 29274, loss:  0.004079027567\n",
      "training step: 29315, loss:  0.002930053743\n",
      "training step: 29356, loss:  0.003638688708\n",
      "training step: 29397, loss:  0.011777833104\n",
      "training step: 29438, loss:  0.025365492329\n",
      "training step: 29479, loss:  0.004085538443\n",
      "training step: 29520, loss:  0.003638698254\n",
      "training step: 29561, loss:  0.003135230392\n",
      "training step: 29602, loss:  0.002385959728\n",
      "training step: 29643, loss:  0.005714345258\n",
      "training step: 29684, loss:  0.005002603866\n",
      "training step: 29725, loss:  0.009003230371\n",
      "training step: 29766, loss:  0.025363977998\n",
      "training step: 29807, loss:  0.004079759121\n",
      "training step: 29848, loss:  0.003880514530\n",
      "training step: 29889, loss:  0.020690968260\n",
      "training step: 29930, loss:  0.002930164570\n",
      "training step: 29971, loss:  0.002930171089\n",
      "training step: 30012, loss:  0.024754829705\n",
      "training step: 30053, loss:  0.004087196663\n",
      "training step: 30094, loss:  0.024756193161\n",
      "training step: 30135, loss:  0.002385901986\n",
      "training step: 30176, loss:  0.007750673220\n",
      "training step: 30217, loss:  0.003638719209\n",
      "training step: 30258, loss:  0.005002769642\n",
      "training step: 30299, loss:  0.005486110691\n",
      "training step: 30340, loss:  0.004086880945\n",
      "training step: 30381, loss:  0.007751713973\n",
      "training step: 30422, loss:  0.003002624260\n",
      "training step: 30463, loss:  0.003880531294\n",
      "training step: 30504, loss:  0.002395881107\n",
      "training step: 30545, loss:  0.009003255516\n",
      "training step: 30586, loss:  0.002395925345\n",
      "training step: 30627, loss:  0.009003307670\n",
      "training step: 30668, loss:  0.002975348849\n",
      "training step: 30709, loss:  0.023757940158\n",
      "training step: 30750, loss:  0.004087060224\n",
      "training step: 30791, loss:  0.003284384497\n",
      "training step: 30832, loss:  0.003284385661\n",
      "training step: 30873, loss:  0.009003869258\n",
      "training step: 30914, loss:  0.003002608428\n",
      "training step: 30955, loss:  0.003467044095\n",
      "training step: 30996, loss:  0.003543945728\n",
      "training step: 31037, loss:  0.009473643266\n",
      "training step: 31078, loss:  0.009003619663\n",
      "training step: 31119, loss:  0.003543902189\n",
      "training step: 31160, loss:  0.003472995013\n",
      "training step: 31201, loss:  0.003986832686\n",
      "training step: 31242, loss:  0.004087273031\n",
      "training step: 31283, loss:  0.025568392128\n",
      "training step: 31324, loss:  0.003011459019\n",
      "training step: 31365, loss:  0.003120360663\n",
      "training step: 31406, loss:  0.016485700384\n",
      "training step: 31447, loss:  0.025362713262\n",
      "training step: 31488, loss:  0.003543926170\n",
      "training step: 31529, loss:  0.002975942800\n",
      "training step: 31570, loss:  0.025567766279\n",
      "training step: 31611, loss:  0.004080227111\n",
      "training step: 31652, loss:  0.003136054380\n",
      "training step: 31693, loss:  0.003473426448\n",
      "training step: 31734, loss:  0.003808662761\n",
      "training step: 31775, loss:  0.004087434616\n",
      "training step: 31816, loss:  0.004087573849\n",
      "training step: 31857, loss:  0.007752680220\n",
      "training step: 31898, loss:  0.023756720126\n",
      "training step: 31939, loss:  0.024754108861\n",
      "training step: 31980, loss:  0.025567511097\n",
      "training step: 32021, loss:  0.016485475004\n",
      "training step: 32062, loss:  0.003002799582\n",
      "training step: 32103, loss:  0.020691318437\n",
      "training step: 32144, loss:  0.005003535189\n",
      "training step: 32185, loss:  0.003986840136\n",
      "training step: 32226, loss:  0.002975489013\n",
      "training step: 32267, loss:  0.007751860190\n",
      "training step: 32308, loss:  0.002386191161\n",
      "training step: 32349, loss:  0.024755358696\n",
      "training step: 32390, loss:  0.003002532059\n",
      "training step: 32431, loss:  0.002930135699\n",
      "training step: 32472, loss:  0.003472995479\n",
      "training step: 32513, loss:  0.003986845724\n",
      "training step: 32554, loss:  0.024755109102\n",
      "training step: 32595, loss:  0.014146475121\n",
      "training step: 32636, loss:  0.020691828802\n",
      "training step: 32677, loss:  0.011776745319\n",
      "training step: 32718, loss:  0.003120245412\n",
      "training step: 32759, loss:  0.005003266968\n",
      "training step: 32800, loss:  0.023757759482\n",
      "training step: 32841, loss:  0.003543909639\n",
      "training step: 32882, loss:  0.005003328901\n",
      "training step: 32923, loss:  0.005003337283\n",
      "training step: 32964, loss:  0.003472959390\n",
      "training step: 33005, loss:  0.011776902713\n",
      "training step: 33046, loss:  0.014146434143\n",
      "training step: 33087, loss:  0.004086991772\n",
      "training step: 33128, loss:  0.005715509877\n",
      "training step: 33169, loss:  0.003135631094\n",
      "training step: 33210, loss:  0.003880517557\n",
      "training step: 33251, loss:  0.003120218404\n",
      "training step: 33292, loss:  0.003638756694\n",
      "training step: 33333, loss:  0.003473083721\n",
      "training step: 33374, loss:  0.020691616461\n",
      "training step: 33415, loss:  0.003135733772\n",
      "training step: 33456, loss:  0.003866124898\n",
      "training step: 33497, loss:  0.003543955972\n",
      "training step: 33538, loss:  0.003866124898\n",
      "training step: 33579, loss:  0.007751445752\n",
      "training step: 33620, loss:  0.005003003869\n",
      "training step: 33661, loss:  0.020691854879\n",
      "training step: 33702, loss:  0.006284629926\n",
      "training step: 33743, loss:  0.003808709094\n",
      "training step: 33784, loss:  0.011776907369\n",
      "training step: 33825, loss:  0.002386154840\n",
      "training step: 33866, loss:  0.003638742957\n",
      "training step: 33907, loss:  0.003808685811\n",
      "training step: 33948, loss:  0.003543915460\n",
      "training step: 33989, loss:  0.003011308610\n",
      "training step: 34030, loss:  0.002396057593\n",
      "training step: 34071, loss:  0.009473483078\n",
      "training step: 34112, loss:  0.003002535785\n",
      "training step: 34153, loss:  0.004086930770\n",
      "training step: 34194, loss:  0.002386176493\n",
      "training step: 34235, loss:  0.005003183614\n",
      "training step: 34276, loss:  0.005003171973\n",
      "training step: 34317, loss:  0.006284886040\n",
      "training step: 34358, loss:  0.003120240988\n",
      "training step: 34399, loss:  0.007877095602\n",
      "training step: 34440, loss:  0.025363614783\n",
      "training step: 34481, loss:  0.003011360532\n",
      "training step: 34522, loss:  0.007341563702\n",
      "training step: 34563, loss:  0.002975500887\n",
      "training step: 34604, loss:  0.022393025458\n",
      "training step: 34645, loss:  0.022392999381\n",
      "training step: 34686, loss:  0.003284370061\n",
      "training step: 34727, loss:  0.024755198509\n",
      "training step: 34768, loss:  0.007877130993\n",
      "training step: 34809, loss:  0.003808690701\n",
      "training step: 34850, loss:  0.003284398466\n",
      "training step: 34891, loss:  0.003866114886\n",
      "training step: 34932, loss:  0.003284401260\n",
      "training step: 34973, loss:  0.007341475692\n",
      "training step: 35014, loss:  0.002975644078\n",
      "training step: 35055, loss:  0.025363430381\n",
      "training step: 35096, loss:  0.003808685811\n",
      "training step: 35137, loss:  0.003986843396\n",
      "training step: 35178, loss:  0.003473053919\n",
      "training step: 35219, loss:  0.002386170439\n",
      "training step: 35260, loss:  0.024755245075\n",
      "training step: 35301, loss:  0.003692974336\n",
      "training step: 35342, loss:  0.009003818035\n",
      "training step: 35383, loss:  0.025363620371\n",
      "training step: 35424, loss:  0.006284862757\n",
      "training step: 35465, loss:  0.003880518954\n",
      "training step: 35506, loss:  0.003776642494\n",
      "training step: 35547, loss:  0.002386201872\n",
      "training step: 35588, loss:  0.004079836886\n",
      "training step: 35629, loss:  0.003135784063\n",
      "training step: 35670, loss:  0.003120244015\n",
      "training step: 35711, loss:  0.003135784063\n",
      "training step: 35752, loss:  0.002396132564\n",
      "training step: 35793, loss:  0.022392889485\n",
      "training step: 35834, loss:  0.006285007577\n",
      "training step: 35875, loss:  0.016486091539\n",
      "training step: 35916, loss:  0.003467056435\n",
      "training step: 35957, loss:  0.011776654050\n",
      "training step: 35998, loss:  0.004079826176\n",
      "training step: 36039, loss:  0.007751925848\n",
      "training step: 36080, loss:  0.003120253794\n",
      "training step: 36121, loss:  0.011776694097\n",
      "training step: 36162, loss:  0.003638745053\n",
      "training step: 36203, loss:  0.003866118612\n",
      "training step: 36244, loss:  0.003011358902\n",
      "training step: 36285, loss:  0.025363497436\n",
      "training step: 36326, loss:  0.002975546988\n",
      "training step: 36367, loss:  0.024755124003\n",
      "training step: 36408, loss:  0.004079831764\n",
      "training step: 36449, loss:  0.009473443963\n",
      "training step: 36490, loss:  0.011776722968\n",
      "training step: 36531, loss:  0.005003256258\n",
      "training step: 36572, loss:  0.007751890924\n",
      "training step: 36613, loss:  0.003120248206\n",
      "training step: 36654, loss:  0.004087053239\n",
      "training step: 36695, loss:  0.022392880172\n",
      "training step: 36736, loss:  0.003284388920\n",
      "training step: 36777, loss:  0.002930150600\n",
      "training step: 36818, loss:  0.002975569107\n",
      "training step: 36859, loss:  0.004079879262\n",
      "training step: 36900, loss:  0.003135796404\n",
      "training step: 36941, loss:  0.003986839205\n",
      "training step: 36982, loss:  0.003692987142\n",
      "training step: 37023, loss:  0.007751949597\n",
      "training step: 37064, loss:  0.018699241802\n",
      "training step: 37105, loss:  0.006285012700\n",
      "training step: 37146, loss:  0.020691476762\n",
      "training step: 37187, loss:  0.025363471359\n",
      "training step: 37228, loss:  0.003473096061\n",
      "training step: 37269, loss:  0.007341512013\n",
      "training step: 37310, loss:  0.005715550389\n",
      "training step: 37351, loss:  0.006285045762\n",
      "training step: 37392, loss:  0.003692993196\n",
      "training step: 37433, loss:  0.003473114688\n",
      "training step: 37474, loss:  0.009004002437\n",
      "training step: 37515, loss:  0.009003958665\n",
      "training step: 37556, loss:  0.009473399259\n",
      "training step: 37597, loss:  0.018699212000\n",
      "training step: 37638, loss:  0.020691435784\n",
      "training step: 37679, loss:  0.011776595376\n",
      "training step: 37720, loss:  0.003002656391\n",
      "training step: 37761, loss:  0.003866114654\n",
      "training step: 37802, loss:  0.014146149158\n",
      "training step: 37843, loss:  0.002386232140\n",
      "training step: 37884, loss:  0.003986824770\n",
      "training step: 37925, loss:  0.007877299562\n",
      "training step: 37966, loss:  0.022392787039\n",
      "training step: 38007, loss:  0.003011394991\n",
      "training step: 38048, loss:  0.002386246342\n",
      "training step: 38089, loss:  0.003808679059\n",
      "training step: 38130, loss:  0.022392731160\n",
      "training step: 38171, loss:  0.003692984115\n",
      "training step: 38212, loss:  0.002396179829\n",
      "training step: 38253, loss:  0.007752106525\n",
      "training step: 38294, loss:  0.003120284295\n",
      "training step: 38335, loss:  0.003900915850\n",
      "training step: 38376, loss:  0.003002685029\n",
      "training step: 38417, loss:  0.009004101157\n",
      "training step: 38458, loss:  0.018699074164\n",
      "training step: 38499, loss:  0.003543939441\n",
      "training step: 38540, loss:  0.003284411738\n",
      "training step: 38581, loss:  0.016485972330\n",
      "training step: 38622, loss:  0.003808674403\n",
      "training step: 38663, loss:  0.004087223671\n",
      "training step: 38704, loss:  0.003011418274\n",
      "training step: 38745, loss:  0.006285186391\n",
      "training step: 38786, loss:  0.009473306127\n",
      "training step: 38827, loss:  0.009473311715\n",
      "training step: 38868, loss:  0.003543939209\n",
      "training step: 38909, loss:  0.003808672540\n",
      "training step: 38950, loss:  0.004087248351\n",
      "training step: 38991, loss:  0.003900913289\n",
      "training step: 39032, loss:  0.011776471511\n",
      "training step: 39073, loss:  0.006285232957\n",
      "training step: 39114, loss:  0.003808673471\n",
      "training step: 39155, loss:  0.007752247620\n",
      "training step: 39196, loss:  0.022392563522\n",
      "training step: 39237, loss:  0.025363164023\n",
      "training step: 39278, loss:  0.023757372051\n",
      "training step: 39319, loss:  0.003473256715\n",
      "training step: 39360, loss:  0.004087297246\n",
      "training step: 39401, loss:  0.007341414690\n",
      "training step: 39442, loss:  0.004080034792\n",
      "training step: 39483, loss:  0.007341413759\n",
      "training step: 39524, loss:  0.007752255537\n",
      "training step: 39565, loss:  0.004087276757\n",
      "training step: 39606, loss:  0.002396238036\n",
      "training step: 39647, loss:  0.007341417018\n",
      "training step: 39688, loss:  0.003692985279\n",
      "training step: 39729, loss:  0.023757414892\n",
      "training step: 39770, loss:  0.003900912125\n",
      "training step: 39811, loss:  0.002386279870\n",
      "training step: 39852, loss:  0.025568196550\n",
      "training step: 39893, loss:  0.004087274428\n",
      "training step: 39934, loss:  0.023757385090\n",
      "training step: 39975, loss:  0.002386274049\n",
      "training step: 40016, loss:  0.025568170473\n",
      "training step: 40057, loss:  0.003002722748\n",
      "training step: 40098, loss:  0.018698981032\n",
      "training step: 40139, loss:  0.003120306879\n",
      "training step: 40180, loss:  0.003543938510\n",
      "training step: 40221, loss:  0.025363165885\n",
      "training step: 40262, loss:  0.003692983417\n",
      "training step: 40303, loss:  0.024754727259\n",
      "training step: 40344, loss:  0.009473269805\n",
      "training step: 40385, loss:  0.004080023151\n",
      "training step: 40426, loss:  0.005003544036\n",
      "training step: 40467, loss:  0.003900913289\n",
      "training step: 40508, loss:  0.003284428967\n",
      "training step: 40549, loss:  0.009473267011\n",
      "training step: 40590, loss:  0.016485886648\n",
      "training step: 40631, loss:  0.009004298598\n",
      "training step: 40672, loss:  0.003692978295\n",
      "training step: 40713, loss:  0.004080046434\n",
      "training step: 40754, loss:  0.003808672074\n",
      "training step: 40795, loss:  0.018698938191\n",
      "training step: 40836, loss:  0.002396260388\n",
      "training step: 40877, loss:  0.007877500728\n",
      "training step: 40918, loss:  0.014145964757\n",
      "training step: 40959, loss:  0.004087318666\n",
      "training step: 41000, loss:  0.023757318035\n",
      "training step: 41041, loss:  0.023757332936\n",
      "training step: 41082, loss:  0.009473244660\n",
      "training step: 41123, loss:  0.003900910262\n",
      "training step: 41164, loss:  0.003011452267\n",
      "training step: 41205, loss:  0.005003597122\n",
      "training step: 41246, loss:  0.003135946812\n",
      "training step: 41287, loss:  0.003543938743\n",
      "training step: 41328, loss:  0.003120319918\n",
      "training step: 41369, loss:  0.009004358202\n",
      "training step: 41410, loss:  0.003866110230\n",
      "training step: 41451, loss:  0.005715786014\n",
      "training step: 41492, loss:  0.003135951469\n",
      "training step: 41533, loss:  0.003120325040\n",
      "training step: 41574, loss:  0.005485931411\n",
      "training step: 41615, loss:  0.004080096260\n",
      "training step: 41656, loss:  0.003638757858\n",
      "training step: 41697, loss:  0.004080071114\n",
      "training step: 41738, loss:  0.003002753248\n",
      "training step: 41779, loss:  0.024754649028\n",
      "training step: 41820, loss:  0.003776642494\n",
      "training step: 41861, loss:  0.018698930740\n",
      "training step: 41902, loss:  0.003986794967\n",
      "training step: 41943, loss:  0.014145929366\n",
      "training step: 41984, loss:  0.002386299660\n",
      "training step: 42025, loss:  0.003284438048\n",
      "training step: 42066, loss:  0.003002758138\n",
      "training step: 42107, loss:  0.003011456458\n",
      "training step: 42148, loss:  0.003011458786\n",
      "training step: 42189, loss:  0.003776642261\n",
      "training step: 42230, loss:  0.003880497068\n",
      "training step: 42271, loss:  0.003002752550\n",
      "training step: 42312, loss:  0.003002759069\n",
      "training step: 42353, loss:  0.007877551951\n",
      "training step: 42394, loss:  0.003808670910\n",
      "training step: 42435, loss:  0.007877543569\n",
      "training step: 42476, loss:  0.003135966137\n",
      "training step: 42517, loss:  0.002930164570\n",
      "training step: 42558, loss:  0.003638758557\n",
      "training step: 42599, loss:  0.003011464141\n",
      "training step: 42640, loss:  0.003473287215\n",
      "training step: 42681, loss:  0.009473229758\n",
      "training step: 42722, loss:  0.003120323177\n",
      "training step: 42763, loss:  0.009004335850\n",
      "training step: 42804, loss:  0.003776642494\n",
      "training step: 42845, loss:  0.003284436185\n",
      "training step: 42886, loss:  0.016485840082\n",
      "training step: 42927, loss:  0.009004312567\n",
      "training step: 42968, loss:  0.002386296168\n",
      "training step: 43009, loss:  0.003692979226\n",
      "training step: 43050, loss:  0.022392483428\n",
      "training step: 43091, loss:  0.009004337713\n",
      "training step: 43132, loss:  0.003900910029\n",
      "training step: 43173, loss:  0.005485927686\n",
      "training step: 43214, loss:  0.003866109997\n",
      "training step: 43255, loss:  0.002975755138\n",
      "training step: 43296, loss:  0.003986796830\n",
      "training step: 43337, loss:  0.007877545431\n",
      "training step: 43378, loss:  0.003692978295\n",
      "training step: 43419, loss:  0.005485933274\n",
      "training step: 43460, loss:  0.024754608050\n",
      "training step: 43501, loss:  0.003808670910\n",
      "training step: 43542, loss:  0.011776424944\n",
      "training step: 43583, loss:  0.007877571508\n",
      "training step: 43624, loss:  0.003543939209\n",
      "training step: 43665, loss:  0.016485841945\n",
      "training step: 43706, loss:  0.002930166200\n",
      "training step: 43747, loss:  0.003473291639\n",
      "training step: 43788, loss:  0.003900909098\n",
      "training step: 43829, loss:  0.005715812556\n",
      "training step: 43870, loss:  0.020691126585\n",
      "training step: 43911, loss:  0.003986800089\n",
      "training step: 43952, loss:  0.002396270400\n",
      "training step: 43993, loss:  0.002930164803\n",
      "training step: 44034, loss:  0.024754649028\n",
      "training step: 44075, loss:  0.003543938743\n",
      "training step: 44116, loss:  0.003692977363\n",
      "training step: 44157, loss:  0.003011458321\n",
      "training step: 44198, loss:  0.003866110230\n",
      "training step: 44239, loss:  0.025568051264\n",
      "training step: 44280, loss:  0.007341372315\n",
      "training step: 44321, loss:  0.018698900938\n",
      "training step: 44362, loss:  0.002386293374\n",
      "training step: 44403, loss:  0.002930164570\n",
      "training step: 44444, loss:  0.003638759255\n",
      "training step: 44485, loss:  0.005485923029\n",
      "training step: 44526, loss:  0.003866109997\n",
      "training step: 44567, loss:  0.022392455488\n",
      "training step: 44608, loss:  0.009004374035\n",
      "training step: 44649, loss:  0.006285314914\n",
      "training step: 44690, loss:  0.002396267839\n",
      "training step: 44731, loss:  0.014145945199\n",
      "training step: 44772, loss:  0.004080113024\n",
      "training step: 44813, loss:  0.003692977596\n",
      "training step: 44854, loss:  0.009473218583\n",
      "training step: 44895, loss:  0.007341375574\n",
      "training step: 44936, loss:  0.003467082977\n",
      "training step: 44977, loss:  0.003866109531\n",
      "training step: 45018, loss:  0.023757286370\n",
      "training step: 45059, loss:  0.003135965671\n",
      "training step: 45100, loss:  0.003284438280\n",
      "training step: 45141, loss:  0.003284441307\n",
      "training step: 45182, loss:  0.007341369055\n",
      "training step: 45223, loss:  0.014145928435\n",
      "training step: 45264, loss:  0.024754606187\n",
      "training step: 45305, loss:  0.002396283438\n",
      "training step: 45346, loss:  0.006285334006\n",
      "training step: 45387, loss:  0.023757293820\n",
      "training step: 45428, loss:  0.003638760420\n",
      "training step: 45469, loss:  0.005715829786\n",
      "training step: 45510, loss:  0.007752356585\n",
      "training step: 45551, loss:  0.003866109066\n",
      "training step: 45592, loss:  0.007877543569\n",
      "training step: 45633, loss:  0.003880496835\n",
      "training step: 45674, loss:  0.003120330861\n",
      "training step: 45715, loss:  0.022392440587\n",
      "training step: 45756, loss:  0.003473300487\n",
      "training step: 45797, loss:  0.003808668815\n",
      "training step: 45838, loss:  0.003776642960\n",
      "training step: 45879, loss:  0.003880496370\n",
      "training step: 45920, loss:  0.018698886037\n",
      "training step: 45961, loss:  0.007752357982\n",
      "training step: 46002, loss:  0.018698871136\n",
      "training step: 46043, loss:  0.003638759721\n",
      "training step: 46084, loss:  0.025363024324\n",
      "training step: 46125, loss:  0.003002762329\n",
      "training step: 46166, loss:  0.003543937812\n",
      "training step: 46207, loss:  0.009004379623\n",
      "training step: 46248, loss:  0.003002761165\n",
      "training step: 46289, loss:  0.003467081115\n",
      "training step: 46330, loss:  0.003808669513\n",
      "training step: 46371, loss:  0.003135966137\n",
      "training step: 46412, loss:  0.003808669513\n",
      "training step: 46453, loss:  0.020691115409\n",
      "training step: 46494, loss:  0.003692977596\n",
      "training step: 46535, loss:  0.003011462977\n",
      "training step: 46576, loss:  0.003135970095\n",
      "training step: 46617, loss:  0.003284441307\n",
      "training step: 46658, loss:  0.004080094397\n",
      "training step: 46699, loss:  0.005715847947\n",
      "training step: 46740, loss:  0.002930166433\n",
      "training step: 46781, loss:  0.005485922098\n",
      "training step: 46822, loss:  0.016485810280\n",
      "training step: 46863, loss:  0.011776389554\n",
      "training step: 46904, loss:  0.003467083443\n",
      "training step: 46945, loss:  0.003467081580\n",
      "training step: 46986, loss:  0.023757277057\n",
      "training step: 47027, loss:  0.002975780284\n",
      "training step: 47068, loss:  0.020691102371\n",
      "training step: 47109, loss:  0.016485823318\n",
      "training step: 47150, loss:  0.005485926755\n",
      "training step: 47191, loss:  0.011776386760\n",
      "training step: 47232, loss:  0.004080091137\n",
      "training step: 47273, loss:  0.007877543569\n",
      "training step: 47314, loss:  0.011776395142\n",
      "training step: 47355, loss:  0.005003628321\n",
      "training step: 47396, loss:  0.003692976898\n",
      "training step: 47437, loss:  0.003776642960\n",
      "training step: 47478, loss:  0.022392436862\n",
      "training step: 47519, loss:  0.014145935886\n",
      "training step: 47560, loss:  0.003638759954\n",
      "training step: 47601, loss:  0.003808669513\n",
      "training step: 47642, loss:  0.003866109066\n",
      "training step: 47683, loss:  0.004080107436\n",
      "training step: 47724, loss:  0.007341363002\n",
      "training step: 47765, loss:  0.016485814005\n",
      "training step: 47806, loss:  0.004087359179\n",
      "training step: 47847, loss:  0.025568025187\n",
      "training step: 47888, loss:  0.002930166433\n",
      "training step: 47929, loss:  0.022392446175\n",
      "training step: 47970, loss:  0.003120332258\n",
      "training step: 48011, loss:  0.011776393279\n",
      "training step: 48052, loss:  0.003880496370\n",
      "training step: 48093, loss:  0.004087341484\n",
      "training step: 48134, loss:  0.020691104233\n",
      "training step: 48175, loss:  0.003866110230\n",
      "training step: 48216, loss:  0.004087361507\n",
      "training step: 48257, loss:  0.018698872998\n",
      "training step: 48298, loss:  0.003011468099\n",
      "training step: 48339, loss:  0.003986796364\n",
      "training step: 48380, loss:  0.007752353791\n",
      "training step: 48421, loss:  0.003638760420\n",
      "training step: 48462, loss:  0.009004369378\n",
      "training step: 48503, loss:  0.002975776093\n",
      "training step: 48544, loss:  0.002930165501\n",
      "training step: 48585, loss:  0.022392455488\n",
      "training step: 48626, loss:  0.014145916328\n",
      "training step: 48667, loss:  0.003866109531\n",
      "training step: 48708, loss:  0.005715840496\n",
      "training step: 48749, loss:  0.014145921916\n",
      "training step: 48790, loss:  0.006285359152\n",
      "training step: 48831, loss:  0.007341364399\n",
      "training step: 48872, loss:  0.002386308042\n",
      "training step: 48913, loss:  0.005715835840\n",
      "training step: 48954, loss:  0.025568028912\n",
      "training step: 48995, loss:  0.003638759721\n",
      "training step: 49036, loss:  0.018698846921\n",
      "training step: 49077, loss:  0.016485793516\n",
      "training step: 49118, loss:  0.003986795899\n",
      "training step: 49159, loss:  0.020691093057\n",
      "training step: 49200, loss:  0.003543939209\n",
      "training step: 49241, loss:  0.009473212995\n",
      "training step: 49282, loss:  0.002975788200\n",
      "training step: 49323, loss:  0.003638760652\n",
      "training step: 49364, loss:  0.023757282645\n",
      "training step: 49405, loss:  0.020691085607\n",
      "training step: 49446, loss:  0.020691100508\n",
      "training step: 49487, loss:  0.020691074431\n",
      "training step: 49528, loss:  0.009004393592\n",
      "training step: 49569, loss:  0.003002766520\n",
      "training step: 49610, loss:  0.003776642494\n",
      "training step: 49651, loss:  0.007877595723\n",
      "training step: 49692, loss:  0.002396281809\n",
      "training step: 49733, loss:  0.005715840496\n",
      "training step: 49774, loss:  0.003986795433\n",
      "training step: 49815, loss:  0.005485924426\n",
      "training step: 49856, loss:  0.005715849809\n",
      "training step: 49897, loss:  0.014145912603\n",
      "training step: 49938, loss:  0.005485922098\n",
      "training step: 49979, loss:  0.024754581973\n",
      "training step: 50020, loss:  0.007341366727\n",
      "training step: 50061, loss:  0.003900908167\n",
      "training step: 50102, loss:  0.018698861822\n",
      "training step: 50143, loss:  0.025568040088\n",
      "training step: 50184, loss:  0.002396280412\n",
      "training step: 50225, loss:  0.003473313525\n",
      "training step: 50266, loss:  0.003120331559\n",
      "training step: 50307, loss:  0.020691085607\n",
      "training step: 50348, loss:  0.024754580110\n",
      "training step: 50389, loss:  0.004080107901\n",
      "training step: 50430, loss:  0.003866109066\n",
      "training step: 50471, loss:  0.003473321907\n",
      "training step: 50512, loss:  0.003284440143\n",
      "training step: 50553, loss:  0.003986793105\n",
      "training step: 50594, loss:  0.003135984531\n",
      "training step: 50635, loss:  0.005003642756\n",
      "training step: 50676, loss:  0.009004390799\n",
      "training step: 50717, loss:  0.002396287397\n",
      "training step: 50758, loss:  0.002396290889\n",
      "training step: 50799, loss:  0.007341360208\n",
      "training step: 50840, loss:  0.002386307111\n",
      "training step: 50881, loss:  0.020691081882\n",
      "training step: 50922, loss:  0.003011472523\n",
      "training step: 50963, loss:  0.004080117680\n",
      "training step: 51004, loss:  0.003866108367\n",
      "training step: 51045, loss:  0.003011472756\n",
      "training step: 51086, loss:  0.009473205544\n",
      "training step: 51127, loss:  0.003011475550\n",
      "training step: 51168, loss:  0.024754580110\n",
      "training step: 51209, loss:  0.024754540995\n",
      "training step: 51250, loss:  0.007877586409\n",
      "training step: 51291, loss:  0.007877599448\n",
      "training step: 51332, loss:  0.003900907701\n",
      "training step: 51373, loss:  0.014145871624\n",
      "training step: 51414, loss:  0.025568019599\n",
      "training step: 51455, loss:  0.003900907701\n",
      "training step: 51496, loss:  0.005003646947\n",
      "training step: 51537, loss:  0.003135977080\n",
      "training step: 51578, loss:  0.011776363477\n",
      "training step: 51619, loss:  0.003776642494\n",
      "training step: 51660, loss:  0.003776642494\n",
      "training step: 51701, loss:  0.011776369065\n",
      "training step: 51742, loss:  0.006285360083\n",
      "training step: 51783, loss:  0.003011469962\n",
      "training step: 51824, loss:  0.003011472290\n",
      "training step: 51865, loss:  0.003776642960\n",
      "training step: 51906, loss:  0.003135979874\n",
      "training step: 51947, loss:  0.003776642960\n",
      "training step: 51988, loss:  0.025362988934\n",
      "training step: 52029, loss:  0.003011477645\n",
      "training step: 52070, loss:  0.020691085607\n",
      "training step: 52111, loss:  0.003638760652\n",
      "training step: 52152, loss:  0.003638761118\n",
      "training step: 52193, loss:  0.007341350894\n",
      "training step: 52234, loss:  0.004080127459\n",
      "training step: 52275, loss:  0.005715877283\n",
      "training step: 52316, loss:  0.009004433639\n",
      "training step: 52357, loss:  0.002975803101\n",
      "training step: 52398, loss:  0.022392386571\n",
      "training step: 52439, loss:  0.018698833883\n",
      "training step: 52480, loss:  0.022392380983\n",
      "training step: 52521, loss:  0.007341342047\n",
      "training step: 52562, loss:  0.003473340301\n",
      "training step: 52603, loss:  0.003473332385\n",
      "training step: 52644, loss:  0.025362964720\n",
      "training step: 52685, loss:  0.002396287629\n",
      "training step: 52726, loss:  0.025362998247\n",
      "training step: 52767, loss:  0.023757230490\n",
      "training step: 52808, loss:  0.004087395966\n",
      "training step: 52849, loss:  0.003986789845\n",
      "training step: 52890, loss:  0.002386309672\n",
      "training step: 52931, loss:  0.003120338777\n",
      "training step: 52972, loss:  0.011776361614\n",
      "training step: 53013, loss:  0.007752378006\n",
      "training step: 53054, loss:  0.002386306413\n",
      "training step: 53095, loss:  0.005003662780\n",
      "training step: 53136, loss:  0.007752408274\n",
      "training step: 53177, loss:  0.014145903289\n",
      "training step: 53218, loss:  0.007752401289\n",
      "training step: 53259, loss:  0.018698826432\n",
      "training step: 53300, loss:  0.003002776532\n",
      "training step: 53341, loss:  0.003986791242\n",
      "training step: 53382, loss:  0.003011475317\n",
      "training step: 53423, loss:  0.003866108833\n",
      "training step: 53464, loss:  0.025362972170\n",
      "training step: 53505, loss:  0.003776642960\n",
      "training step: 53546, loss:  0.007752404083\n",
      "training step: 53587, loss:  0.005003650207\n",
      "training step: 53628, loss:  0.003011482535\n",
      "training step: 53669, loss:  0.007752404548\n",
      "training step: 53710, loss:  0.003776642960\n",
      "training step: 53751, loss:  0.003473339602\n",
      "training step: 53792, loss:  0.006285377312\n",
      "training step: 53833, loss:  0.009473186918\n",
      "training step: 53874, loss:  0.003135987557\n",
      "training step: 53915, loss:  0.018698841333\n",
      "training step: 53956, loss:  0.025567999110\n",
      "training step: 53997, loss:  0.005003635772\n",
      "training step: 54038, loss:  0.003284445964\n",
      "training step: 54079, loss:  0.003638761118\n",
      "training step: 54120, loss:  0.003638760652\n",
      "training step: 54161, loss:  0.003638760420\n",
      "training step: 54202, loss:  0.003543940606\n",
      "training step: 54243, loss:  0.002930167364\n",
      "training step: 54284, loss:  0.003986791708\n",
      "training step: 54325, loss:  0.007341340650\n",
      "training step: 54366, loss:  0.003866108833\n",
      "training step: 54407, loss:  0.018698835745\n",
      "training step: 54448, loss:  0.005003645085\n",
      "training step: 54489, loss:  0.003808669513\n",
      "training step: 54530, loss:  0.003638760652\n",
      "training step: 54571, loss:  0.018698815256\n",
      "training step: 54612, loss:  0.003011474153\n",
      "training step: 54653, loss:  0.002975800307\n",
      "training step: 54694, loss:  0.003808670212\n",
      "training step: 54735, loss:  0.009004402906\n",
      "training step: 54776, loss:  0.003473325865\n",
      "training step: 54817, loss:  0.002930167364\n",
      "training step: 54858, loss:  0.024754554033\n",
      "training step: 54899, loss:  0.025567982346\n",
      "training step: 54940, loss:  0.003986789845\n",
      "training step: 54981, loss:  0.003986788448\n",
      "training step: 55022, loss:  0.003808670212\n",
      "training step: 55063, loss:  0.003284443170\n",
      "training step: 55104, loss:  0.016485789791\n",
      "training step: 55145, loss:  0.003692978295\n",
      "training step: 55186, loss:  0.003808670212\n",
      "training step: 55227, loss:  0.025362998247\n",
      "training step: 55268, loss:  0.002930166433\n",
      "training step: 55309, loss:  0.007877593860\n",
      "training step: 55350, loss:  0.002396288328\n",
      "training step: 55391, loss:  0.007752368227\n",
      "training step: 55432, loss:  0.003011471359\n",
      "training step: 55473, loss:  0.002930167830\n",
      "training step: 55514, loss:  0.003120334120\n",
      "training step: 55555, loss:  0.003002771176\n",
      "training step: 55596, loss:  0.024754581973\n",
      "training step: 55637, loss:  0.024754565209\n",
      "training step: 55678, loss:  0.003120335750\n",
      "training step: 55719, loss:  0.003638760652\n",
      "training step: 55760, loss:  0.003120333422\n",
      "training step: 55801, loss:  0.004087383859\n",
      "training step: 55842, loss:  0.009004422463\n",
      "training step: 55883, loss:  0.023757267743\n",
      "training step: 55924, loss:  0.025567999110\n",
      "training step: 55965, loss:  0.005003625527\n",
      "training step: 56006, loss:  0.014145900495\n",
      "training step: 56047, loss:  0.004080122337\n",
      "training step: 56088, loss:  0.022392425686\n",
      "training step: 56129, loss:  0.003986792173\n",
      "training step: 56170, loss:  0.018698854372\n",
      "training step: 56211, loss:  0.002386311302\n",
      "training step: 56252, loss:  0.002386307111\n",
      "training step: 56293, loss:  0.020691080019\n",
      "training step: 56334, loss:  0.025568012148\n",
      "training step: 56375, loss:  0.002975786105\n",
      "training step: 56416, loss:  0.009473199025\n",
      "training step: 56457, loss:  0.007341352291\n",
      "training step: 56498, loss:  0.009004435502\n",
      "training step: 56539, loss:  0.003467084840\n",
      "training step: 56580, loss:  0.009473204613\n",
      "training step: 56621, loss:  0.003135978011\n",
      "training step: 56662, loss:  0.003002766520\n",
      "training step: 56703, loss:  0.011776364408\n",
      "training step: 56744, loss:  0.003467082977\n",
      "training step: 56785, loss:  0.018698854372\n",
      "training step: 56826, loss:  0.005003635306\n",
      "training step: 56867, loss:  0.003880497068\n",
      "training step: 56908, loss:  0.003880496370\n",
      "training step: 56949, loss:  0.005485916976\n",
      "training step: 56990, loss:  0.003011469729\n",
      "training step: 57031, loss:  0.002975798678\n",
      "training step: 57072, loss:  0.005485923029\n",
      "training step: 57113, loss:  0.003776642494\n",
      "training step: 57154, loss:  0.023757249117\n",
      "training step: 57195, loss:  0.006285357755\n",
      "training step: 57236, loss:  0.003808670677\n",
      "training step: 57277, loss:  0.016485765576\n",
      "training step: 57318, loss:  0.005003638566\n",
      "training step: 57359, loss:  0.003900907701\n",
      "training step: 57400, loss:  0.006285368465\n",
      "training step: 57441, loss:  0.003986793570\n",
      "training step: 57482, loss:  0.003543941304\n",
      "training step: 57523, loss:  0.005485918839\n",
      "training step: 57564, loss:  0.007877602242\n",
      "training step: 57605, loss:  0.007877581753\n",
      "training step: 57646, loss:  0.003692979226\n",
      "training step: 57687, loss:  0.023757262155\n",
      "training step: 57728, loss:  0.003638760652\n",
      "training step: 57769, loss:  0.025363018736\n",
      "training step: 57810, loss:  0.009473199956\n",
      "training step: 57851, loss:  0.009004403837\n",
      "training step: 57892, loss:  0.002386308741\n",
      "training step: 57933, loss:  0.005485916510\n",
      "training step: 57974, loss:  0.003900908167\n",
      "training step: 58015, loss:  0.003880497068\n",
      "training step: 58056, loss:  0.003866108833\n",
      "training step: 58097, loss:  0.009473196231\n",
      "training step: 58138, loss:  0.005715865642\n",
      "training step: 58179, loss:  0.004087375943\n",
      "training step: 58220, loss:  0.003120336682\n",
      "training step: 58261, loss:  0.002975791227\n",
      "training step: 58302, loss:  0.003011475317\n",
      "training step: 58343, loss:  0.005485916976\n",
      "training step: 58384, loss:  0.002975787735\n",
      "training step: 58425, loss:  0.007341343444\n",
      "training step: 58466, loss:  0.003808669979\n",
      "training step: 58507, loss:  0.009004396386\n",
      "training step: 58548, loss:  0.002975786105\n",
      "training step: 58589, loss:  0.003880497068\n",
      "training step: 58630, loss:  0.009473202750\n",
      "training step: 58671, loss:  0.003808670677\n",
      "training step: 58712, loss:  0.005715860054\n",
      "training step: 58753, loss:  0.003120339243\n",
      "training step: 58794, loss:  0.003900908399\n",
      "training step: 58835, loss:  0.002975796117\n",
      "training step: 58876, loss:  0.018698843196\n",
      "training step: 58917, loss:  0.003011472058\n",
      "training step: 58958, loss:  0.011776371859\n",
      "training step: 58999, loss:  0.002386311302\n",
      "training step: 59040, loss:  0.009004408494\n",
      "training step: 59081, loss:  0.003120334353\n",
      "training step: 59122, loss:  0.002396294381\n",
      "training step: 59163, loss:  0.003866109066\n",
      "training step: 59204, loss:  0.005003619008\n",
      "training step: 59245, loss:  0.023757256567\n",
      "training step: 59286, loss:  0.003135983134\n",
      "training step: 59327, loss:  0.003900908167\n",
      "training step: 59368, loss:  0.002396282274\n",
      "training step: 59409, loss:  0.003543941770\n",
      "training step: 59450, loss:  0.003880497534\n",
      "training step: 59491, loss:  0.003543941770\n",
      "training step: 59532, loss:  0.003808671376\n",
      "training step: 59573, loss:  0.007752370555\n",
      "training step: 59614, loss:  0.003120331792\n",
      "training step: 59655, loss:  0.006285354961\n",
      "training step: 59696, loss:  0.025363035500\n",
      "training step: 59737, loss:  0.004080103710\n",
      "training step: 59778, loss:  0.003808671376\n",
      "training step: 59819, loss:  0.003808671376\n",
      "training step: 59860, loss:  0.003011471592\n",
      "training step: 59901, loss:  0.025363037363\n",
      "training step: 59942, loss:  0.004087368958\n",
      "training step: 59983, loss:  0.011776383035\n",
      "training step: 60024, loss:  0.020691091195\n",
      "training step: 60065, loss:  0.024754602462\n",
      "training step: 60106, loss:  0.005715838633\n",
      "training step: 60147, loss:  0.002975779120\n",
      "training step: 60188, loss:  0.003011471359\n",
      "training step: 60229, loss:  0.007341356017\n",
      "training step: 60270, loss:  0.025568043813\n",
      "training step: 60311, loss:  0.016485791653\n",
      "training step: 60352, loss:  0.022392449901\n",
      "training step: 60393, loss:  0.003986793105\n",
      "training step: 60434, loss:  0.003900909098\n",
      "training step: 60475, loss:  0.025568028912\n",
      "training step: 60516, loss:  0.009004392661\n",
      "training step: 60557, loss:  0.002396286465\n",
      "training step: 60598, loss:  0.005003618076\n",
      "training step: 60639, loss:  0.009004377760\n",
      "training step: 60680, loss:  0.022392457351\n",
      "training step: 60721, loss:  0.003135974053\n",
      "training step: 60762, loss:  0.003638759954\n",
      "training step: 60803, loss:  0.009473210201\n",
      "training step: 60844, loss:  0.003135968931\n",
      "training step: 60885, loss:  0.007752367761\n",
      "training step: 60926, loss:  0.002396279946\n",
      "training step: 60967, loss:  0.016485789791\n",
      "training step: 61008, loss:  0.003638759954\n",
      "training step: 61049, loss:  0.023757275194\n",
      "training step: 61090, loss:  0.003638759954\n",
      "training step: 61131, loss:  0.005715837702\n",
      "training step: 61172, loss:  0.002930166433\n",
      "training step: 61213, loss:  0.006285325624\n",
      "training step: 61254, loss:  0.023757258430\n",
      "training step: 61295, loss:  0.003284438746\n",
      "training step: 61336, loss:  0.009004384279\n",
      "training step: 61377, loss:  0.018698865548\n",
      "training step: 61418, loss:  0.003880497999\n",
      "training step: 61459, loss:  0.005715844687\n",
      "training step: 61500, loss:  0.018698858097\n",
      "training step: 61541, loss:  0.003866109531\n",
      "training step: 61582, loss:  0.003808671841\n",
      "training step: 61623, loss:  0.016485795379\n",
      "training step: 61664, loss:  0.002396280179\n",
      "training step: 61705, loss:  0.011776390485\n",
      "training step: 61746, loss:  0.003543941770\n",
      "training step: 61787, loss:  0.005485924892\n",
      "training step: 61828, loss:  0.002975780284\n",
      "training step: 61869, loss:  0.003986794036\n",
      "training step: 61910, loss:  0.007752367761\n",
      "training step: 61951, loss:  0.003986794036\n",
      "training step: 61992, loss:  0.003284437582\n",
      "training step: 62033, loss:  0.004080102313\n",
      "training step: 62074, loss:  0.023757269606\n",
      "training step: 62115, loss:  0.005003630184\n",
      "training step: 62156, loss:  0.003776643425\n",
      "training step: 62197, loss:  0.009004375897\n",
      "training step: 62238, loss:  0.002975786338\n",
      "training step: 62279, loss:  0.025363026187\n",
      "training step: 62320, loss:  0.018698856235\n",
      "training step: 62361, loss:  0.002386303851\n",
      "training step: 62402, loss:  0.018698861822\n",
      "training step: 62443, loss:  0.007752361242\n",
      "training step: 62484, loss:  0.002386303851\n",
      "training step: 62525, loss:  0.004087362438\n",
      "training step: 62566, loss:  0.009004385211\n",
      "training step: 62607, loss:  0.003473307705\n",
      "training step: 62648, loss:  0.018698871136\n",
      "training step: 62689, loss:  0.014145912603\n",
      "training step: 62730, loss:  0.002396280877\n",
      "training step: 62771, loss:  0.022392442450\n",
      "training step: 62812, loss:  0.003866109066\n",
      "training step: 62853, loss:  0.003900908865\n",
      "training step: 62894, loss:  0.020691107959\n",
      "training step: 62935, loss:  0.016485800967\n",
      "training step: 62976, loss:  0.003808671841\n",
      "training step: 63017, loss:  0.014145910740\n",
      "training step: 63058, loss:  0.006285352167\n",
      "training step: 63099, loss:  0.003120335052\n",
      "training step: 63140, loss:  0.004080094863\n",
      "training step: 63181, loss:  0.003808671841\n",
      "training step: 63222, loss:  0.009473200887\n",
      "training step: 63263, loss:  0.003284438280\n",
      "training step: 63304, loss:  0.003467083443\n",
      "training step: 63345, loss:  0.007877584547\n",
      "training step: 63386, loss:  0.007341358811\n",
      "training step: 63427, loss:  0.007877580822\n",
      "training step: 63468, loss:  0.004080102313\n",
      "training step: 63509, loss:  0.003776642960\n",
      "training step: 63550, loss:  0.003011472058\n",
      "training step: 63591, loss:  0.006285341922\n",
      "training step: 63632, loss:  0.003776642960\n",
      "training step: 63673, loss:  0.004087376408\n",
      "training step: 63714, loss:  0.009004391730\n",
      "training step: 63755, loss:  0.014145910740\n",
      "training step: 63796, loss:  0.003900909098\n",
      "training step: 63837, loss:  0.003866109531\n",
      "training step: 63878, loss:  0.003473307705\n",
      "training step: 63919, loss:  0.003473307705\n",
      "training step: 63960, loss:  0.007752361242\n",
      "training step: 64001, loss:  0.016485804692\n",
      "training step: 64042, loss:  0.004087368492\n",
      "training step: 64083, loss:  0.003808671376\n",
      "training step: 64124, loss:  0.005715856329\n",
      "training step: 64165, loss:  0.002386304550\n",
      "training step: 64206, loss:  0.003135972889\n",
      "training step: 64247, loss:  0.002930165501\n",
      "training step: 64288, loss:  0.003638759954\n",
      "training step: 64329, loss:  0.002975782845\n",
      "training step: 64370, loss:  0.006285348441\n",
      "training step: 64411, loss:  0.003866109531\n",
      "training step: 64452, loss:  0.003776642960\n",
      "training step: 64493, loss:  0.003011462977\n",
      "training step: 64534, loss:  0.003808671376\n",
      "training step: 64575, loss:  0.005715861451\n",
      "training step: 64616, loss:  0.003986791708\n",
      "training step: 64657, loss:  0.007341358811\n",
      "training step: 64698, loss:  0.004080097657\n",
      "training step: 64739, loss:  0.002396279247\n",
      "training step: 64780, loss:  0.020691093057\n",
      "training step: 64821, loss:  0.023757269606\n",
      "training step: 64862, loss:  0.006285340525\n",
      "training step: 64903, loss:  0.002930166433\n",
      "training step: 64944, loss:  0.003880497068\n",
      "training step: 64985, loss:  0.025363029912\n",
      "training step: 65026, loss:  0.002930166433\n",
      "training step: 65067, loss:  0.002396274358\n",
      "training step: 65108, loss:  0.014145899564\n",
      "training step: 65149, loss:  0.003135979874\n",
      "training step: 65190, loss:  0.003692979924\n",
      "training step: 65231, loss:  0.003808672074\n",
      "training step: 65272, loss:  0.002396275755\n",
      "training step: 65313, loss:  0.003120332491\n",
      "training step: 65354, loss:  0.020691068843\n",
      "training step: 65395, loss:  0.011776382104\n",
      "training step: 65436, loss:  0.003284437349\n",
      "training step: 65477, loss:  0.003866109066\n",
      "training step: 65518, loss:  0.007877576165\n",
      "training step: 65559, loss:  0.025568034500\n",
      "training step: 65600, loss:  0.005485923029\n",
      "training step: 65641, loss:  0.003776642960\n",
      "training step: 65682, loss:  0.003776642960\n",
      "training step: 65723, loss:  0.003120332491\n",
      "training step: 65764, loss:  0.018698854372\n",
      "training step: 65805, loss:  0.009004381485\n",
      "training step: 65846, loss:  0.002975780284\n",
      "training step: 65887, loss:  0.003473316319\n",
      "training step: 65928, loss:  0.002930166200\n",
      "training step: 65969, loss:  0.003692980157\n",
      "training step: 66010, loss:  0.002930166200\n",
      "training step: 66051, loss:  0.002386306413\n",
      "training step: 66092, loss:  0.003467082512\n",
      "training step: 66133, loss:  0.007341358811\n",
      "training step: 66174, loss:  0.003986794967\n",
      "training step: 66215, loss:  0.009004381485\n",
      "training step: 66256, loss:  0.005715847481\n",
      "training step: 66297, loss:  0.018698869273\n",
      "training step: 66338, loss:  0.004087358247\n",
      "training step: 66379, loss:  0.003986794967\n",
      "training step: 66420, loss:  0.025363024324\n",
      "training step: 66461, loss:  0.009473213926\n",
      "training step: 66502, loss:  0.003692979924\n",
      "training step: 66543, loss:  0.002396277152\n",
      "training step: 66584, loss:  0.007877573371\n",
      "training step: 66625, loss:  0.003808671376\n",
      "training step: 66666, loss:  0.006285338663\n",
      "training step: 66707, loss:  0.003986794967\n",
      "training step: 66748, loss:  0.003120334819\n",
      "training step: 66789, loss:  0.004087357782\n",
      "training step: 66830, loss:  0.007341357414\n",
      "training step: 66871, loss:  0.003002756508\n",
      "training step: 66912, loss:  0.007752358913\n",
      "training step: 66953, loss:  0.003135973355\n",
      "training step: 66994, loss:  0.024754574522\n",
      "training step: 67035, loss:  0.005485920236\n",
      "training step: 67076, loss:  0.011776385829\n",
      "training step: 67117, loss:  0.002396277152\n",
      "training step: 67158, loss:  0.025568023324\n",
      "training step: 67199, loss:  0.003880497534\n",
      "training step: 67240, loss:  0.020691083744\n",
      "training step: 67281, loss:  0.002386306645\n",
      "training step: 67322, loss:  0.005485921632\n",
      "training step: 67363, loss:  0.003692980157\n",
      "training step: 67404, loss:  0.024754574522\n",
      "training step: 67445, loss:  0.005715861451\n",
      "training step: 67486, loss:  0.003880497534\n",
      "training step: 67527, loss:  0.003900909098\n",
      "training step: 67568, loss:  0.003002756508\n",
      "training step: 67609, loss:  0.005485921632\n",
      "training step: 67650, loss:  0.003638759954\n",
      "training step: 67691, loss:  0.016485812142\n",
      "training step: 67732, loss:  0.003011467867\n",
      "training step: 67773, loss:  0.002930166200\n",
      "training step: 67814, loss:  0.005003619473\n",
      "training step: 67855, loss:  0.002396278549\n",
      "training step: 67896, loss:  0.025568028912\n",
      "training step: 67937, loss:  0.023757271469\n",
      "training step: 67978, loss:  0.024754576385\n",
      "training step: 68019, loss:  0.003880497534\n",
      "training step: 68060, loss:  0.003866109066\n",
      "training step: 68101, loss:  0.006285340991\n",
      "training step: 68142, loss:  0.003776642960\n",
      "training step: 68183, loss:  0.007341357879\n",
      "training step: 68224, loss:  0.024754576385\n",
      "training step: 68265, loss:  0.022392429411\n",
      "training step: 68306, loss:  0.003986795433\n",
      "training step: 68347, loss:  0.003866109066\n",
      "training step: 68388, loss:  0.003776642960\n",
      "training step: 68429, loss:  0.003880496835\n",
      "training step: 68470, loss:  0.002386304317\n",
      "training step: 68511, loss:  0.022392438725\n",
      "training step: 68552, loss:  0.007752357982\n",
      "training step: 68593, loss:  0.025363041088\n",
      "training step: 68634, loss:  0.024754581973\n",
      "training step: 68675, loss:  0.005485920701\n",
      "training step: 68716, loss:  0.024754580110\n",
      "training step: 68757, loss:  0.003467082512\n",
      "training step: 68798, loss:  0.004087364301\n",
      "training step: 68839, loss:  0.002930166200\n",
      "training step: 68880, loss:  0.011776369065\n",
      "training step: 68921, loss:  0.006285344716\n",
      "training step: 68962, loss:  0.003467082512\n",
      "training step: 69003, loss:  0.020691115409\n",
      "training step: 69044, loss:  0.007752361707\n",
      "training step: 69085, loss:  0.003866109066\n",
      "training step: 69126, loss:  0.003880497534\n",
      "training step: 69167, loss:  0.020691115409\n",
      "training step: 69208, loss:  0.003900909098\n",
      "training step: 69249, loss:  0.003900909098\n",
      "training step: 69290, loss:  0.003135968931\n",
      "training step: 69331, loss:  0.020691106096\n",
      "training step: 69372, loss:  0.003284439910\n",
      "training step: 69413, loss:  0.003808671376\n",
      "training step: 69454, loss:  0.023757277057\n",
      "training step: 69495, loss:  0.003473308869\n",
      "training step: 69536, loss:  0.003776642960\n",
      "training step: 69577, loss:  0.005715851206\n",
      "training step: 69618, loss:  0.005003617145\n",
      "training step: 69659, loss:  0.003808671376\n",
      "training step: 69700, loss:  0.005003617145\n",
      "training step: 69741, loss:  0.007877571508\n",
      "training step: 69782, loss:  0.009004380554\n",
      "training step: 69823, loss:  0.020691094920\n",
      "training step: 69864, loss:  0.007877571508\n",
      "training step: 69905, loss:  0.003002766287\n",
      "training step: 69946, loss:  0.011776372790\n",
      "training step: 69987, loss:  0.025568028912\n",
      "training step: 70028, loss:  0.003986796364\n",
      "training step: 70069, loss:  0.003002766287\n",
      "training step: 70110, loss:  0.002930165967\n",
      "training step: 70151, loss:  0.025568028912\n",
      "training step: 70192, loss:  0.003011467867\n",
      "training step: 70233, loss:  0.003467081115\n",
      "training step: 70274, loss:  0.003880497534\n",
      "training step: 70315, loss:  0.003866109066\n",
      "training step: 70356, loss:  0.002396280877\n",
      "training step: 70397, loss:  0.014145907946\n",
      "training step: 70438, loss:  0.003808671376\n",
      "training step: 70479, loss:  0.007341363933\n",
      "training step: 70520, loss:  0.003473304445\n",
      "training step: 70561, loss:  0.007752376143\n",
      "training step: 70602, loss:  0.025363050401\n",
      "training step: 70643, loss:  0.018698848784\n",
      "training step: 70684, loss:  0.005485923961\n",
      "training step: 70725, loss:  0.003473304445\n",
      "training step: 70766, loss:  0.005715851206\n",
      "training step: 70807, loss:  0.024754585698\n",
      "training step: 70848, loss:  0.003120334120\n",
      "training step: 70889, loss:  0.016485799104\n",
      "training step: 70930, loss:  0.011776375584\n",
      "training step: 70971, loss:  0.003866109066\n",
      "training step: 71012, loss:  0.023757275194\n",
      "training step: 71053, loss:  0.003135975217\n",
      "training step: 71094, loss:  0.023757275194\n",
      "training step: 71135, loss:  0.003467081115\n",
      "training step: 71176, loss:  0.002975781681\n",
      "training step: 71217, loss:  0.003866109066\n",
      "training step: 71258, loss:  0.011776376516\n",
      "training step: 71299, loss:  0.005715851206\n",
      "training step: 71340, loss:  0.003986796364\n",
      "training step: 71381, loss:  0.007752377074\n",
      "training step: 71422, loss:  0.018698858097\n",
      "training step: 71463, loss:  0.003866109066\n",
      "training step: 71504, loss:  0.020691094920\n",
      "training step: 71545, loss:  0.007877563126\n",
      "training step: 71586, loss:  0.002396280877\n",
      "training step: 71627, loss:  0.005715851206\n",
      "training step: 71668, loss:  0.003808671376\n",
      "training step: 71709, loss:  0.002930165734\n",
      "training step: 71750, loss:  0.002930165734\n",
      "training step: 71791, loss:  0.007341358811\n",
      "training step: 71832, loss:  0.007752380800\n",
      "training step: 71873, loss:  0.018698845059\n",
      "training step: 71914, loss:  0.003467082279\n",
      "training step: 71955, loss:  0.003880497534\n",
      "training step: 71996, loss:  0.020691098645\n",
      "training step: 72037, loss:  0.005485922098\n",
      "training step: 72078, loss:  0.002975780284\n",
      "training step: 72119, loss:  0.020691098645\n",
      "training step: 72160, loss:  0.003638759954\n",
      "training step: 72201, loss:  0.025363046676\n",
      "training step: 72242, loss:  0.003900909098\n",
      "training step: 72283, loss:  0.025363046676\n",
      "training step: 72324, loss:  0.002975780284\n",
      "training step: 72365, loss:  0.009004401974\n",
      "training step: 72406, loss:  0.005003615282\n",
      "training step: 72447, loss:  0.023757282645\n",
      "training step: 72488, loss:  0.005715855863\n",
      "training step: 72529, loss:  0.003638759954\n",
      "training step: 72570, loss:  0.009473215789\n",
      "training step: 72611, loss:  0.003986798227\n",
      "training step: 72652, loss:  0.005003615282\n",
      "training step: 72693, loss:  0.003135976614\n",
      "training step: 72734, loss:  0.003473304445\n",
      "training step: 72775, loss:  0.011776376516\n",
      "training step: 72816, loss:  0.005485921632\n",
      "training step: 72857, loss:  0.002396284137\n",
      "training step: 72898, loss:  0.005485921632\n",
      "training step: 72939, loss:  0.003473304445\n",
      "training step: 72980, loss:  0.003002761398\n",
      "training step: 73021, loss:  0.007341358811\n",
      "training step: 73062, loss:  0.004080093000\n",
      "training step: 73103, loss:  0.018698843196\n",
      "training step: 73144, loss:  0.016485802829\n",
      "training step: 73185, loss:  0.003120336682\n",
      "training step: 73226, loss:  0.003011468332\n",
      "training step: 73267, loss:  0.023757282645\n",
      "training step: 73308, loss:  0.003467082279\n",
      "training step: 73349, loss:  0.003866109066\n",
      "training step: 73390, loss:  0.004087368958\n",
      "training step: 73431, loss:  0.003638759954\n",
      "training step: 73472, loss:  0.016485802829\n",
      "training step: 73513, loss:  0.007341358811\n",
      "training step: 73554, loss:  0.003011468332\n",
      "training step: 73595, loss:  0.009004401974\n",
      "training step: 73636, loss:  0.003543942003\n",
      "training step: 73677, loss:  0.020691098645\n",
      "training step: 73718, loss:  0.003120336682\n",
      "training step: 73759, loss:  0.003543942003\n",
      "training step: 73800, loss:  0.003120336682\n",
      "training step: 73841, loss:  0.004080093000\n",
      "training step: 73882, loss:  0.005715856329\n",
      "training step: 73923, loss:  0.016485802829\n",
      "training step: 73964, loss:  0.005715856329\n",
      "training step: 74005, loss:  0.003692979924\n",
      "training step: 74046, loss:  0.003135976614\n",
      "training step: 74087, loss:  0.002396283671\n",
      "training step: 74128, loss:  0.003543942003\n",
      "training step: 74169, loss:  0.006285338197\n",
      "training step: 74210, loss:  0.006285338197\n",
      "training step: 74251, loss:  0.018698843196\n",
      "training step: 74292, loss:  0.003120336682\n",
      "training step: 74333, loss:  0.003467082279\n",
      "training step: 74374, loss:  0.004087368958\n",
      "training step: 74415, loss:  0.003135976614\n",
      "training step: 74456, loss:  0.007877553813\n",
      "training step: 74497, loss:  0.003002761398\n",
      "training step: 74538, loss:  0.009004401974\n",
      "training step: 74579, loss:  0.005003615282\n",
      "training step: 74620, loss:  0.005485921632\n",
      "training step: 74661, loss:  0.004087368958\n",
      "training step: 74702, loss:  0.003776642960\n",
      "training step: 74743, loss:  0.002930165734\n",
      "training step: 74784, loss:  0.006285338197\n",
      "training step: 74825, loss:  0.025568028912\n",
      "training step: 74866, loss:  0.002930165734\n",
      "training step: 74907, loss:  0.002930165734\n",
      "training step: 74948, loss:  0.007877553813\n",
      "training step: 74989, loss:  0.003543942003\n",
      "training step: 75030, loss:  0.024754593149\n",
      "training step: 75071, loss:  0.003866109066\n",
      "training step: 75112, loss:  0.022392421961\n",
      "training step: 75153, loss:  0.003986798227\n",
      "training step: 75194, loss:  0.007752379868\n",
      "training step: 75235, loss:  0.003808671376\n",
      "training step: 75276, loss:  0.002396283671\n",
      "training step: 75317, loss:  0.011776376516\n",
      "training step: 75358, loss:  0.020691098645\n",
      "training step: 75399, loss:  0.025568028912\n",
      "training step: 75440, loss:  0.009004401974\n",
      "training step: 75481, loss:  0.003808671376\n",
      "training step: 75522, loss:  0.020691098645\n",
      "training step: 75563, loss:  0.006285338197\n",
      "training step: 75604, loss:  0.009004401974\n",
      "training step: 75645, loss:  0.016485802829\n",
      "training step: 75686, loss:  0.003880497534\n",
      "training step: 75727, loss:  0.003880497534\n",
      "training step: 75768, loss:  0.003808671376\n",
      "training step: 75809, loss:  0.007877553813\n",
      "training step: 75850, loss:  0.009473215789\n",
      "training step: 75891, loss:  0.007752379868\n",
      "training step: 75932, loss:  0.004080093466\n",
      "training step: 75973, loss:  0.002386306180\n",
      "training step: 76014, loss:  0.004080093466\n",
      "training step: 76055, loss:  0.003986798227\n",
      "training step: 76096, loss:  0.023757282645\n",
      "training step: 76137, loss:  0.002386306180\n",
      "training step: 76178, loss:  0.004087368958\n",
      "training step: 76219, loss:  0.003135976614\n",
      "training step: 76260, loss:  0.025363046676\n",
      "training step: 76301, loss:  0.003284437582\n",
      "training step: 76342, loss:  0.003692979924\n",
      "training step: 76383, loss:  0.004080093466\n",
      "training step: 76424, loss:  0.003284437582\n",
      "training step: 76465, loss:  0.003866109066\n",
      "training step: 76506, loss:  0.002975780051\n",
      "training step: 76547, loss:  0.022392421961\n",
      "training step: 76588, loss:  0.003900909098\n",
      "training step: 76629, loss:  0.004080093466\n",
      "training step: 76670, loss:  0.003473303979\n",
      "training step: 76711, loss:  0.009473215789\n",
      "training step: 76752, loss:  0.020691098645\n",
      "training step: 76793, loss:  0.002975780051\n",
      "training step: 76834, loss:  0.005003615282\n",
      "training step: 76875, loss:  0.004087368958\n",
      "training step: 76916, loss:  0.003776642960\n",
      "training step: 76957, loss:  0.003120336682\n",
      "training step: 76998, loss:  0.018698843196\n",
      "training step: 77039, loss:  0.003866109066\n",
      "training step: 77080, loss:  0.007877553813\n",
      "training step: 77121, loss:  0.003866109066\n",
      "training step: 77162, loss:  0.003638759954\n",
      "training step: 77203, loss:  0.025363046676\n",
      "training step: 77244, loss:  0.004080093466\n",
      "training step: 77285, loss:  0.007341358811\n",
      "training step: 77326, loss:  0.005485921632\n",
      "training step: 77367, loss:  0.004087368958\n",
      "training step: 77408, loss:  0.002396283671\n",
      "training step: 77449, loss:  0.003866109066\n",
      "training step: 77490, loss:  0.007752379868\n",
      "training step: 77531, loss:  0.003002761398\n",
      "training step: 77572, loss:  0.003467082279\n",
      "training step: 77613, loss:  0.022392421961\n",
      "training step: 77654, loss:  0.003543942003\n",
      "training step: 77695, loss:  0.003986798227\n",
      "training step: 77736, loss:  0.007877553813\n",
      "training step: 77777, loss:  0.003986798227\n",
      "training step: 77818, loss:  0.022392421961\n",
      "training step: 77859, loss:  0.002930165734\n",
      "training step: 77900, loss:  0.005003615282\n",
      "training step: 77941, loss:  0.003011468332\n",
      "training step: 77982, loss:  0.003900909098\n",
      "training step: 78023, loss:  0.003543942003\n",
      "training step: 78064, loss:  0.025568028912\n",
      "training step: 78105, loss:  0.003467082279\n",
      "training step: 78146, loss:  0.005003615282\n",
      "training step: 78187, loss:  0.011776376516\n",
      "training step: 78228, loss:  0.003284437582\n",
      "training step: 78269, loss:  0.007341358811\n",
      "training step: 78310, loss:  0.005485921632\n",
      "training step: 78351, loss:  0.007341358811\n",
      "training step: 78392, loss:  0.003866109066\n",
      "training step: 78433, loss:  0.007877553813\n",
      "training step: 78474, loss:  0.014145905152\n",
      "training step: 78515, loss:  0.005485921632\n",
      "training step: 78556, loss:  0.005003615282\n",
      "training step: 78597, loss:  0.016485802829\n",
      "training step: 78638, loss:  0.003638759954\n",
      "training step: 78679, loss:  0.016485802829\n",
      "training step: 78720, loss:  0.006285338197\n",
      "training step: 78761, loss:  0.025568028912\n",
      "training step: 78802, loss:  0.003473303979\n",
      "training step: 78843, loss:  0.003135976614\n",
      "training step: 78884, loss:  0.023757282645\n",
      "training step: 78925, loss:  0.003986798227\n",
      "training step: 78966, loss:  0.007752379868\n",
      "training step: 79007, loss:  0.003776642960\n",
      "training step: 79048, loss:  0.003808671376\n",
      "training step: 79089, loss:  0.003638759954\n",
      "training step: 79130, loss:  0.003900909098\n",
      "training step: 79171, loss:  0.005003615282\n",
      "training step: 79212, loss:  0.002930165734\n",
      "training step: 79253, loss:  0.003776642960\n",
      "training step: 79294, loss:  0.002930165734\n",
      "training step: 79335, loss:  0.003284437582\n",
      "training step: 79376, loss:  0.009473215789\n",
      "training step: 79417, loss:  0.003692979924\n",
      "training step: 79458, loss:  0.003776642960\n",
      "training step: 79499, loss:  0.024754593149\n",
      "training step: 79540, loss:  0.011776376516\n",
      "training step: 79581, loss:  0.025363046676\n",
      "training step: 79622, loss:  0.003011468332\n",
      "training step: 79663, loss:  0.007341358811\n",
      "training step: 79704, loss:  0.003986798227\n",
      "training step: 79745, loss:  0.007341358811\n",
      "training step: 79786, loss:  0.009004401974\n",
      "training step: 79827, loss:  0.020691098645\n",
      "training step: 79868, loss:  0.003473303979\n",
      "training step: 79909, loss:  0.007341358811\n",
      "training step: 79950, loss:  0.007341358811\n",
      "training step: 79991, loss:  0.003284437582\n",
      "training step: 80032, loss:  0.002396283671\n",
      "training step: 80073, loss:  0.003692979924\n",
      "training step: 80114, loss:  0.011776376516\n",
      "training step: 80155, loss:  0.002386306180\n",
      "training step: 80196, loss:  0.007877553813\n",
      "training step: 80237, loss:  0.003473303979\n",
      "training step: 80278, loss:  0.003866109066\n",
      "training step: 80319, loss:  0.002386306180\n",
      "training step: 80360, loss:  0.025568028912\n",
      "training step: 80401, loss:  0.003467082279\n",
      "training step: 80442, loss:  0.003120336682\n",
      "training step: 80483, loss:  0.003880497534\n",
      "training step: 80524, loss:  0.009004401974\n",
      "training step: 80565, loss:  0.022392421961\n",
      "training step: 80606, loss:  0.025363046676\n",
      "training step: 80647, loss:  0.002396283671\n",
      "training step: 80688, loss:  0.007877553813\n",
      "training step: 80729, loss:  0.014145905152\n",
      "training step: 80770, loss:  0.003900909098\n",
      "training step: 80811, loss:  0.009004401974\n",
      "training step: 80852, loss:  0.003880497534\n",
      "training step: 80893, loss:  0.007752379868\n",
      "training step: 80934, loss:  0.003135976614\n",
      "training step: 80975, loss:  0.003866109066\n",
      "training step: 81016, loss:  0.004080093466\n",
      "training step: 81057, loss:  0.003900909098\n",
      "training step: 81098, loss:  0.009473215789\n",
      "training step: 81139, loss:  0.003776642960\n",
      "training step: 81180, loss:  0.003543942003\n",
      "training step: 81221, loss:  0.002396283671\n",
      "training step: 81262, loss:  0.003900909098\n",
      "training step: 81303, loss:  0.014145905152\n",
      "training step: 81344, loss:  0.014145905152\n",
      "training step: 81385, loss:  0.009473215789\n",
      "training step: 81426, loss:  0.018698843196\n",
      "training step: 81467, loss:  0.003011468332\n",
      "training step: 81508, loss:  0.011776376516\n",
      "training step: 81549, loss:  0.003135976614\n",
      "training step: 81590, loss:  0.005715856329\n",
      "training step: 81631, loss:  0.006285338197\n",
      "training step: 81672, loss:  0.016485802829\n",
      "training step: 81713, loss:  0.003473303979\n",
      "training step: 81754, loss:  0.016485802829\n",
      "training step: 81795, loss:  0.003467082279\n",
      "training step: 81836, loss:  0.022392421961\n",
      "training step: 81877, loss:  0.003638759954\n",
      "training step: 81918, loss:  0.009473215789\n",
      "training step: 81959, loss:  0.003120336682\n",
      "training step: 82000, loss:  0.025363046676\n",
      "training step: 82041, loss:  0.002386306180\n",
      "training step: 82082, loss:  0.020691098645\n",
      "training step: 82123, loss:  0.003900909098\n",
      "training step: 82164, loss:  0.003284437582\n",
      "training step: 82205, loss:  0.003467082279\n",
      "training step: 82246, loss:  0.007341358811\n",
      "training step: 82287, loss:  0.004087368958\n",
      "training step: 82328, loss:  0.003808671376\n",
      "training step: 82369, loss:  0.002386306180\n",
      "training step: 82410, loss:  0.003638759954\n",
      "training step: 82451, loss:  0.007341358811\n",
      "training step: 82492, loss:  0.025568028912\n",
      "training step: 82533, loss:  0.003880497534\n",
      "training step: 82574, loss:  0.005715856329\n",
      "training step: 82615, loss:  0.002396283671\n",
      "training step: 82656, loss:  0.003543942003\n",
      "training step: 82697, loss:  0.020691098645\n",
      "training step: 82738, loss:  0.003866109066\n",
      "training step: 82779, loss:  0.020691098645\n",
      "training step: 82820, loss:  0.003467082279\n",
      "training step: 82861, loss:  0.018698843196\n",
      "training step: 82902, loss:  0.025568028912\n",
      "training step: 82943, loss:  0.003808671376\n",
      "training step: 82984, loss:  0.004087368958\n",
      "training step: 83025, loss:  0.011776376516\n",
      "training step: 83066, loss:  0.020691098645\n",
      "training step: 83107, loss:  0.020691098645\n",
      "training step: 83148, loss:  0.003135976614\n",
      "training step: 83189, loss:  0.009473215789\n",
      "training step: 83230, loss:  0.018698843196\n",
      "training step: 83271, loss:  0.003284437582\n",
      "training step: 83312, loss:  0.003986798227\n",
      "training step: 83353, loss:  0.018698843196\n",
      "training step: 83394, loss:  0.003467082279\n",
      "training step: 83435, loss:  0.025363046676\n",
      "training step: 83476, loss:  0.005003615282\n",
      "training step: 83517, loss:  0.014145905152\n",
      "training step: 83558, loss:  0.006285338197\n",
      "training step: 83599, loss:  0.024754593149\n",
      "training step: 83640, loss:  0.003120336682\n",
      "training step: 83681, loss:  0.007877553813\n",
      "training step: 83722, loss:  0.005715856329\n",
      "training step: 83763, loss:  0.003473303979\n",
      "training step: 83804, loss:  0.003808671376\n",
      "training step: 83845, loss:  0.006285338197\n",
      "training step: 83886, loss:  0.022392421961\n",
      "training step: 83927, loss:  0.007877553813\n",
      "training step: 83968, loss:  0.003692979924\n",
      "training step: 84009, loss:  0.004080093466\n",
      "training step: 84050, loss:  0.003011468332\n",
      "training step: 84091, loss:  0.003135976614\n",
      "training step: 84132, loss:  0.002396283671\n",
      "training step: 84173, loss:  0.025363046676\n",
      "training step: 84214, loss:  0.011776376516\n",
      "training step: 84255, loss:  0.007341358811\n",
      "training step: 84296, loss:  0.003776642960\n",
      "training step: 84337, loss:  0.005003615282\n",
      "training step: 84378, loss:  0.009004401974\n",
      "training step: 84419, loss:  0.003880497534\n",
      "training step: 84460, loss:  0.006285338197\n",
      "training step: 84501, loss:  0.009004401974\n",
      "training step: 84542, loss:  0.003900909098\n",
      "training step: 84583, loss:  0.005485921632\n",
      "training step: 84624, loss:  0.020691098645\n",
      "training step: 84665, loss:  0.003776642960\n",
      "training step: 84706, loss:  0.024754593149\n",
      "training step: 84747, loss:  0.020691098645\n",
      "training step: 84788, loss:  0.020691098645\n",
      "training step: 84829, loss:  0.007877553813\n",
      "training step: 84870, loss:  0.004080093466\n",
      "training step: 84911, loss:  0.020691098645\n",
      "training step: 84952, loss:  0.003986798227\n",
      "training step: 84993, loss:  0.003986798227\n",
      "training step: 85034, loss:  0.014145905152\n",
      "training step: 85075, loss:  0.007341358811\n",
      "training step: 85116, loss:  0.014145905152\n",
      "training step: 85157, loss:  0.022392421961\n",
      "training step: 85198, loss:  0.003002761398\n",
      "training step: 85239, loss:  0.014145905152\n",
      "training step: 85280, loss:  0.003011468332\n",
      "training step: 85321, loss:  0.003473303979\n",
      "training step: 85362, loss:  0.003120336682\n",
      "training step: 85403, loss:  0.003638759954\n",
      "training step: 85444, loss:  0.004087368958\n",
      "training step: 85485, loss:  0.024754593149\n",
      "training step: 85526, loss:  0.003473303979\n",
      "training step: 85567, loss:  0.004080093466\n",
      "training step: 85608, loss:  0.003692979924\n",
      "training step: 85649, loss:  0.003986798227\n",
      "training step: 85690, loss:  0.016485802829\n",
      "training step: 85731, loss:  0.003543942003\n",
      "training step: 85772, loss:  0.016485802829\n",
      "training step: 85813, loss:  0.003002761398\n",
      "training step: 85854, loss:  0.003776642960\n",
      "training step: 85895, loss:  0.005003615282\n",
      "training step: 85936, loss:  0.003467082279\n",
      "training step: 85977, loss:  0.003473303979\n",
      "training step: 86018, loss:  0.004087368958\n",
      "training step: 86059, loss:  0.004080093466\n",
      "training step: 86100, loss:  0.016485802829\n",
      "training step: 86141, loss:  0.005485921632\n",
      "training step: 86182, loss:  0.003880497534\n",
      "training step: 86223, loss:  0.003692979924\n",
      "training step: 86264, loss:  0.003467082279\n",
      "training step: 86305, loss:  0.005003615282\n",
      "training step: 86346, loss:  0.003467082279\n",
      "training step: 86387, loss:  0.007877553813\n",
      "training step: 86428, loss:  0.007341358811\n",
      "training step: 86469, loss:  0.024754593149\n",
      "training step: 86510, loss:  0.004080093466\n",
      "training step: 86551, loss:  0.002396283671\n",
      "training step: 86592, loss:  0.016485802829\n",
      "training step: 86633, loss:  0.016485802829\n",
      "training step: 86674, loss:  0.003692979924\n",
      "training step: 86715, loss:  0.003135976614\n",
      "training step: 86756, loss:  0.009473215789\n",
      "training step: 86797, loss:  0.009473215789\n",
      "training step: 86838, loss:  0.005485921632\n",
      "training step: 86879, loss:  0.005715856329\n",
      "training step: 86920, loss:  0.003986798227\n",
      "training step: 86961, loss:  0.025363046676\n",
      "training step: 87002, loss:  0.018698843196\n",
      "training step: 87043, loss:  0.005003615282\n",
      "training step: 87084, loss:  0.024754593149\n",
      "training step: 87125, loss:  0.002396283671\n",
      "training step: 87166, loss:  0.025568028912\n",
      "training step: 87207, loss:  0.003638759954\n",
      "training step: 87248, loss:  0.005003615282\n",
      "training step: 87289, loss:  0.024754593149\n",
      "training step: 87330, loss:  0.025568028912\n",
      "training step: 87371, loss:  0.003866109066\n",
      "training step: 87412, loss:  0.003900909098\n",
      "training step: 87453, loss:  0.018698843196\n",
      "training step: 87494, loss:  0.003473303979\n",
      "training step: 87535, loss:  0.006285338197\n",
      "training step: 87576, loss:  0.002975780051\n",
      "training step: 87617, loss:  0.024754593149\n",
      "training step: 87658, loss:  0.003900909098\n",
      "training step: 87699, loss:  0.005715856329\n",
      "training step: 87740, loss:  0.003900909098\n",
      "training step: 87781, loss:  0.003776642960\n",
      "training step: 87822, loss:  0.025568028912\n",
      "training step: 87863, loss:  0.002396283671\n",
      "training step: 87904, loss:  0.003880497534\n",
      "training step: 87945, loss:  0.002386306180\n",
      "training step: 87986, loss:  0.006285338197\n",
      "training step: 88027, loss:  0.003692979924\n",
      "training step: 88068, loss:  0.025363046676\n",
      "training step: 88109, loss:  0.005715856329\n",
      "training step: 88150, loss:  0.003808671376\n",
      "training step: 88191, loss:  0.006285338197\n",
      "training step: 88232, loss:  0.003467082279\n",
      "training step: 88273, loss:  0.003135976614\n",
      "training step: 88314, loss:  0.009473215789\n",
      "training step: 88355, loss:  0.011776376516\n",
      "training step: 88396, loss:  0.003808671376\n",
      "training step: 88437, loss:  0.005485921632\n",
      "training step: 88478, loss:  0.025568028912\n",
      "training step: 88519, loss:  0.003135976614\n",
      "training step: 88560, loss:  0.005715856329\n",
      "training step: 88601, loss:  0.018698843196\n",
      "training step: 88642, loss:  0.003120336682\n",
      "training step: 88683, loss:  0.004087368958\n",
      "training step: 88724, loss:  0.006285338197\n",
      "training step: 88765, loss:  0.003543942003\n",
      "training step: 88806, loss:  0.018698843196\n",
      "training step: 88847, loss:  0.016485802829\n",
      "training step: 88888, loss:  0.006285338197\n",
      "training step: 88929, loss:  0.003776642960\n",
      "training step: 88970, loss:  0.003776642960\n",
      "training step: 89011, loss:  0.003808671376\n",
      "training step: 89052, loss:  0.006285338197\n",
      "training step: 89093, loss:  0.014145905152\n",
      "training step: 89134, loss:  0.003900909098\n",
      "training step: 89175, loss:  0.003880497534\n",
      "training step: 89216, loss:  0.024754593149\n",
      "training step: 89257, loss:  0.003002761398\n",
      "training step: 89298, loss:  0.009473215789\n",
      "training step: 89339, loss:  0.003467082279\n",
      "training step: 89380, loss:  0.009004401974\n",
      "training step: 89421, loss:  0.003692979924\n",
      "training step: 89462, loss:  0.003986798227\n",
      "training step: 89503, loss:  0.003011468332\n",
      "training step: 89544, loss:  0.002386306180\n",
      "training step: 89585, loss:  0.003880497534\n",
      "training step: 89626, loss:  0.003638759954\n",
      "training step: 89667, loss:  0.003002761398\n",
      "training step: 89708, loss:  0.005485921632\n",
      "training step: 89749, loss:  0.009473215789\n",
      "training step: 89790, loss:  0.009004401974\n",
      "training step: 89831, loss:  0.003880497534\n",
      "training step: 89872, loss:  0.003866109066\n",
      "training step: 89913, loss:  0.016485802829\n",
      "training step: 89954, loss:  0.002396283671\n",
      "training step: 89995, loss:  0.020691098645\n",
      "training step: 90036, loss:  0.003692979924\n",
      "training step: 90077, loss:  0.003986798227\n",
      "training step: 90118, loss:  0.024754593149\n",
      "training step: 90159, loss:  0.003692979924\n",
      "training step: 90200, loss:  0.006285338197\n",
      "training step: 90241, loss:  0.009004401974\n",
      "training step: 90282, loss:  0.002396283671\n",
      "training step: 90323, loss:  0.022392421961\n",
      "training step: 90364, loss:  0.003638759954\n",
      "training step: 90405, loss:  0.014145905152\n",
      "training step: 90446, loss:  0.003900909098\n",
      "training step: 90487, loss:  0.003120336682\n",
      "training step: 90528, loss:  0.003866109066\n",
      "training step: 90569, loss:  0.002930165734\n",
      "training step: 90610, loss:  0.003543942003\n",
      "training step: 90651, loss:  0.003284437582\n",
      "training step: 90692, loss:  0.023757282645\n",
      "training step: 90733, loss:  0.006285338197\n",
      "training step: 90774, loss:  0.003986798227\n",
      "training step: 90815, loss:  0.005715856329\n",
      "training step: 90856, loss:  0.003776642960\n",
      "training step: 90897, loss:  0.016485802829\n",
      "training step: 90938, loss:  0.025363046676\n",
      "training step: 90979, loss:  0.003880497534\n",
      "training step: 91020, loss:  0.003692979924\n",
      "training step: 91061, loss:  0.003120336682\n",
      "training step: 91102, loss:  0.025568028912\n",
      "training step: 91143, loss:  0.003120336682\n",
      "training step: 91184, loss:  0.005485921632\n",
      "training step: 91225, loss:  0.002975780051\n",
      "training step: 91266, loss:  0.003467082279\n",
      "training step: 91307, loss:  0.002386306180\n",
      "training step: 91348, loss:  0.003866109066\n",
      "training step: 91389, loss:  0.003692979924\n",
      "training step: 91430, loss:  0.007877553813\n",
      "training step: 91471, loss:  0.020691098645\n",
      "training step: 91512, loss:  0.024754593149\n",
      "training step: 91553, loss:  0.003638759954\n",
      "training step: 91594, loss:  0.005485921632\n",
      "training step: 91635, loss:  0.024754593149\n",
      "training step: 91676, loss:  0.025363046676\n",
      "training step: 91717, loss:  0.003692979924\n",
      "training step: 91758, loss:  0.003900909098\n",
      "training step: 91799, loss:  0.003866109066\n",
      "training step: 91840, loss:  0.005715856329\n",
      "training step: 91881, loss:  0.003467082279\n",
      "training step: 91922, loss:  0.003880497534\n",
      "training step: 91963, loss:  0.003284437582\n",
      "training step: 92004, loss:  0.025568028912\n",
      "training step: 92045, loss:  0.003776642960\n",
      "training step: 92086, loss:  0.009473215789\n",
      "training step: 92127, loss:  0.022392421961\n",
      "training step: 92168, loss:  0.016485802829\n",
      "training step: 92209, loss:  0.018698843196\n",
      "training step: 92250, loss:  0.003986798227\n",
      "training step: 92291, loss:  0.018698843196\n",
      "training step: 92332, loss:  0.004087368958\n",
      "training step: 92373, loss:  0.007877553813\n",
      "training step: 92414, loss:  0.007341358811\n",
      "training step: 92455, loss:  0.025363046676\n",
      "training step: 92496, loss:  0.003692979924\n",
      "training step: 92537, loss:  0.025363046676\n",
      "training step: 92578, loss:  0.014145905152\n",
      "training step: 92619, loss:  0.002396283671\n",
      "training step: 92660, loss:  0.003002761398\n",
      "training step: 92701, loss:  0.003900909098\n",
      "training step: 92742, loss:  0.002975780051\n",
      "training step: 92783, loss:  0.002930165734\n",
      "training step: 92824, loss:  0.003808671376\n",
      "training step: 92865, loss:  0.003011468332\n",
      "training step: 92906, loss:  0.004080093466\n",
      "training step: 92947, loss:  0.003638759954\n",
      "training step: 92988, loss:  0.003543942003\n",
      "training step: 93029, loss:  0.007877553813\n",
      "training step: 93070, loss:  0.004087368958\n",
      "training step: 93111, loss:  0.003002761398\n",
      "training step: 93152, loss:  0.004087368958\n",
      "training step: 93193, loss:  0.018698843196\n",
      "training step: 93234, loss:  0.003467082279\n",
      "training step: 93275, loss:  0.002975780051\n",
      "training step: 93316, loss:  0.002396283671\n",
      "training step: 93357, loss:  0.002975780051\n",
      "training step: 93398, loss:  0.002386306180\n",
      "training step: 93439, loss:  0.003866109066\n",
      "training step: 93480, loss:  0.003135976614\n",
      "training step: 93521, loss:  0.006285338197\n",
      "training step: 93562, loss:  0.003692979924\n",
      "training step: 93603, loss:  0.016485802829\n",
      "training step: 93644, loss:  0.003467082279\n",
      "training step: 93685, loss:  0.004080093466\n",
      "training step: 93726, loss:  0.002396283671\n",
      "training step: 93767, loss:  0.007877553813\n",
      "training step: 93808, loss:  0.023757282645\n",
      "training step: 93849, loss:  0.003776642960\n",
      "training step: 93890, loss:  0.003543942003\n",
      "training step: 93931, loss:  0.022392421961\n",
      "training step: 93972, loss:  0.002975780051\n",
      "training step: 94013, loss:  0.005003615282\n",
      "training step: 94054, loss:  0.002396283671\n",
      "training step: 94095, loss:  0.006285338197\n",
      "training step: 94136, loss:  0.003284437582\n",
      "training step: 94177, loss:  0.002975780051\n",
      "training step: 94218, loss:  0.007341358811\n",
      "training step: 94259, loss:  0.002930165734\n",
      "training step: 94300, loss:  0.003473303979\n",
      "training step: 94341, loss:  0.002386306180\n",
      "training step: 94382, loss:  0.025568028912\n",
      "training step: 94423, loss:  0.003135976614\n",
      "training step: 94464, loss:  0.003692979924\n",
      "training step: 94505, loss:  0.003284437582\n",
      "training step: 94546, loss:  0.003467082279\n",
      "training step: 94587, loss:  0.003866109066\n",
      "training step: 94628, loss:  0.022392421961\n",
      "training step: 94669, loss:  0.005715856329\n",
      "training step: 94710, loss:  0.007752379868\n",
      "training step: 94751, loss:  0.003467082279\n",
      "training step: 94792, loss:  0.007341358811\n",
      "training step: 94833, loss:  0.018698843196\n",
      "training step: 94874, loss:  0.023757282645\n",
      "training step: 94915, loss:  0.024754593149\n",
      "training step: 94956, loss:  0.003543942003\n",
      "training step: 94997, loss:  0.003543942003\n",
      "training step: 95038, loss:  0.003135976614\n",
      "training step: 95079, loss:  0.007752379868\n",
      "training step: 95120, loss:  0.009004401974\n",
      "training step: 95161, loss:  0.009004401974\n",
      "training step: 95202, loss:  0.014145905152\n",
      "training step: 95243, loss:  0.003776642960\n",
      "training step: 95284, loss:  0.007341358811\n",
      "training step: 95325, loss:  0.003011468332\n",
      "training step: 95366, loss:  0.007877553813\n",
      "training step: 95407, loss:  0.025363046676\n",
      "training step: 95448, loss:  0.006285338197\n",
      "training step: 95489, loss:  0.006285338197\n",
      "training step: 95530, loss:  0.003473303979\n",
      "training step: 95571, loss:  0.004080093466\n",
      "training step: 95612, loss:  0.002930165734\n",
      "training step: 95653, loss:  0.003692979924\n",
      "training step: 95694, loss:  0.003543942003\n",
      "training step: 95735, loss:  0.003880497534\n",
      "training step: 95776, loss:  0.003692979924\n",
      "training step: 95817, loss:  0.003011468332\n",
      "training step: 95858, loss:  0.004080093466\n",
      "training step: 95899, loss:  0.011776376516\n",
      "training step: 95940, loss:  0.003135976614\n",
      "training step: 95981, loss:  0.007341358811\n",
      "training step: 96022, loss:  0.002930165734\n",
      "training step: 96063, loss:  0.003638759954\n",
      "training step: 96104, loss:  0.003002761398\n",
      "training step: 96145, loss:  0.003120336682\n",
      "training step: 96186, loss:  0.003638759954\n",
      "training step: 96227, loss:  0.007752379868\n",
      "training step: 96268, loss:  0.024754593149\n",
      "training step: 96309, loss:  0.016485802829\n",
      "training step: 96350, loss:  0.003776642960\n",
      "training step: 96391, loss:  0.016485802829\n",
      "training step: 96432, loss:  0.007877553813\n",
      "training step: 96473, loss:  0.003808671376\n",
      "training step: 96514, loss:  0.003002761398\n",
      "training step: 96555, loss:  0.003543942003\n",
      "training step: 96596, loss:  0.009004401974\n",
      "training step: 96637, loss:  0.003467082279\n",
      "training step: 96678, loss:  0.002930165734\n",
      "training step: 96719, loss:  0.006285338197\n",
      "training step: 96760, loss:  0.003776642960\n",
      "training step: 96801, loss:  0.016485802829\n",
      "training step: 96842, loss:  0.003986798227\n",
      "training step: 96883, loss:  0.003986798227\n",
      "training step: 96924, loss:  0.002930165734\n",
      "training step: 96965, loss:  0.003467082279\n",
      "training step: 97006, loss:  0.011776376516\n",
      "training step: 97047, loss:  0.003284437582\n",
      "training step: 97088, loss:  0.003543942003\n",
      "training step: 97129, loss:  0.002386306180\n",
      "training step: 97170, loss:  0.003002761398\n",
      "training step: 97211, loss:  0.018698843196\n",
      "training step: 97252, loss:  0.003866109066\n",
      "training step: 97293, loss:  0.003638759954\n",
      "training step: 97334, loss:  0.003638759954\n",
      "training step: 97375, loss:  0.025568028912\n",
      "training step: 97416, loss:  0.003120336682\n",
      "training step: 97457, loss:  0.020691098645\n",
      "training step: 97498, loss:  0.004080093466\n",
      "training step: 97539, loss:  0.022392421961\n",
      "training step: 97580, loss:  0.025568028912\n",
      "training step: 97621, loss:  0.009473215789\n",
      "training step: 97662, loss:  0.003900909098\n",
      "training step: 97703, loss:  0.022392421961\n",
      "training step: 97744, loss:  0.003692979924\n",
      "training step: 97785, loss:  0.003638759954\n",
      "training step: 97826, loss:  0.024754593149\n",
      "training step: 97867, loss:  0.025568028912\n",
      "training step: 97908, loss:  0.003467082279\n",
      "training step: 97949, loss:  0.003120336682\n",
      "training step: 97990, loss:  0.003900909098\n",
      "training step: 98031, loss:  0.003880497534\n",
      "training step: 98072, loss:  0.003011468332\n",
      "training step: 98113, loss:  0.003543942003\n",
      "training step: 98154, loss:  0.016485802829\n",
      "training step: 98195, loss:  0.005715856329\n",
      "training step: 98236, loss:  0.003880497534\n",
      "training step: 98277, loss:  0.025568028912\n",
      "training step: 98318, loss:  0.003467082279\n",
      "training step: 98359, loss:  0.007752379868\n",
      "training step: 98400, loss:  0.003135976614\n",
      "training step: 98441, loss:  0.007752379868\n",
      "training step: 98482, loss:  0.003002761398\n",
      "training step: 98523, loss:  0.005003615282\n",
      "training step: 98564, loss:  0.007341358811\n",
      "training step: 98605, loss:  0.003880497534\n",
      "training step: 98646, loss:  0.024754593149\n",
      "training step: 98687, loss:  0.018698843196\n",
      "training step: 98728, loss:  0.003692979924\n",
      "training step: 98769, loss:  0.025568028912\n",
      "training step: 98810, loss:  0.003120336682\n",
      "training step: 98851, loss:  0.003467082279\n",
      "training step: 98892, loss:  0.011776376516\n",
      "training step: 98933, loss:  0.003900909098\n",
      "training step: 98974, loss:  0.014145905152\n",
      "training step: 99015, loss:  0.025568028912\n",
      "training step: 99056, loss:  0.005485921632\n",
      "training step: 99097, loss:  0.003011468332\n",
      "training step: 99138, loss:  0.003011468332\n",
      "training step: 99179, loss:  0.005715856329\n",
      "training step: 99220, loss:  0.004080093466\n",
      "training step: 99261, loss:  0.003808671376\n",
      "training step: 99302, loss:  0.003543942003\n",
      "training step: 99343, loss:  0.003120336682\n",
      "training step: 99384, loss:  0.009004401974\n",
      "training step: 99425, loss:  0.003473303979\n",
      "training step: 99466, loss:  0.005715856329\n",
      "training step: 99507, loss:  0.005003615282\n",
      "training step: 99548, loss:  0.007877553813\n",
      "training step: 99589, loss:  0.002386306180\n",
      "training step: 99630, loss:  0.007752379868\n",
      "training step: 99671, loss:  0.003284437582\n",
      "training step: 99712, loss:  0.002396283671\n",
      "training step: 99753, loss:  0.025363046676\n",
      "training step: 99794, loss:  0.003880497534\n",
      "training step: 99835, loss:  0.009004401974\n",
      "training step: 99876, loss:  0.024754593149\n",
      "training step: 99917, loss:  0.003866109066\n",
      "training step: 99958, loss:  0.007341358811\n",
      "training step: 99999, loss:  0.003135976614\n",
      "training step: 100040, loss:  0.007877553813\n",
      "training step: 100081, loss:  0.003467082279\n",
      "training step: 100122, loss:  0.005485921632\n",
      "training step: 100163, loss:  0.003692979924\n",
      "training step: 100204, loss:  0.005715856329\n",
      "training step: 100245, loss:  0.007752379868\n",
      "training step: 100286, loss:  0.003866109066\n",
      "training step: 100327, loss:  0.024754593149\n",
      "training step: 100368, loss:  0.011776376516\n",
      "training step: 100409, loss:  0.003900909098\n",
      "training step: 100450, loss:  0.025363046676\n",
      "training step: 100491, loss:  0.007877553813\n",
      "training step: 100532, loss:  0.014145905152\n",
      "training step: 100573, loss:  0.003638759954\n",
      "training step: 100614, loss:  0.006285338197\n",
      "training step: 100655, loss:  0.005003615282\n",
      "training step: 100696, loss:  0.006285338197\n",
      "training step: 100737, loss:  0.003986798227\n",
      "training step: 100778, loss:  0.003011468332\n",
      "training step: 100819, loss:  0.016485802829\n",
      "training step: 100860, loss:  0.004080093466\n",
      "training step: 100901, loss:  0.014145905152\n",
      "training step: 100942, loss:  0.003543942003\n",
      "training step: 100983, loss:  0.016485802829\n",
      "training step: 101024, loss:  0.005715856329\n",
      "training step: 101065, loss:  0.003467082279\n",
      "training step: 101106, loss:  0.002396283671\n",
      "training step: 101147, loss:  0.005715856329\n",
      "training step: 101188, loss:  0.003284437582\n",
      "training step: 101229, loss:  0.003135976614\n",
      "training step: 101270, loss:  0.003986798227\n",
      "training step: 101311, loss:  0.009473215789\n",
      "training step: 101352, loss:  0.018698843196\n",
      "training step: 101393, loss:  0.002396283671\n",
      "training step: 101434, loss:  0.003120336682\n",
      "training step: 101475, loss:  0.020691098645\n",
      "training step: 101516, loss:  0.007341358811\n",
      "training step: 101557, loss:  0.003880497534\n",
      "training step: 101598, loss:  0.003002761398\n",
      "training step: 101639, loss:  0.009004401974\n",
      "training step: 101680, loss:  0.009473215789\n",
      "training step: 101721, loss:  0.006285338197\n",
      "training step: 101762, loss:  0.007752379868\n",
      "training step: 101803, loss:  0.003808671376\n",
      "training step: 101844, loss:  0.003986798227\n",
      "training step: 101885, loss:  0.018698843196\n",
      "training step: 101926, loss:  0.018698843196\n",
      "training step: 101967, loss:  0.003692979924\n",
      "training step: 102008, loss:  0.009473215789\n",
      "training step: 102049, loss:  0.003692979924\n",
      "training step: 102090, loss:  0.003776642960\n",
      "training step: 102131, loss:  0.003543942003\n",
      "training step: 102172, loss:  0.024754593149\n",
      "training step: 102213, loss:  0.009004401974\n",
      "training step: 102254, loss:  0.003135976614\n",
      "training step: 102295, loss:  0.003900909098\n",
      "training step: 102336, loss:  0.025568028912\n",
      "training step: 102377, loss:  0.003473303979\n",
      "training step: 102418, loss:  0.003880497534\n",
      "training step: 102459, loss:  0.022392421961\n",
      "training step: 102500, loss:  0.002930165734\n",
      "training step: 102541, loss:  0.003467082279\n",
      "training step: 102582, loss:  0.009004401974\n",
      "training step: 102623, loss:  0.004087368958\n",
      "training step: 102664, loss:  0.003808671376\n",
      "training step: 102705, loss:  0.003900909098\n",
      "training step: 102746, loss:  0.007341358811\n",
      "training step: 102787, loss:  0.018698843196\n",
      "training step: 102828, loss:  0.007752379868\n",
      "training step: 102869, loss:  0.005003615282\n",
      "training step: 102910, loss:  0.003473303979\n",
      "training step: 102951, loss:  0.003638759954\n",
      "training step: 102992, loss:  0.014145905152\n",
      "training step: 103033, loss:  0.005485921632\n",
      "training step: 103074, loss:  0.003900909098\n",
      "training step: 103115, loss:  0.009004401974\n",
      "training step: 103156, loss:  0.003002761398\n",
      "training step: 103197, loss:  0.003011468332\n",
      "training step: 103238, loss:  0.003880497534\n",
      "training step: 103279, loss:  0.007341358811\n",
      "training step: 103320, loss:  0.002386306180\n",
      "training step: 103361, loss:  0.023757282645\n",
      "training step: 103402, loss:  0.003692979924\n",
      "training step: 103443, loss:  0.003135976614\n",
      "training step: 103484, loss:  0.005003615282\n",
      "training step: 103525, loss:  0.003638759954\n",
      "training step: 103566, loss:  0.007752379868\n",
      "training step: 103607, loss:  0.025363046676\n",
      "training step: 103648, loss:  0.003473303979\n",
      "training step: 103689, loss:  0.003986798227\n",
      "training step: 103730, loss:  0.003808671376\n",
      "training step: 103771, loss:  0.003866109066\n",
      "training step: 103812, loss:  0.003638759954\n",
      "training step: 103853, loss:  0.003808671376\n",
      "training step: 103894, loss:  0.022392421961\n",
      "training step: 103935, loss:  0.007877553813\n",
      "training step: 103976, loss:  0.005715856329\n",
      "training step: 104017, loss:  0.025568028912\n",
      "training step: 104058, loss:  0.003692979924\n",
      "training step: 104099, loss:  0.011776376516\n",
      "training step: 104140, loss:  0.016485802829\n",
      "training step: 104181, loss:  0.002930165734\n",
      "training step: 104222, loss:  0.018698843196\n",
      "training step: 104263, loss:  0.025363046676\n",
      "training step: 104304, loss:  0.003638759954\n",
      "training step: 104345, loss:  0.014145905152\n",
      "training step: 104386, loss:  0.022392421961\n",
      "training step: 104427, loss:  0.009473215789\n",
      "training step: 104468, loss:  0.007752379868\n",
      "training step: 104509, loss:  0.003467082279\n",
      "training step: 104550, loss:  0.006285338197\n",
      "training step: 104591, loss:  0.005715856329\n",
      "training step: 104632, loss:  0.003638759954\n",
      "training step: 104673, loss:  0.003866109066\n",
      "training step: 104714, loss:  0.002975780051\n",
      "training step: 104755, loss:  0.003866109066\n",
      "training step: 104796, loss:  0.002396283671\n",
      "training step: 104837, loss:  0.018698843196\n",
      "training step: 104878, loss:  0.003135976614\n",
      "training step: 104919, loss:  0.020691098645\n",
      "training step: 104960, loss:  0.014145905152\n",
      "training step: 105001, loss:  0.003473303979\n",
      "training step: 105042, loss:  0.002396283671\n",
      "training step: 105083, loss:  0.018698843196\n",
      "training step: 105124, loss:  0.009004401974\n",
      "training step: 105165, loss:  0.003808671376\n",
      "training step: 105206, loss:  0.003543942003\n",
      "training step: 105247, loss:  0.003808671376\n",
      "training step: 105288, loss:  0.007341358811\n",
      "training step: 105329, loss:  0.005485921632\n",
      "training step: 105370, loss:  0.025568028912\n",
      "training step: 105411, loss:  0.003808671376\n",
      "training step: 105452, loss:  0.006285338197\n",
      "training step: 105493, loss:  0.002396283671\n",
      "training step: 105534, loss:  0.002975780051\n",
      "training step: 105575, loss:  0.003776642960\n",
      "training step: 105616, loss:  0.016485802829\n",
      "training step: 105657, loss:  0.002396283671\n",
      "training step: 105698, loss:  0.003135976614\n",
      "training step: 105739, loss:  0.003808671376\n",
      "training step: 105780, loss:  0.003011468332\n",
      "training step: 105821, loss:  0.005003615282\n",
      "training step: 105862, loss:  0.007877553813\n",
      "training step: 105903, loss:  0.005003615282\n",
      "training step: 105944, loss:  0.003011468332\n",
      "training step: 105985, loss:  0.002396283671\n",
      "training step: 106026, loss:  0.002930165734\n",
      "training step: 106067, loss:  0.002930165734\n",
      "training step: 106108, loss:  0.003002761398\n",
      "training step: 106149, loss:  0.003866109066\n",
      "training step: 106190, loss:  0.002930165734\n",
      "training step: 106231, loss:  0.004087368958\n",
      "training step: 106272, loss:  0.003776642960\n",
      "training step: 106313, loss:  0.018698843196\n",
      "training step: 106354, loss:  0.003900909098\n",
      "training step: 106395, loss:  0.002396283671\n",
      "training step: 106436, loss:  0.003638759954\n",
      "training step: 106477, loss:  0.003808671376\n",
      "training step: 106518, loss:  0.022392421961\n",
      "training step: 106559, loss:  0.002975780051\n",
      "training step: 106600, loss:  0.003986798227\n",
      "training step: 106641, loss:  0.023757282645\n",
      "training step: 106682, loss:  0.002396283671\n",
      "training step: 106723, loss:  0.009004401974\n",
      "training step: 106764, loss:  0.003776642960\n",
      "training step: 106805, loss:  0.003986798227\n",
      "training step: 106846, loss:  0.003135976614\n",
      "training step: 106887, loss:  0.005485921632\n",
      "training step: 106928, loss:  0.009004401974\n",
      "training step: 106969, loss:  0.003638759954\n",
      "training step: 107010, loss:  0.004080093466\n",
      "training step: 107051, loss:  0.009004401974\n",
      "training step: 107092, loss:  0.003900909098\n",
      "training step: 107133, loss:  0.022392421961\n",
      "training step: 107174, loss:  0.003467082279\n",
      "training step: 107215, loss:  0.003808671376\n",
      "training step: 107256, loss:  0.006285338197\n",
      "training step: 107297, loss:  0.020691098645\n",
      "training step: 107338, loss:  0.003543942003\n",
      "training step: 107379, loss:  0.005485921632\n",
      "training step: 107420, loss:  0.020691098645\n",
      "training step: 107461, loss:  0.024754593149\n",
      "training step: 107502, loss:  0.020691098645\n",
      "training step: 107543, loss:  0.023757282645\n",
      "training step: 107584, loss:  0.024754593149\n",
      "training step: 107625, loss:  0.005715856329\n",
      "training step: 107666, loss:  0.002386306180\n",
      "training step: 107707, loss:  0.003900909098\n",
      "training step: 107748, loss:  0.025568028912\n",
      "training step: 107789, loss:  0.022392421961\n",
      "training step: 107830, loss:  0.003473303979\n",
      "training step: 107871, loss:  0.007877553813\n",
      "training step: 107912, loss:  0.009473215789\n",
      "training step: 107953, loss:  0.025363046676\n",
      "training step: 107994, loss:  0.007341358811\n",
      "training step: 108035, loss:  0.002386306180\n",
      "training step: 108076, loss:  0.002386306180\n",
      "training step: 108117, loss:  0.003808671376\n",
      "training step: 108158, loss:  0.007752379868\n",
      "training step: 108199, loss:  0.003866109066\n",
      "training step: 108240, loss:  0.002386306180\n",
      "training step: 108281, loss:  0.003866109066\n",
      "training step: 108322, loss:  0.003284437582\n",
      "training step: 108363, loss:  0.007752379868\n",
      "training step: 108404, loss:  0.003638759954\n",
      "training step: 108445, loss:  0.003543942003\n",
      "training step: 108486, loss:  0.004087368958\n",
      "training step: 108527, loss:  0.024754593149\n",
      "training step: 108568, loss:  0.003866109066\n",
      "training step: 108609, loss:  0.007341358811\n",
      "training step: 108650, loss:  0.005485921632\n",
      "training step: 108691, loss:  0.016485802829\n",
      "training step: 108732, loss:  0.005003615282\n",
      "training step: 108773, loss:  0.020691098645\n",
      "training step: 108814, loss:  0.022392421961\n",
      "training step: 108855, loss:  0.003467082279\n",
      "training step: 108896, loss:  0.003135976614\n",
      "training step: 108937, loss:  0.002930165734\n",
      "training step: 108978, loss:  0.020691098645\n",
      "training step: 109019, loss:  0.003692979924\n",
      "training step: 109060, loss:  0.003467082279\n",
      "training step: 109101, loss:  0.002930165734\n",
      "training step: 109142, loss:  0.014145905152\n",
      "training step: 109183, loss:  0.002386306180\n",
      "training step: 109224, loss:  0.016485802829\n",
      "training step: 109265, loss:  0.009473215789\n",
      "training step: 109306, loss:  0.003986798227\n",
      "training step: 109347, loss:  0.018698843196\n",
      "training step: 109388, loss:  0.018698843196\n",
      "training step: 109429, loss:  0.025363046676\n",
      "training step: 109470, loss:  0.004080093466\n",
      "training step: 109511, loss:  0.018698843196\n",
      "training step: 109552, loss:  0.025363046676\n",
      "training step: 109593, loss:  0.022392421961\n",
      "training step: 109634, loss:  0.002975780051\n",
      "training step: 109675, loss:  0.003120336682\n",
      "training step: 109716, loss:  0.007341358811\n",
      "training step: 109757, loss:  0.002396283671\n",
      "training step: 109798, loss:  0.003880497534\n",
      "training step: 109839, loss:  0.004080093466\n",
      "training step: 109880, loss:  0.005003615282\n",
      "training step: 109921, loss:  0.003808671376\n",
      "training step: 109962, loss:  0.003880497534\n",
      "training step: 110003, loss:  0.025568028912\n",
      "training step: 110044, loss:  0.007877553813\n",
      "training step: 110085, loss:  0.020691098645\n",
      "training step: 110126, loss:  0.003900909098\n",
      "training step: 110167, loss:  0.025363046676\n",
      "training step: 110208, loss:  0.007341358811\n",
      "training step: 110249, loss:  0.003776642960\n",
      "training step: 110290, loss:  0.025568028912\n",
      "training step: 110331, loss:  0.003011468332\n",
      "training step: 110372, loss:  0.018698843196\n",
      "training step: 110413, loss:  0.024754593149\n",
      "training step: 110454, loss:  0.020691098645\n",
      "training step: 110495, loss:  0.020691098645\n",
      "training step: 110536, loss:  0.025568028912\n",
      "training step: 110577, loss:  0.003467082279\n",
      "training step: 110618, loss:  0.025568028912\n",
      "training step: 110659, loss:  0.003467082279\n",
      "training step: 110700, loss:  0.003467082279\n",
      "training step: 110741, loss:  0.005003615282\n",
      "training step: 110782, loss:  0.024754593149\n",
      "training step: 110823, loss:  0.018698843196\n",
      "training step: 110864, loss:  0.003120336682\n",
      "training step: 110905, loss:  0.007341358811\n",
      "training step: 110946, loss:  0.009473215789\n",
      "training step: 110987, loss:  0.025568028912\n",
      "training step: 111028, loss:  0.002930165734\n",
      "training step: 111069, loss:  0.005715856329\n",
      "training step: 111110, loss:  0.002386306180\n",
      "training step: 111151, loss:  0.004087368958\n",
      "training step: 111192, loss:  0.009004401974\n",
      "training step: 111233, loss:  0.007877553813\n",
      "training step: 111274, loss:  0.005715856329\n",
      "training step: 111315, loss:  0.003135976614\n",
      "training step: 111356, loss:  0.003866109066\n",
      "training step: 111397, loss:  0.003120336682\n",
      "training step: 111438, loss:  0.005715856329\n",
      "training step: 111479, loss:  0.025363046676\n",
      "training step: 111520, loss:  0.009473215789\n",
      "training step: 111561, loss:  0.003638759954\n",
      "training step: 111602, loss:  0.002930165734\n",
      "training step: 111643, loss:  0.004087368958\n",
      "training step: 111684, loss:  0.020691098645\n",
      "training step: 111725, loss:  0.003284437582\n",
      "training step: 111766, loss:  0.003135976614\n",
      "training step: 111807, loss:  0.014145905152\n",
      "training step: 111848, loss:  0.003866109066\n",
      "training step: 111889, loss:  0.006285338197\n",
      "training step: 111930, loss:  0.003467082279\n",
      "training step: 111971, loss:  0.003638759954\n",
      "training step: 112012, loss:  0.022392421961\n",
      "training step: 112053, loss:  0.002396283671\n",
      "training step: 112094, loss:  0.003473303979\n",
      "training step: 112135, loss:  0.003543942003\n",
      "training step: 112176, loss:  0.003692979924\n",
      "training step: 112217, loss:  0.009004401974\n",
      "training step: 112258, loss:  0.009004401974\n",
      "training step: 112299, loss:  0.002975780051\n",
      "training step: 112340, loss:  0.003467082279\n",
      "training step: 112381, loss:  0.002396283671\n",
      "training step: 112422, loss:  0.002975780051\n",
      "training step: 112463, loss:  0.003880497534\n",
      "training step: 112504, loss:  0.005485921632\n",
      "training step: 112545, loss:  0.003284437582\n",
      "training step: 112586, loss:  0.014145905152\n",
      "training step: 112627, loss:  0.025363046676\n",
      "training step: 112668, loss:  0.007752379868\n",
      "training step: 112709, loss:  0.003900909098\n",
      "training step: 112750, loss:  0.007877553813\n",
      "training step: 112791, loss:  0.003135976614\n",
      "training step: 112832, loss:  0.018698843196\n",
      "training step: 112873, loss:  0.003776642960\n",
      "training step: 112914, loss:  0.005715856329\n",
      "training step: 112955, loss:  0.003011468332\n",
      "training step: 112996, loss:  0.002396283671\n",
      "training step: 113037, loss:  0.003808671376\n",
      "training step: 113078, loss:  0.003880497534\n",
      "training step: 113119, loss:  0.024754593149\n",
      "training step: 113160, loss:  0.003900909098\n",
      "training step: 113201, loss:  0.005003615282\n",
      "training step: 113242, loss:  0.007752379868\n",
      "training step: 113283, loss:  0.004080093466\n",
      "training step: 113324, loss:  0.003986798227\n",
      "training step: 113365, loss:  0.003473303979\n",
      "training step: 113406, loss:  0.025363046676\n",
      "training step: 113447, loss:  0.003866109066\n",
      "training step: 113488, loss:  0.020691098645\n",
      "training step: 113529, loss:  0.003284437582\n",
      "training step: 113570, loss:  0.003808671376\n",
      "training step: 113611, loss:  0.002386306180\n",
      "training step: 113652, loss:  0.024754593149\n",
      "training step: 113693, loss:  0.003543942003\n",
      "training step: 113734, loss:  0.003900909098\n",
      "training step: 113775, loss:  0.003002761398\n",
      "training step: 113816, loss:  0.004080093466\n",
      "training step: 113857, loss:  0.011776376516\n",
      "training step: 113898, loss:  0.014145905152\n",
      "training step: 113939, loss:  0.002975780051\n",
      "training step: 113980, loss:  0.020691098645\n",
      "training step: 114021, loss:  0.003638759954\n",
      "training step: 114062, loss:  0.003638759954\n",
      "training step: 114103, loss:  0.006285338197\n",
      "training step: 114144, loss:  0.003986798227\n",
      "training step: 114185, loss:  0.002386306180\n",
      "training step: 114226, loss:  0.003776642960\n",
      "training step: 114267, loss:  0.003638759954\n",
      "training step: 114308, loss:  0.003120336682\n",
      "training step: 114349, loss:  0.003135976614\n",
      "training step: 114390, loss:  0.009004401974\n",
      "training step: 114431, loss:  0.009004401974\n",
      "training step: 114472, loss:  0.002386306180\n",
      "training step: 114513, loss:  0.005485921632\n",
      "training step: 114554, loss:  0.003692979924\n",
      "training step: 114595, loss:  0.023757282645\n",
      "training step: 114636, loss:  0.007341358811\n",
      "training step: 114677, loss:  0.007752379868\n",
      "training step: 114718, loss:  0.003986798227\n",
      "training step: 114759, loss:  0.003135976614\n",
      "training step: 114800, loss:  0.003866109066\n",
      "training step: 114841, loss:  0.002396283671\n",
      "training step: 114882, loss:  0.025363046676\n",
      "training step: 114923, loss:  0.009004401974\n",
      "training step: 114964, loss:  0.016485802829\n",
      "training step: 115005, loss:  0.005485921632\n",
      "training step: 115046, loss:  0.003467082279\n",
      "training step: 115087, loss:  0.003638759954\n",
      "training step: 115128, loss:  0.022392421961\n",
      "training step: 115169, loss:  0.002396283671\n",
      "training step: 115210, loss:  0.003808671376\n",
      "training step: 115251, loss:  0.003002761398\n",
      "training step: 115292, loss:  0.007752379868\n",
      "training step: 115333, loss:  0.009004401974\n",
      "training step: 115374, loss:  0.011776376516\n",
      "training step: 115415, loss:  0.020691098645\n",
      "training step: 115456, loss:  0.005715856329\n",
      "training step: 115497, loss:  0.002396283671\n",
      "training step: 115538, loss:  0.003284437582\n",
      "training step: 115579, loss:  0.025568028912\n",
      "training step: 115620, loss:  0.002975780051\n",
      "training step: 115661, loss:  0.005485921632\n",
      "training step: 115702, loss:  0.007341358811\n",
      "training step: 115743, loss:  0.002975780051\n",
      "training step: 115784, loss:  0.025363046676\n",
      "training step: 115825, loss:  0.003120336682\n",
      "training step: 115866, loss:  0.005715856329\n",
      "training step: 115907, loss:  0.003880497534\n",
      "training step: 115948, loss:  0.003011468332\n",
      "training step: 115989, loss:  0.018698843196\n",
      "training step: 116030, loss:  0.003776642960\n",
      "training step: 116071, loss:  0.004087368958\n",
      "training step: 116112, loss:  0.005715856329\n",
      "training step: 116153, loss:  0.007752379868\n",
      "training step: 116194, loss:  0.005003615282\n",
      "training step: 116235, loss:  0.005715856329\n",
      "training step: 116276, loss:  0.005485921632\n",
      "training step: 116317, loss:  0.023757282645\n",
      "training step: 116358, loss:  0.003467082279\n",
      "training step: 116399, loss:  0.020691098645\n",
      "training step: 116440, loss:  0.009473215789\n",
      "training step: 116481, loss:  0.003986798227\n",
      "training step: 116522, loss:  0.018698843196\n",
      "training step: 116563, loss:  0.003543942003\n",
      "training step: 116604, loss:  0.002930165734\n",
      "training step: 116645, loss:  0.003638759954\n",
      "training step: 116686, loss:  0.003900909098\n",
      "training step: 116727, loss:  0.018698843196\n",
      "training step: 116768, loss:  0.003284437582\n",
      "training step: 116809, loss:  0.007877553813\n",
      "training step: 116850, loss:  0.003135976614\n",
      "training step: 116891, loss:  0.003473303979\n",
      "training step: 116932, loss:  0.005715856329\n",
      "training step: 116973, loss:  0.006285338197\n",
      "training step: 117014, loss:  0.007752379868\n",
      "training step: 117055, loss:  0.025568028912\n",
      "training step: 117096, loss:  0.003638759954\n",
      "training step: 117137, loss:  0.002386306180\n",
      "training step: 117178, loss:  0.003002761398\n",
      "training step: 117219, loss:  0.007752379868\n",
      "training step: 117260, loss:  0.003692979924\n",
      "training step: 117301, loss:  0.007752379868\n",
      "training step: 117342, loss:  0.007877553813\n",
      "training step: 117383, loss:  0.003808671376\n",
      "training step: 117424, loss:  0.003467082279\n",
      "training step: 117465, loss:  0.003638759954\n",
      "training step: 117506, loss:  0.003986798227\n",
      "training step: 117547, loss:  0.025568028912\n",
      "training step: 117588, loss:  0.003543942003\n",
      "training step: 117629, loss:  0.003467082279\n",
      "training step: 117670, loss:  0.004080093466\n",
      "training step: 117711, loss:  0.002930165734\n",
      "training step: 117752, loss:  0.011776376516\n",
      "training step: 117793, loss:  0.025363046676\n",
      "training step: 117834, loss:  0.003880497534\n",
      "training step: 117875, loss:  0.003776642960\n",
      "training step: 117916, loss:  0.023757282645\n",
      "training step: 117957, loss:  0.003880497534\n",
      "training step: 117998, loss:  0.003866109066\n",
      "training step: 118039, loss:  0.004087368958\n",
      "training step: 118080, loss:  0.007752379868\n",
      "training step: 118121, loss:  0.025568028912\n",
      "training step: 118162, loss:  0.004080093466\n",
      "training step: 118203, loss:  0.003776642960\n",
      "training step: 118244, loss:  0.011776376516\n",
      "training step: 118285, loss:  0.024754593149\n",
      "training step: 118326, loss:  0.003120336682\n",
      "training step: 118367, loss:  0.025363046676\n",
      "training step: 118408, loss:  0.005485921632\n",
      "training step: 118449, loss:  0.003808671376\n",
      "training step: 118490, loss:  0.018698843196\n",
      "training step: 118531, loss:  0.005003615282\n",
      "training step: 118572, loss:  0.018698843196\n",
      "training step: 118613, loss:  0.003467082279\n",
      "training step: 118654, loss:  0.007752379868\n",
      "training step: 118695, loss:  0.007341358811\n",
      "training step: 118736, loss:  0.003638759954\n",
      "training step: 118777, loss:  0.016485802829\n",
      "training step: 118818, loss:  0.003986798227\n",
      "training step: 118859, loss:  0.023757282645\n",
      "training step: 118900, loss:  0.003002761398\n",
      "training step: 118941, loss:  0.003986798227\n",
      "training step: 118982, loss:  0.003808671376\n",
      "training step: 119023, loss:  0.003284437582\n",
      "training step: 119064, loss:  0.003866109066\n",
      "training step: 119105, loss:  0.002930165734\n",
      "training step: 119146, loss:  0.003120336682\n",
      "training step: 119187, loss:  0.003467082279\n",
      "training step: 119228, loss:  0.003135976614\n",
      "training step: 119269, loss:  0.023757282645\n",
      "training step: 119310, loss:  0.003120336682\n",
      "training step: 119351, loss:  0.007877553813\n",
      "training step: 119392, loss:  0.003692979924\n",
      "training step: 119433, loss:  0.003638759954\n",
      "training step: 119474, loss:  0.005003615282\n",
      "training step: 119515, loss:  0.003986798227\n",
      "training step: 119556, loss:  0.005003615282\n",
      "training step: 119597, loss:  0.009004401974\n",
      "training step: 119638, loss:  0.002930165734\n",
      "training step: 119679, loss:  0.003002761398\n",
      "training step: 119720, loss:  0.003284437582\n",
      "training step: 119761, loss:  0.003692979924\n",
      "training step: 119802, loss:  0.004080093466\n",
      "training step: 119843, loss:  0.003284437582\n",
      "training step: 119884, loss:  0.007752379868\n",
      "training step: 119925, loss:  0.003120336682\n",
      "training step: 119966, loss:  0.005715856329\n",
      "training step: 120007, loss:  0.003002761398\n",
      "training step: 120048, loss:  0.003866109066\n",
      "training step: 120089, loss:  0.003880497534\n",
      "training step: 120130, loss:  0.006285338197\n",
      "training step: 120171, loss:  0.007752379868\n",
      "training step: 120212, loss:  0.016485802829\n",
      "training step: 120253, loss:  0.003866109066\n",
      "training step: 120294, loss:  0.003776642960\n",
      "training step: 120335, loss:  0.003866109066\n",
      "training step: 120376, loss:  0.007877553813\n",
      "training step: 120417, loss:  0.003002761398\n",
      "training step: 120458, loss:  0.014145905152\n",
      "training step: 120499, loss:  0.003900909098\n",
      "training step: 120540, loss:  0.003011468332\n",
      "training step: 120581, loss:  0.002975780051\n",
      "training step: 120622, loss:  0.009004401974\n",
      "training step: 120663, loss:  0.003002761398\n",
      "training step: 120704, loss:  0.003011468332\n",
      "training step: 120745, loss:  0.004080093466\n",
      "training step: 120786, loss:  0.007752379868\n",
      "training step: 120827, loss:  0.003543942003\n",
      "training step: 120868, loss:  0.003002761398\n",
      "training step: 120909, loss:  0.002930165734\n",
      "training step: 120950, loss:  0.002975780051\n",
      "training step: 120991, loss:  0.003900909098\n",
      "training step: 121032, loss:  0.004087368958\n",
      "training step: 121073, loss:  0.003808671376\n",
      "training step: 121114, loss:  0.003866109066\n",
      "training step: 121155, loss:  0.009473215789\n",
      "training step: 121196, loss:  0.004080093466\n",
      "training step: 121237, loss:  0.002930165734\n",
      "training step: 121278, loss:  0.002396283671\n",
      "training step: 121319, loss:  0.003002761398\n",
      "training step: 121360, loss:  0.022392421961\n",
      "training step: 121401, loss:  0.003866109066\n",
      "training step: 121442, loss:  0.003900909098\n",
      "training step: 121483, loss:  0.025363046676\n",
      "training step: 121524, loss:  0.024754593149\n",
      "training step: 121565, loss:  0.022392421961\n",
      "training step: 121606, loss:  0.004080093466\n",
      "training step: 121647, loss:  0.025363046676\n",
      "training step: 121688, loss:  0.004087368958\n",
      "training step: 121729, loss:  0.007752379868\n",
      "training step: 121770, loss:  0.025363046676\n",
      "training step: 121811, loss:  0.003543942003\n",
      "training step: 121852, loss:  0.003135976614\n",
      "training step: 121893, loss:  0.003135976614\n",
      "training step: 121934, loss:  0.003638759954\n",
      "training step: 121975, loss:  0.003776642960\n",
      "training step: 122016, loss:  0.004080093466\n",
      "training step: 122057, loss:  0.002975780051\n",
      "training step: 122098, loss:  0.009473215789\n",
      "training step: 122139, loss:  0.003692979924\n",
      "training step: 122180, loss:  0.009004401974\n",
      "training step: 122221, loss:  0.003120336682\n",
      "training step: 122262, loss:  0.003808671376\n",
      "training step: 122303, loss:  0.020691098645\n",
      "training step: 122344, loss:  0.011776376516\n",
      "training step: 122385, loss:  0.007752379868\n",
      "training step: 122426, loss:  0.025568028912\n",
      "training step: 122467, loss:  0.016485802829\n",
      "training step: 122508, loss:  0.003120336682\n",
      "training step: 122549, loss:  0.014145905152\n",
      "training step: 122590, loss:  0.009004401974\n",
      "training step: 122631, loss:  0.023757282645\n",
      "training step: 122672, loss:  0.003638759954\n",
      "training step: 122713, loss:  0.011776376516\n",
      "training step: 122754, loss:  0.003986798227\n",
      "training step: 122795, loss:  0.025568028912\n",
      "training step: 122836, loss:  0.014145905152\n",
      "training step: 122877, loss:  0.009004401974\n",
      "training step: 122918, loss:  0.003543942003\n",
      "training step: 122959, loss:  0.023757282645\n",
      "training step: 123000, loss:  0.016485802829\n",
      "training step: 123041, loss:  0.003638759954\n",
      "training step: 123082, loss:  0.003776642960\n",
      "training step: 123123, loss:  0.003002761398\n",
      "training step: 123164, loss:  0.005715856329\n",
      "training step: 123205, loss:  0.011776376516\n",
      "training step: 123246, loss:  0.003543942003\n",
      "training step: 123287, loss:  0.003986798227\n",
      "training step: 123328, loss:  0.024754593149\n",
      "training step: 123369, loss:  0.020691098645\n",
      "training step: 123410, loss:  0.007877553813\n",
      "training step: 123451, loss:  0.024754593149\n",
      "training step: 123492, loss:  0.025363046676\n",
      "training step: 123533, loss:  0.003002761398\n",
      "training step: 123574, loss:  0.005485921632\n",
      "training step: 123615, loss:  0.003120336682\n",
      "training step: 123656, loss:  0.005715856329\n",
      "training step: 123697, loss:  0.018698843196\n",
      "training step: 123738, loss:  0.025363046676\n",
      "training step: 123779, loss:  0.003284437582\n",
      "training step: 123820, loss:  0.025568028912\n",
      "training step: 123861, loss:  0.006285338197\n",
      "training step: 123902, loss:  0.002930165734\n",
      "training step: 123943, loss:  0.003002761398\n",
      "training step: 123984, loss:  0.003900909098\n",
      "training step: 124025, loss:  0.004080093466\n",
      "training step: 124066, loss:  0.003543942003\n",
      "training step: 124107, loss:  0.003808671376\n",
      "training step: 124148, loss:  0.025568028912\n",
      "training step: 124189, loss:  0.005715856329\n",
      "training step: 124230, loss:  0.007341358811\n",
      "training step: 124271, loss:  0.003808671376\n",
      "training step: 124312, loss:  0.002975780051\n",
      "training step: 124353, loss:  0.009473215789\n",
      "training step: 124394, loss:  0.003880497534\n",
      "training step: 124435, loss:  0.003473303979\n",
      "training step: 124476, loss:  0.004080093466\n",
      "training step: 124517, loss:  0.022392421961\n",
      "training step: 124558, loss:  0.003011468332\n",
      "training step: 124599, loss:  0.024754593149\n",
      "training step: 124640, loss:  0.003808671376\n",
      "training step: 124681, loss:  0.002386306180\n",
      "training step: 124722, loss:  0.003808671376\n",
      "training step: 124763, loss:  0.003473303979\n",
      "training step: 124804, loss:  0.007341358811\n",
      "training step: 124845, loss:  0.003543942003\n",
      "training step: 124886, loss:  0.003880497534\n",
      "training step: 124927, loss:  0.016485802829\n",
      "training step: 124968, loss:  0.020691098645\n",
      "training step: 125009, loss:  0.005485921632\n",
      "training step: 125050, loss:  0.003638759954\n",
      "training step: 125091, loss:  0.025363046676\n",
      "training step: 125132, loss:  0.006285338197\n",
      "training step: 125173, loss:  0.002396283671\n",
      "training step: 125214, loss:  0.003808671376\n",
      "training step: 125255, loss:  0.003900909098\n",
      "training step: 125296, loss:  0.003135976614\n",
      "training step: 125337, loss:  0.003986798227\n",
      "training step: 125378, loss:  0.009004401974\n",
      "training step: 125419, loss:  0.002396283671\n",
      "training step: 125460, loss:  0.024754593149\n",
      "training step: 125501, loss:  0.003284437582\n",
      "training step: 125542, loss:  0.003473303979\n",
      "training step: 125583, loss:  0.003467082279\n",
      "training step: 125624, loss:  0.003638759954\n",
      "training step: 125665, loss:  0.003638759954\n",
      "training step: 125706, loss:  0.020691098645\n",
      "training step: 125747, loss:  0.003120336682\n",
      "training step: 125788, loss:  0.003135976614\n",
      "training step: 125829, loss:  0.003880497534\n",
      "training step: 125870, loss:  0.002386306180\n",
      "training step: 125911, loss:  0.016485802829\n",
      "training step: 125952, loss:  0.003284437582\n",
      "training step: 125993, loss:  0.007877553813\n",
      "training step: 126034, loss:  0.003880497534\n",
      "training step: 126075, loss:  0.003543942003\n",
      "training step: 126116, loss:  0.016485802829\n",
      "training step: 126157, loss:  0.025363046676\n",
      "training step: 126198, loss:  0.003900909098\n",
      "training step: 126239, loss:  0.002930165734\n",
      "training step: 126280, loss:  0.006285338197\n",
      "training step: 126321, loss:  0.003473303979\n",
      "training step: 126362, loss:  0.024754593149\n",
      "training step: 126403, loss:  0.003776642960\n",
      "training step: 126444, loss:  0.007877553813\n",
      "training step: 126485, loss:  0.006285338197\n",
      "training step: 126526, loss:  0.025363046676\n",
      "training step: 126567, loss:  0.003284437582\n",
      "training step: 126608, loss:  0.003638759954\n",
      "training step: 126649, loss:  0.003900909098\n",
      "training step: 126690, loss:  0.004087368958\n",
      "training step: 126731, loss:  0.003002761398\n",
      "training step: 126772, loss:  0.003986798227\n",
      "training step: 126813, loss:  0.003692979924\n",
      "training step: 126854, loss:  0.002396283671\n",
      "training step: 126895, loss:  0.003467082279\n",
      "training step: 126936, loss:  0.003900909098\n",
      "training step: 126977, loss:  0.003900909098\n",
      "training step: 127018, loss:  0.005715856329\n",
      "training step: 127059, loss:  0.003467082279\n",
      "training step: 127100, loss:  0.003866109066\n",
      "training step: 127141, loss:  0.003467082279\n",
      "training step: 127182, loss:  0.018698843196\n",
      "training step: 127223, loss:  0.009473215789\n",
      "training step: 127264, loss:  0.003808671376\n",
      "training step: 127305, loss:  0.002930165734\n",
      "training step: 127346, loss:  0.003986798227\n",
      "training step: 127387, loss:  0.003638759954\n",
      "training step: 127428, loss:  0.023757282645\n",
      "training step: 127469, loss:  0.004087368958\n",
      "training step: 127510, loss:  0.003135976614\n",
      "training step: 127551, loss:  0.003120336682\n",
      "training step: 127592, loss:  0.018698843196\n",
      "training step: 127633, loss:  0.023757282645\n",
      "training step: 127674, loss:  0.009004401974\n",
      "training step: 127715, loss:  0.020691098645\n",
      "training step: 127756, loss:  0.003002761398\n",
      "training step: 127797, loss:  0.007877553813\n",
      "training step: 127838, loss:  0.024754593149\n",
      "training step: 127879, loss:  0.003467082279\n",
      "training step: 127920, loss:  0.005003615282\n",
      "training step: 127961, loss:  0.002975780051\n",
      "training step: 128002, loss:  0.007341358811\n",
      "training step: 128043, loss:  0.024754593149\n",
      "training step: 128084, loss:  0.020691098645\n",
      "training step: 128125, loss:  0.007877553813\n",
      "training step: 128166, loss:  0.003776642960\n",
      "training step: 128207, loss:  0.004087368958\n",
      "training step: 128248, loss:  0.003002761398\n",
      "training step: 128289, loss:  0.003473303979\n",
      "training step: 128330, loss:  0.003866109066\n",
      "training step: 128371, loss:  0.003866109066\n",
      "training step: 128412, loss:  0.003986798227\n",
      "training step: 128453, loss:  0.011776376516\n",
      "training step: 128494, loss:  0.009473215789\n",
      "training step: 128535, loss:  0.025568028912\n",
      "training step: 128576, loss:  0.003900909098\n",
      "training step: 128617, loss:  0.007341358811\n",
      "training step: 128658, loss:  0.023757282645\n",
      "training step: 128699, loss:  0.007341358811\n",
      "training step: 128740, loss:  0.003467082279\n",
      "training step: 128781, loss:  0.004087368958\n",
      "training step: 128822, loss:  0.009473215789\n",
      "training step: 128863, loss:  0.025363046676\n",
      "training step: 128904, loss:  0.003776642960\n",
      "training step: 128945, loss:  0.003880497534\n",
      "training step: 128986, loss:  0.004080093466\n",
      "training step: 129027, loss:  0.022392421961\n",
      "training step: 129068, loss:  0.005485921632\n",
      "training step: 129109, loss:  0.005715856329\n",
      "training step: 129150, loss:  0.003120336682\n",
      "training step: 129191, loss:  0.018698843196\n",
      "training step: 129232, loss:  0.005715856329\n",
      "training step: 129273, loss:  0.003986798227\n",
      "training step: 129314, loss:  0.003002761398\n",
      "training step: 129355, loss:  0.023757282645\n",
      "training step: 129396, loss:  0.016485802829\n",
      "training step: 129437, loss:  0.005715856329\n",
      "training step: 129478, loss:  0.005485921632\n",
      "training step: 129519, loss:  0.003284437582\n",
      "training step: 129560, loss:  0.004087368958\n",
      "training step: 129601, loss:  0.005485921632\n",
      "training step: 129642, loss:  0.018698843196\n",
      "training step: 129683, loss:  0.025363046676\n",
      "training step: 129724, loss:  0.007341358811\n",
      "training step: 129765, loss:  0.024754593149\n",
      "training step: 129806, loss:  0.003808671376\n",
      "training step: 129847, loss:  0.003776642960\n",
      "training step: 129888, loss:  0.003120336682\n",
      "training step: 129929, loss:  0.003900909098\n",
      "training step: 129970, loss:  0.024754593149\n",
      "training step: 130011, loss:  0.005003615282\n",
      "training step: 130052, loss:  0.025363046676\n",
      "training step: 130093, loss:  0.003135976614\n",
      "training step: 130134, loss:  0.003467082279\n",
      "training step: 130175, loss:  0.003120336682\n",
      "training step: 130216, loss:  0.003011468332\n",
      "training step: 130257, loss:  0.003866109066\n",
      "training step: 130298, loss:  0.003120336682\n",
      "training step: 130339, loss:  0.003120336682\n",
      "training step: 130380, loss:  0.016485802829\n",
      "training step: 130421, loss:  0.003284437582\n",
      "training step: 130462, loss:  0.005003615282\n",
      "training step: 130503, loss:  0.005715856329\n",
      "training step: 130544, loss:  0.004080093466\n",
      "training step: 130585, loss:  0.003880497534\n",
      "training step: 130626, loss:  0.003986798227\n",
      "training step: 130667, loss:  0.005485921632\n",
      "training step: 130708, loss:  0.020691098645\n",
      "training step: 130749, loss:  0.014145905152\n",
      "training step: 130790, loss:  0.009473215789\n",
      "training step: 130831, loss:  0.005003615282\n",
      "training step: 130872, loss:  0.004080093466\n",
      "training step: 130913, loss:  0.003120336682\n",
      "training step: 130954, loss:  0.002930165734\n",
      "training step: 130995, loss:  0.009004401974\n",
      "training step: 131036, loss:  0.003120336682\n",
      "training step: 131077, loss:  0.003284437582\n",
      "training step: 131118, loss:  0.006285338197\n",
      "training step: 131159, loss:  0.002386306180\n",
      "training step: 131200, loss:  0.002975780051\n",
      "training step: 131241, loss:  0.003473303979\n",
      "training step: 131282, loss:  0.022392421961\n",
      "training step: 131323, loss:  0.009473215789\n",
      "training step: 131364, loss:  0.003284437582\n",
      "training step: 131405, loss:  0.022392421961\n",
      "training step: 131446, loss:  0.003880497534\n",
      "training step: 131487, loss:  0.004080093466\n",
      "training step: 131528, loss:  0.003880497534\n",
      "training step: 131569, loss:  0.003467082279\n",
      "training step: 131610, loss:  0.009004401974\n",
      "training step: 131651, loss:  0.002930165734\n",
      "training step: 131692, loss:  0.025363046676\n",
      "training step: 131733, loss:  0.006285338197\n",
      "training step: 131774, loss:  0.003284437582\n",
      "training step: 131815, loss:  0.002975780051\n",
      "training step: 131856, loss:  0.003467082279\n",
      "training step: 131897, loss:  0.018698843196\n",
      "training step: 131938, loss:  0.011776376516\n",
      "training step: 131979, loss:  0.003135976614\n",
      "training step: 132020, loss:  0.005715856329\n",
      "training step: 132061, loss:  0.018698843196\n",
      "training step: 132102, loss:  0.024754593149\n",
      "training step: 132143, loss:  0.002975780051\n",
      "training step: 132184, loss:  0.011776376516\n",
      "training step: 132225, loss:  0.023757282645\n",
      "training step: 132266, loss:  0.011776376516\n",
      "training step: 132307, loss:  0.003473303979\n",
      "training step: 132348, loss:  0.016485802829\n",
      "training step: 132389, loss:  0.007341358811\n",
      "training step: 132430, loss:  0.006285338197\n",
      "training step: 132471, loss:  0.025363046676\n",
      "training step: 132512, loss:  0.025363046676\n",
      "training step: 132553, loss:  0.007877553813\n",
      "training step: 132594, loss:  0.009004401974\n",
      "training step: 132635, loss:  0.002386306180\n",
      "training step: 132676, loss:  0.011776376516\n",
      "training step: 132717, loss:  0.003467082279\n",
      "training step: 132758, loss:  0.022392421961\n",
      "training step: 132799, loss:  0.025568028912\n",
      "training step: 132840, loss:  0.023757282645\n",
      "training step: 132881, loss:  0.003543942003\n",
      "training step: 132922, loss:  0.025568028912\n",
      "training step: 132963, loss:  0.007877553813\n",
      "training step: 133004, loss:  0.004080093466\n",
      "training step: 133045, loss:  0.003135976614\n",
      "training step: 133086, loss:  0.002386306180\n",
      "training step: 133127, loss:  0.025568028912\n",
      "training step: 133168, loss:  0.003900909098\n",
      "training step: 133209, loss:  0.011776376516\n",
      "training step: 133250, loss:  0.005715856329\n",
      "training step: 133291, loss:  0.002386306180\n",
      "training step: 133332, loss:  0.003135976614\n",
      "training step: 133373, loss:  0.002930165734\n",
      "training step: 133414, loss:  0.022392421961\n",
      "training step: 133455, loss:  0.007341358811\n",
      "training step: 133496, loss:  0.016485802829\n",
      "training step: 133537, loss:  0.003002761398\n",
      "training step: 133578, loss:  0.003776642960\n",
      "training step: 133619, loss:  0.005715856329\n",
      "training step: 133660, loss:  0.003776642960\n",
      "training step: 133701, loss:  0.016485802829\n",
      "training step: 133742, loss:  0.004080093466\n",
      "training step: 133783, loss:  0.011776376516\n",
      "training step: 133824, loss:  0.003467082279\n",
      "training step: 133865, loss:  0.011776376516\n",
      "training step: 133906, loss:  0.003776642960\n",
      "training step: 133947, loss:  0.025363046676\n",
      "training step: 133988, loss:  0.003002761398\n",
      "training step: 134029, loss:  0.003120336682\n",
      "training step: 134070, loss:  0.006285338197\n",
      "training step: 134111, loss:  0.002386306180\n",
      "training step: 134152, loss:  0.003880497534\n",
      "training step: 134193, loss:  0.003692979924\n",
      "training step: 134234, loss:  0.007752379868\n",
      "training step: 134275, loss:  0.004080093466\n",
      "training step: 134316, loss:  0.011776376516\n",
      "training step: 134357, loss:  0.003473303979\n",
      "training step: 134398, loss:  0.003473303979\n",
      "training step: 134439, loss:  0.003900909098\n",
      "training step: 134480, loss:  0.024754593149\n",
      "training step: 134521, loss:  0.016485802829\n",
      "training step: 134562, loss:  0.003473303979\n",
      "training step: 134603, loss:  0.023757282645\n",
      "training step: 134644, loss:  0.024754593149\n",
      "training step: 134685, loss:  0.003002761398\n",
      "training step: 134726, loss:  0.005003615282\n",
      "training step: 134767, loss:  0.003692979924\n",
      "training step: 134808, loss:  0.024754593149\n",
      "training step: 134849, loss:  0.020691098645\n",
      "training step: 134890, loss:  0.003002761398\n",
      "training step: 134931, loss:  0.009473215789\n",
      "training step: 134972, loss:  0.003011468332\n",
      "training step: 135013, loss:  0.009473215789\n",
      "training step: 135054, loss:  0.011776376516\n",
      "training step: 135095, loss:  0.002975780051\n",
      "training step: 135136, loss:  0.003135976614\n",
      "training step: 135177, loss:  0.003284437582\n",
      "training step: 135218, loss:  0.003900909098\n",
      "training step: 135259, loss:  0.002386306180\n",
      "training step: 135300, loss:  0.014145905152\n",
      "training step: 135341, loss:  0.003284437582\n",
      "training step: 135382, loss:  0.007341358811\n",
      "training step: 135423, loss:  0.009004401974\n",
      "training step: 135464, loss:  0.003543942003\n",
      "training step: 135505, loss:  0.004087368958\n",
      "training step: 135546, loss:  0.007341358811\n",
      "training step: 135587, loss:  0.005485921632\n",
      "training step: 135628, loss:  0.003900909098\n",
      "training step: 135669, loss:  0.003135976614\n",
      "training step: 135710, loss:  0.002930165734\n",
      "training step: 135751, loss:  0.003880497534\n",
      "training step: 135792, loss:  0.003467082279\n",
      "training step: 135833, loss:  0.003692979924\n",
      "training step: 135874, loss:  0.003473303979\n",
      "training step: 135915, loss:  0.002396283671\n",
      "training step: 135956, loss:  0.003986798227\n",
      "training step: 135997, loss:  0.022392421961\n",
      "training step: 136038, loss:  0.024754593149\n",
      "training step: 136079, loss:  0.025568028912\n",
      "training step: 136120, loss:  0.003638759954\n",
      "training step: 136161, loss:  0.003011468332\n",
      "training step: 136202, loss:  0.002396283671\n",
      "training step: 136243, loss:  0.009473215789\n",
      "training step: 136284, loss:  0.025363046676\n",
      "training step: 136325, loss:  0.025363046676\n",
      "training step: 136366, loss:  0.003866109066\n",
      "training step: 136407, loss:  0.023757282645\n",
      "training step: 136448, loss:  0.009004401974\n",
      "training step: 136489, loss:  0.003002761398\n",
      "training step: 136530, loss:  0.014145905152\n",
      "training step: 136571, loss:  0.004087368958\n",
      "training step: 136612, loss:  0.011776376516\n",
      "training step: 136653, loss:  0.003120336682\n",
      "training step: 136694, loss:  0.003808671376\n",
      "training step: 136735, loss:  0.005485921632\n",
      "training step: 136776, loss:  0.003473303979\n",
      "training step: 136817, loss:  0.003473303979\n",
      "training step: 136858, loss:  0.023757282645\n",
      "training step: 136899, loss:  0.022392421961\n",
      "training step: 136940, loss:  0.020691098645\n",
      "training step: 136981, loss:  0.025568028912\n",
      "training step: 137022, loss:  0.003880497534\n",
      "training step: 137063, loss:  0.002930165734\n",
      "training step: 137104, loss:  0.003543942003\n",
      "training step: 137145, loss:  0.003692979924\n",
      "training step: 137186, loss:  0.003467082279\n",
      "training step: 137227, loss:  0.003543942003\n",
      "training step: 137268, loss:  0.009473215789\n",
      "training step: 137309, loss:  0.025363046676\n",
      "training step: 137350, loss:  0.003692979924\n",
      "training step: 137391, loss:  0.002396283671\n",
      "training step: 137432, loss:  0.003866109066\n",
      "training step: 137473, loss:  0.002386306180\n",
      "training step: 137514, loss:  0.003473303979\n",
      "training step: 137555, loss:  0.003011468332\n",
      "training step: 137596, loss:  0.022392421961\n",
      "training step: 137637, loss:  0.004080093466\n",
      "training step: 137678, loss:  0.009004401974\n",
      "training step: 137719, loss:  0.003467082279\n",
      "training step: 137760, loss:  0.007877553813\n",
      "training step: 137801, loss:  0.023757282645\n",
      "training step: 137842, loss:  0.003866109066\n",
      "training step: 137883, loss:  0.025363046676\n",
      "training step: 137924, loss:  0.003284437582\n",
      "training step: 137965, loss:  0.003866109066\n",
      "training step: 138006, loss:  0.016485802829\n",
      "training step: 138047, loss:  0.006285338197\n",
      "training step: 138088, loss:  0.003638759954\n",
      "training step: 138129, loss:  0.002975780051\n",
      "training step: 138170, loss:  0.009004401974\n",
      "training step: 138211, loss:  0.003284437582\n",
      "training step: 138252, loss:  0.024754593149\n",
      "training step: 138293, loss:  0.007341358811\n",
      "training step: 138334, loss:  0.003900909098\n",
      "training step: 138375, loss:  0.007752379868\n",
      "training step: 138416, loss:  0.005485921632\n",
      "training step: 138457, loss:  0.003692979924\n",
      "training step: 138498, loss:  0.020691098645\n",
      "training step: 138539, loss:  0.004087368958\n",
      "training step: 138580, loss:  0.002386306180\n",
      "training step: 138621, loss:  0.003900909098\n",
      "training step: 138662, loss:  0.006285338197\n",
      "training step: 138703, loss:  0.003638759954\n",
      "training step: 138744, loss:  0.009004401974\n",
      "training step: 138785, loss:  0.016485802829\n",
      "training step: 138826, loss:  0.003284437582\n",
      "training step: 138867, loss:  0.007341358811\n",
      "training step: 138908, loss:  0.003002761398\n",
      "training step: 138949, loss:  0.003011468332\n",
      "training step: 138990, loss:  0.003120336682\n",
      "training step: 139031, loss:  0.003692979924\n",
      "training step: 139072, loss:  0.016485802829\n",
      "training step: 139113, loss:  0.014145905152\n",
      "training step: 139154, loss:  0.003776642960\n",
      "training step: 139195, loss:  0.016485802829\n",
      "training step: 139236, loss:  0.003808671376\n",
      "training step: 139277, loss:  0.003638759954\n",
      "training step: 139318, loss:  0.025363046676\n",
      "training step: 139359, loss:  0.006285338197\n",
      "training step: 139400, loss:  0.003002761398\n",
      "training step: 139441, loss:  0.003543942003\n",
      "training step: 139482, loss:  0.003543942003\n",
      "training step: 139523, loss:  0.022392421961\n",
      "training step: 139564, loss:  0.007877553813\n",
      "training step: 139605, loss:  0.009473215789\n",
      "training step: 139646, loss:  0.002396283671\n",
      "training step: 139687, loss:  0.003866109066\n",
      "training step: 139728, loss:  0.005003615282\n",
      "training step: 139769, loss:  0.002396283671\n",
      "training step: 139810, loss:  0.005003615282\n",
      "training step: 139851, loss:  0.025568028912\n",
      "training step: 139892, loss:  0.002930165734\n",
      "training step: 139933, loss:  0.007877553813\n",
      "training step: 139974, loss:  0.003011468332\n",
      "training step: 140015, loss:  0.003808671376\n",
      "training step: 140056, loss:  0.005485921632\n",
      "training step: 140097, loss:  0.003002761398\n",
      "training step: 140138, loss:  0.003002761398\n",
      "training step: 140179, loss:  0.003002761398\n",
      "training step: 140220, loss:  0.003808671376\n",
      "training step: 140261, loss:  0.024754593149\n",
      "training step: 140302, loss:  0.024754593149\n",
      "training step: 140343, loss:  0.002975780051\n",
      "training step: 140384, loss:  0.023757282645\n",
      "training step: 140425, loss:  0.014145905152\n",
      "training step: 140466, loss:  0.009473215789\n",
      "training step: 140507, loss:  0.003776642960\n",
      "training step: 140548, loss:  0.003002761398\n",
      "training step: 140589, loss:  0.003002761398\n",
      "training step: 140630, loss:  0.004080093466\n",
      "training step: 140671, loss:  0.005715856329\n",
      "training step: 140712, loss:  0.025363046676\n",
      "training step: 140753, loss:  0.003011468332\n",
      "training step: 140794, loss:  0.003011468332\n",
      "training step: 140835, loss:  0.003638759954\n",
      "training step: 140876, loss:  0.003543942003\n",
      "training step: 140917, loss:  0.005715856329\n",
      "training step: 140958, loss:  0.025363046676\n",
      "training step: 140999, loss:  0.009473215789\n",
      "training step: 141040, loss:  0.003284437582\n",
      "training step: 141081, loss:  0.011776376516\n",
      "training step: 141122, loss:  0.002975780051\n",
      "training step: 141163, loss:  0.003776642960\n",
      "training step: 141204, loss:  0.003638759954\n",
      "training step: 141245, loss:  0.007752379868\n",
      "training step: 141286, loss:  0.003284437582\n",
      "training step: 141327, loss:  0.022392421961\n",
      "training step: 141368, loss:  0.014145905152\n",
      "training step: 141409, loss:  0.005003615282\n",
      "training step: 141450, loss:  0.005715856329\n",
      "training step: 141491, loss:  0.014145905152\n",
      "training step: 141532, loss:  0.003120336682\n",
      "training step: 141573, loss:  0.003986798227\n",
      "training step: 141614, loss:  0.003135976614\n",
      "training step: 141655, loss:  0.006285338197\n",
      "training step: 141696, loss:  0.007877553813\n",
      "training step: 141737, loss:  0.005003615282\n",
      "training step: 141778, loss:  0.003880497534\n",
      "training step: 141819, loss:  0.003866109066\n",
      "training step: 141860, loss:  0.005715856329\n",
      "training step: 141901, loss:  0.016485802829\n",
      "training step: 141942, loss:  0.025363046676\n",
      "training step: 141983, loss:  0.005485921632\n",
      "training step: 142024, loss:  0.003120336682\n",
      "training step: 142065, loss:  0.003692979924\n",
      "training step: 142106, loss:  0.025363046676\n",
      "training step: 142147, loss:  0.022392421961\n",
      "training step: 142188, loss:  0.007877553813\n",
      "training step: 142229, loss:  0.003866109066\n",
      "training step: 142270, loss:  0.003638759954\n",
      "training step: 142311, loss:  0.005003615282\n",
      "training step: 142352, loss:  0.005485921632\n",
      "training step: 142393, loss:  0.003866109066\n",
      "training step: 142434, loss:  0.003467082279\n",
      "training step: 142475, loss:  0.002386306180\n",
      "training step: 142516, loss:  0.003543942003\n",
      "training step: 142557, loss:  0.022392421961\n",
      "training step: 142598, loss:  0.009473215789\n",
      "training step: 142639, loss:  0.016485802829\n",
      "training step: 142680, loss:  0.003543942003\n",
      "training step: 142721, loss:  0.009473215789\n",
      "training step: 142762, loss:  0.022392421961\n",
      "training step: 142803, loss:  0.025363046676\n",
      "training step: 142844, loss:  0.003866109066\n",
      "training step: 142885, loss:  0.003808671376\n",
      "training step: 142926, loss:  0.003543942003\n",
      "training step: 142967, loss:  0.005715856329\n",
      "training step: 143008, loss:  0.005003615282\n",
      "training step: 143049, loss:  0.003473303979\n",
      "training step: 143090, loss:  0.003880497534\n",
      "training step: 143131, loss:  0.003284437582\n",
      "training step: 143172, loss:  0.023757282645\n",
      "training step: 143213, loss:  0.005485921632\n",
      "training step: 143254, loss:  0.003692979924\n",
      "training step: 143295, loss:  0.025363046676\n",
      "training step: 143336, loss:  0.004080093466\n",
      "training step: 143377, loss:  0.023757282645\n",
      "training step: 143418, loss:  0.003866109066\n",
      "training step: 143459, loss:  0.007877553813\n",
      "training step: 143500, loss:  0.025363046676\n",
      "training step: 143541, loss:  0.004080093466\n",
      "training step: 143582, loss:  0.005485921632\n",
      "training step: 143623, loss:  0.003808671376\n",
      "training step: 143664, loss:  0.003808671376\n",
      "training step: 143705, loss:  0.007877553813\n",
      "training step: 143746, loss:  0.005715856329\n",
      "training step: 143787, loss:  0.009473215789\n",
      "training step: 143828, loss:  0.011776376516\n",
      "training step: 143869, loss:  0.024754593149\n",
      "training step: 143910, loss:  0.003692979924\n",
      "training step: 143951, loss:  0.003900909098\n",
      "training step: 143992, loss:  0.023757282645\n",
      "training step: 144033, loss:  0.003866109066\n",
      "training step: 144074, loss:  0.003473303979\n",
      "training step: 144115, loss:  0.007877553813\n",
      "training step: 144156, loss:  0.003638759954\n",
      "training step: 144197, loss:  0.024754593149\n",
      "training step: 144238, loss:  0.002975780051\n",
      "training step: 144279, loss:  0.003900909098\n",
      "training step: 144320, loss:  0.025363046676\n",
      "training step: 144361, loss:  0.003467082279\n",
      "training step: 144402, loss:  0.025568028912\n",
      "training step: 144443, loss:  0.003120336682\n",
      "training step: 144484, loss:  0.022392421961\n",
      "training step: 144525, loss:  0.025363046676\n",
      "training step: 144566, loss:  0.005003615282\n",
      "training step: 144607, loss:  0.003473303979\n",
      "training step: 144648, loss:  0.002975780051\n",
      "training step: 144689, loss:  0.025568028912\n",
      "training step: 144730, loss:  0.003866109066\n",
      "training step: 144771, loss:  0.007752379868\n",
      "training step: 144812, loss:  0.006285338197\n",
      "training step: 144853, loss:  0.003986798227\n",
      "training step: 144894, loss:  0.022392421961\n",
      "training step: 144935, loss:  0.022392421961\n",
      "training step: 144976, loss:  0.002975780051\n",
      "training step: 145017, loss:  0.022392421961\n",
      "training step: 145058, loss:  0.003120336682\n",
      "training step: 145099, loss:  0.014145905152\n",
      "training step: 145140, loss:  0.003866109066\n",
      "training step: 145181, loss:  0.002396283671\n",
      "training step: 145222, loss:  0.003986798227\n",
      "training step: 145263, loss:  0.003473303979\n",
      "training step: 145304, loss:  0.003135976614\n",
      "training step: 145345, loss:  0.003880497534\n",
      "training step: 145386, loss:  0.025363046676\n",
      "training step: 145427, loss:  0.006285338197\n",
      "training step: 145468, loss:  0.004080093466\n",
      "training step: 145509, loss:  0.024754593149\n",
      "training step: 145550, loss:  0.025568028912\n",
      "training step: 145591, loss:  0.022392421961\n",
      "training step: 145632, loss:  0.011776376516\n",
      "training step: 145673, loss:  0.003880497534\n",
      "training step: 145714, loss:  0.003543942003\n",
      "training step: 145755, loss:  0.005715856329\n",
      "training step: 145796, loss:  0.011776376516\n",
      "training step: 145837, loss:  0.011776376516\n",
      "training step: 145878, loss:  0.005003615282\n",
      "training step: 145919, loss:  0.004080093466\n",
      "training step: 145960, loss:  0.003467082279\n",
      "training step: 146001, loss:  0.002975780051\n",
      "training step: 146042, loss:  0.024754593149\n",
      "training step: 146083, loss:  0.003880497534\n",
      "training step: 146124, loss:  0.016485802829\n",
      "training step: 146165, loss:  0.002396283671\n",
      "training step: 146206, loss:  0.003880497534\n",
      "training step: 146247, loss:  0.009004401974\n",
      "training step: 146288, loss:  0.007752379868\n",
      "training step: 146329, loss:  0.003002761398\n",
      "training step: 146370, loss:  0.024754593149\n",
      "training step: 146411, loss:  0.009004401974\n",
      "training step: 146452, loss:  0.014145905152\n",
      "training step: 146493, loss:  0.003880497534\n",
      "training step: 146534, loss:  0.011776376516\n",
      "training step: 146575, loss:  0.011776376516\n",
      "training step: 146616, loss:  0.025363046676\n",
      "training step: 146657, loss:  0.003543942003\n",
      "training step: 146698, loss:  0.003543942003\n",
      "training step: 146739, loss:  0.005485921632\n",
      "training step: 146780, loss:  0.009473215789\n",
      "training step: 146821, loss:  0.025363046676\n",
      "training step: 146862, loss:  0.007752379868\n",
      "training step: 146903, loss:  0.006285338197\n",
      "training step: 146944, loss:  0.002930165734\n",
      "training step: 146985, loss:  0.002386306180\n",
      "training step: 147026, loss:  0.002396283671\n",
      "training step: 147067, loss:  0.002386306180\n",
      "training step: 147108, loss:  0.003473303979\n",
      "training step: 147149, loss:  0.003900909098\n",
      "training step: 147190, loss:  0.004087368958\n",
      "training step: 147231, loss:  0.003776642960\n",
      "training step: 147272, loss:  0.018698843196\n",
      "training step: 147313, loss:  0.003467082279\n",
      "training step: 147354, loss:  0.003002761398\n",
      "training step: 147395, loss:  0.006285338197\n",
      "training step: 147436, loss:  0.016485802829\n",
      "training step: 147477, loss:  0.005003615282\n",
      "training step: 147518, loss:  0.005715856329\n",
      "training step: 147559, loss:  0.003473303979\n",
      "training step: 147600, loss:  0.003986798227\n",
      "training step: 147641, loss:  0.002396283671\n",
      "training step: 147682, loss:  0.007752379868\n",
      "training step: 147723, loss:  0.018698843196\n",
      "training step: 147764, loss:  0.009473215789\n",
      "training step: 147805, loss:  0.020691098645\n",
      "training step: 147846, loss:  0.003900909098\n",
      "training step: 147887, loss:  0.004080093466\n",
      "training step: 147928, loss:  0.003284437582\n",
      "training step: 147969, loss:  0.003002761398\n",
      "training step: 148010, loss:  0.003808671376\n",
      "training step: 148051, loss:  0.016485802829\n",
      "training step: 148092, loss:  0.002930165734\n",
      "training step: 148133, loss:  0.003866109066\n",
      "training step: 148174, loss:  0.024754593149\n",
      "training step: 148215, loss:  0.002386306180\n",
      "training step: 148256, loss:  0.002396283671\n",
      "training step: 148297, loss:  0.003120336682\n",
      "training step: 148338, loss:  0.003002761398\n",
      "training step: 148379, loss:  0.011776376516\n",
      "training step: 148420, loss:  0.003880497534\n",
      "training step: 148461, loss:  0.005485921632\n",
      "training step: 148502, loss:  0.009004401974\n",
      "training step: 148543, loss:  0.003900909098\n",
      "training step: 148584, loss:  0.025363046676\n",
      "training step: 148625, loss:  0.003120336682\n",
      "training step: 148666, loss:  0.016485802829\n",
      "training step: 148707, loss:  0.009473215789\n",
      "training step: 148748, loss:  0.003776642960\n",
      "training step: 148789, loss:  0.002930165734\n",
      "training step: 148830, loss:  0.003808671376\n",
      "training step: 148871, loss:  0.009004401974\n",
      "training step: 148912, loss:  0.005003615282\n",
      "training step: 148953, loss:  0.005485921632\n",
      "training step: 148994, loss:  0.011776376516\n",
      "training step: 149035, loss:  0.002930165734\n",
      "training step: 149076, loss:  0.003900909098\n",
      "training step: 149117, loss:  0.011776376516\n",
      "training step: 149158, loss:  0.003880497534\n",
      "training step: 149199, loss:  0.002930165734\n",
      "training step: 149240, loss:  0.022392421961\n",
      "training step: 149281, loss:  0.003011468332\n",
      "training step: 149322, loss:  0.003880497534\n",
      "training step: 149363, loss:  0.003880497534\n",
      "training step: 149404, loss:  0.004080093466\n",
      "training step: 149445, loss:  0.003776642960\n",
      "training step: 149486, loss:  0.003135976614\n",
      "training step: 149527, loss:  0.003692979924\n",
      "training step: 149568, loss:  0.003120336682\n",
      "training step: 149609, loss:  0.006285338197\n",
      "training step: 149650, loss:  0.004080093466\n",
      "training step: 149691, loss:  0.003638759954\n",
      "training step: 149732, loss:  0.004080093466\n",
      "training step: 149773, loss:  0.011776376516\n",
      "training step: 149814, loss:  0.003011468332\n",
      "training step: 149855, loss:  0.011776376516\n",
      "training step: 149896, loss:  0.004080093466\n",
      "training step: 149937, loss:  0.024754593149\n",
      "training step: 149978, loss:  0.016485802829\n",
      "training step: 150019, loss:  0.024754593149\n",
      "training step: 150060, loss:  0.005715856329\n",
      "training step: 150101, loss:  0.003638759954\n",
      "training step: 150142, loss:  0.003473303979\n",
      "training step: 150183, loss:  0.025568028912\n",
      "training step: 150224, loss:  0.011776376516\n",
      "training step: 150265, loss:  0.003543942003\n",
      "training step: 150306, loss:  0.002930165734\n",
      "training step: 150347, loss:  0.003011468332\n",
      "training step: 150388, loss:  0.014145905152\n",
      "training step: 150429, loss:  0.004080093466\n",
      "training step: 150470, loss:  0.016485802829\n",
      "training step: 150511, loss:  0.003135976614\n",
      "training step: 150552, loss:  0.007341358811\n",
      "training step: 150593, loss:  0.003002761398\n",
      "training step: 150634, loss:  0.022392421961\n",
      "training step: 150675, loss:  0.003638759954\n",
      "training step: 150716, loss:  0.003284437582\n",
      "training step: 150757, loss:  0.003002761398\n",
      "training step: 150798, loss:  0.025363046676\n",
      "training step: 150839, loss:  0.024754593149\n",
      "training step: 150880, loss:  0.003002761398\n",
      "training step: 150921, loss:  0.025568028912\n",
      "training step: 150962, loss:  0.005715856329\n",
      "training step: 151003, loss:  0.003002761398\n",
      "training step: 151044, loss:  0.025568028912\n",
      "training step: 151085, loss:  0.003866109066\n",
      "training step: 151126, loss:  0.022392421961\n",
      "training step: 151167, loss:  0.016485802829\n",
      "training step: 151208, loss:  0.003002761398\n",
      "training step: 151249, loss:  0.003002761398\n",
      "training step: 151290, loss:  0.003866109066\n",
      "training step: 151331, loss:  0.003002761398\n",
      "training step: 151372, loss:  0.003467082279\n",
      "training step: 151413, loss:  0.005485921632\n",
      "training step: 151454, loss:  0.003986798227\n",
      "training step: 151495, loss:  0.004080093466\n",
      "training step: 151536, loss:  0.003880497534\n",
      "training step: 151577, loss:  0.003011468332\n",
      "training step: 151618, loss:  0.003284437582\n",
      "training step: 151659, loss:  0.003866109066\n",
      "training step: 151700, loss:  0.003776642960\n",
      "training step: 151741, loss:  0.024754593149\n",
      "training step: 151782, loss:  0.002396283671\n",
      "training step: 151823, loss:  0.002975780051\n",
      "training step: 151864, loss:  0.007341358811\n",
      "training step: 151905, loss:  0.003692979924\n",
      "training step: 151946, loss:  0.005715856329\n",
      "training step: 151987, loss:  0.005485921632\n",
      "training step: 152028, loss:  0.003543942003\n",
      "training step: 152069, loss:  0.009473215789\n",
      "training step: 152110, loss:  0.003011468332\n",
      "training step: 152151, loss:  0.003880497534\n",
      "training step: 152192, loss:  0.016485802829\n",
      "training step: 152233, loss:  0.003011468332\n",
      "training step: 152274, loss:  0.003120336682\n",
      "training step: 152315, loss:  0.018698843196\n",
      "training step: 152356, loss:  0.006285338197\n",
      "training step: 152397, loss:  0.003011468332\n",
      "training step: 152438, loss:  0.003135976614\n",
      "training step: 152479, loss:  0.014145905152\n",
      "training step: 152520, loss:  0.005003615282\n",
      "training step: 152561, loss:  0.003880497534\n",
      "training step: 152602, loss:  0.016485802829\n",
      "training step: 152643, loss:  0.005715856329\n",
      "training step: 152684, loss:  0.009004401974\n",
      "training step: 152725, loss:  0.011776376516\n",
      "training step: 152766, loss:  0.007341358811\n",
      "training step: 152807, loss:  0.003880497534\n",
      "training step: 152848, loss:  0.007877553813\n",
      "training step: 152889, loss:  0.003011468332\n",
      "training step: 152930, loss:  0.003808671376\n",
      "training step: 152971, loss:  0.003120336682\n",
      "training step: 153012, loss:  0.018698843196\n",
      "training step: 153053, loss:  0.003808671376\n",
      "training step: 153094, loss:  0.003011468332\n",
      "training step: 153135, loss:  0.003986798227\n",
      "training step: 153176, loss:  0.003900909098\n",
      "training step: 153217, loss:  0.003986798227\n",
      "training step: 153258, loss:  0.005715856329\n",
      "training step: 153299, loss:  0.005485921632\n",
      "training step: 153340, loss:  0.003120336682\n",
      "training step: 153381, loss:  0.020691098645\n",
      "training step: 153422, loss:  0.020691098645\n",
      "training step: 153463, loss:  0.002930165734\n",
      "training step: 153504, loss:  0.003638759954\n",
      "training step: 153545, loss:  0.003808671376\n",
      "training step: 153586, loss:  0.002930165734\n",
      "training step: 153627, loss:  0.003284437582\n",
      "training step: 153668, loss:  0.004080093466\n",
      "training step: 153709, loss:  0.007877553813\n",
      "training step: 153750, loss:  0.003467082279\n",
      "training step: 153791, loss:  0.020691098645\n",
      "training step: 153832, loss:  0.018698843196\n",
      "training step: 153873, loss:  0.003808671376\n",
      "training step: 153914, loss:  0.025363046676\n",
      "training step: 153955, loss:  0.020691098645\n",
      "training step: 153996, loss:  0.020691098645\n",
      "training step: 154037, loss:  0.003284437582\n",
      "training step: 154078, loss:  0.011776376516\n",
      "training step: 154119, loss:  0.003692979924\n",
      "training step: 154160, loss:  0.003986798227\n",
      "training step: 154201, loss:  0.003692979924\n",
      "training step: 154242, loss:  0.020691098645\n",
      "training step: 154283, loss:  0.025568028912\n",
      "training step: 154324, loss:  0.003776642960\n",
      "training step: 154365, loss:  0.003776642960\n",
      "training step: 154406, loss:  0.003900909098\n",
      "training step: 154447, loss:  0.006285338197\n",
      "training step: 154488, loss:  0.024754593149\n",
      "training step: 154529, loss:  0.002396283671\n",
      "training step: 154570, loss:  0.002930165734\n",
      "training step: 154611, loss:  0.005715856329\n",
      "training step: 154652, loss:  0.003011468332\n",
      "training step: 154693, loss:  0.003880497534\n",
      "training step: 154734, loss:  0.002930165734\n",
      "training step: 154775, loss:  0.003011468332\n",
      "training step: 154816, loss:  0.020691098645\n",
      "training step: 154857, loss:  0.003986798227\n",
      "training step: 154898, loss:  0.002930165734\n",
      "training step: 154939, loss:  0.020691098645\n",
      "training step: 154980, loss:  0.018698843196\n",
      "training step: 155021, loss:  0.003284437582\n",
      "training step: 155062, loss:  0.011776376516\n",
      "training step: 155103, loss:  0.009004401974\n",
      "training step: 155144, loss:  0.003284437582\n",
      "training step: 155185, loss:  0.003808671376\n",
      "training step: 155226, loss:  0.003808671376\n",
      "training step: 155267, loss:  0.009473215789\n",
      "training step: 155308, loss:  0.003986798227\n",
      "training step: 155349, loss:  0.003638759954\n",
      "training step: 155390, loss:  0.003986798227\n",
      "training step: 155431, loss:  0.003011468332\n",
      "training step: 155472, loss:  0.003692979924\n",
      "training step: 155513, loss:  0.003543942003\n",
      "training step: 155554, loss:  0.005003615282\n",
      "training step: 155595, loss:  0.003808671376\n",
      "training step: 155636, loss:  0.005715856329\n",
      "training step: 155677, loss:  0.003284437582\n",
      "training step: 155718, loss:  0.003986798227\n",
      "training step: 155759, loss:  0.003866109066\n",
      "training step: 155800, loss:  0.005003615282\n",
      "training step: 155841, loss:  0.005003615282\n",
      "training step: 155882, loss:  0.003002761398\n",
      "training step: 155923, loss:  0.007877553813\n",
      "training step: 155964, loss:  0.024754593149\n",
      "training step: 156005, loss:  0.004080093466\n",
      "training step: 156046, loss:  0.007752379868\n",
      "training step: 156087, loss:  0.025363046676\n",
      "training step: 156128, loss:  0.022392421961\n",
      "training step: 156169, loss:  0.002386306180\n",
      "training step: 156210, loss:  0.003692979924\n",
      "training step: 156251, loss:  0.007877553813\n",
      "training step: 156292, loss:  0.011776376516\n",
      "training step: 156333, loss:  0.014145905152\n",
      "training step: 156374, loss:  0.002930165734\n",
      "training step: 156415, loss:  0.003866109066\n",
      "training step: 156456, loss:  0.002975780051\n",
      "training step: 156497, loss:  0.006285338197\n",
      "training step: 156538, loss:  0.002396283671\n",
      "training step: 156579, loss:  0.011776376516\n",
      "training step: 156620, loss:  0.024754593149\n",
      "training step: 156661, loss:  0.018698843196\n",
      "training step: 156702, loss:  0.003473303979\n",
      "training step: 156743, loss:  0.025363046676\n",
      "training step: 156784, loss:  0.007752379868\n",
      "training step: 156825, loss:  0.003473303979\n",
      "training step: 156866, loss:  0.003808671376\n",
      "training step: 156907, loss:  0.003135976614\n",
      "training step: 156948, loss:  0.006285338197\n",
      "training step: 156989, loss:  0.002975780051\n",
      "training step: 157030, loss:  0.014145905152\n",
      "training step: 157071, loss:  0.003986798227\n",
      "training step: 157112, loss:  0.003900909098\n",
      "training step: 157153, loss:  0.005485921632\n",
      "training step: 157194, loss:  0.003808671376\n",
      "training step: 157235, loss:  0.007341358811\n",
      "training step: 157276, loss:  0.003135976614\n",
      "training step: 157317, loss:  0.002396283671\n",
      "training step: 157358, loss:  0.003880497534\n",
      "training step: 157399, loss:  0.002975780051\n",
      "training step: 157440, loss:  0.014145905152\n",
      "training step: 157481, loss:  0.014145905152\n",
      "training step: 157522, loss:  0.007752379868\n",
      "training step: 157563, loss:  0.003135976614\n",
      "training step: 157604, loss:  0.025363046676\n",
      "training step: 157645, loss:  0.023757282645\n",
      "training step: 157686, loss:  0.003467082279\n",
      "training step: 157727, loss:  0.020691098645\n",
      "training step: 157768, loss:  0.003543942003\n",
      "training step: 157809, loss:  0.003900909098\n",
      "training step: 157850, loss:  0.009004401974\n",
      "training step: 157891, loss:  0.009473215789\n",
      "training step: 157932, loss:  0.014145905152\n",
      "training step: 157973, loss:  0.003986798227\n",
      "training step: 158014, loss:  0.005003615282\n",
      "training step: 158055, loss:  0.003473303979\n",
      "training step: 158096, loss:  0.023757282645\n",
      "training step: 158137, loss:  0.003120336682\n",
      "training step: 158178, loss:  0.009004401974\n",
      "training step: 158219, loss:  0.003284437582\n",
      "training step: 158260, loss:  0.002930165734\n",
      "training step: 158301, loss:  0.023757282645\n",
      "training step: 158342, loss:  0.014145905152\n",
      "training step: 158383, loss:  0.007752379868\n",
      "training step: 158424, loss:  0.003638759954\n",
      "training step: 158465, loss:  0.003880497534\n",
      "training step: 158506, loss:  0.003135976614\n",
      "training step: 158547, loss:  0.007877553813\n",
      "training step: 158588, loss:  0.005715856329\n",
      "training step: 158629, loss:  0.005715856329\n",
      "training step: 158670, loss:  0.003135976614\n",
      "training step: 158711, loss:  0.003135976614\n",
      "training step: 158752, loss:  0.003900909098\n",
      "training step: 158793, loss:  0.003473303979\n",
      "training step: 158834, loss:  0.005003615282\n",
      "training step: 158875, loss:  0.022392421961\n",
      "training step: 158916, loss:  0.003002761398\n",
      "training step: 158957, loss:  0.025363046676\n",
      "training step: 158998, loss:  0.003638759954\n",
      "training step: 159039, loss:  0.002396283671\n",
      "training step: 159080, loss:  0.004080093466\n",
      "training step: 159121, loss:  0.025568028912\n",
      "training step: 159162, loss:  0.014145905152\n",
      "training step: 159203, loss:  0.007341358811\n",
      "training step: 159244, loss:  0.003473303979\n",
      "training step: 159285, loss:  0.003473303979\n",
      "training step: 159326, loss:  0.004080093466\n",
      "training step: 159367, loss:  0.004080093466\n",
      "training step: 159408, loss:  0.003808671376\n",
      "training step: 159449, loss:  0.007877553813\n",
      "training step: 159490, loss:  0.003120336682\n",
      "training step: 159531, loss:  0.003866109066\n",
      "training step: 159572, loss:  0.002975780051\n",
      "training step: 159613, loss:  0.005715856329\n",
      "training step: 159654, loss:  0.003880497534\n",
      "training step: 159695, loss:  0.004080093466\n",
      "training step: 159736, loss:  0.003135976614\n",
      "training step: 159777, loss:  0.007752379868\n",
      "training step: 159818, loss:  0.002975780051\n",
      "training step: 159859, loss:  0.009004401974\n",
      "training step: 159900, loss:  0.003776642960\n",
      "training step: 159941, loss:  0.023757282645\n",
      "training step: 159982, loss:  0.024754593149\n",
      "training step: 160023, loss:  0.003900909098\n",
      "training step: 160064, loss:  0.003473303979\n",
      "training step: 160105, loss:  0.003011468332\n",
      "training step: 160146, loss:  0.002396283671\n",
      "training step: 160187, loss:  0.003120336682\n",
      "training step: 160228, loss:  0.014145905152\n",
      "training step: 160269, loss:  0.002386306180\n",
      "training step: 160310, loss:  0.003135976614\n",
      "training step: 160351, loss:  0.005003615282\n",
      "training step: 160392, loss:  0.007752379868\n",
      "training step: 160433, loss:  0.002396283671\n",
      "training step: 160474, loss:  0.003284437582\n",
      "training step: 160515, loss:  0.024754593149\n",
      "training step: 160556, loss:  0.002975780051\n",
      "training step: 160597, loss:  0.003135976614\n",
      "training step: 160638, loss:  0.023757282645\n",
      "training step: 160679, loss:  0.006285338197\n",
      "training step: 160720, loss:  0.003880497534\n",
      "training step: 160761, loss:  0.018698843196\n",
      "training step: 160802, loss:  0.002930165734\n",
      "training step: 160843, loss:  0.003467082279\n",
      "training step: 160884, loss:  0.022392421961\n",
      "training step: 160925, loss:  0.005003615282\n",
      "training step: 160966, loss:  0.025568028912\n",
      "training step: 161007, loss:  0.003011468332\n",
      "training step: 161048, loss:  0.003638759954\n",
      "training step: 161089, loss:  0.003880497534\n",
      "training step: 161130, loss:  0.022392421961\n",
      "training step: 161171, loss:  0.003776642960\n",
      "training step: 161212, loss:  0.005003615282\n",
      "training step: 161253, loss:  0.023757282645\n",
      "training step: 161294, loss:  0.016485802829\n",
      "training step: 161335, loss:  0.003467082279\n",
      "training step: 161376, loss:  0.002386306180\n",
      "training step: 161417, loss:  0.003284437582\n",
      "training step: 161458, loss:  0.002930165734\n",
      "training step: 161499, loss:  0.003467082279\n",
      "training step: 161540, loss:  0.018698843196\n",
      "training step: 161581, loss:  0.020691098645\n",
      "training step: 161622, loss:  0.003002761398\n",
      "training step: 161663, loss:  0.005485921632\n",
      "training step: 161704, loss:  0.003002761398\n",
      "training step: 161745, loss:  0.002930165734\n",
      "training step: 161786, loss:  0.016485802829\n",
      "training step: 161827, loss:  0.016485802829\n",
      "training step: 161868, loss:  0.003543942003\n",
      "training step: 161909, loss:  0.020691098645\n",
      "training step: 161950, loss:  0.018698843196\n",
      "training step: 161991, loss:  0.005003615282\n",
      "training step: 162032, loss:  0.007877553813\n",
      "training step: 162073, loss:  0.002930165734\n",
      "training step: 162114, loss:  0.023757282645\n",
      "training step: 162155, loss:  0.002975780051\n",
      "training step: 162196, loss:  0.003543942003\n",
      "training step: 162237, loss:  0.007752379868\n",
      "training step: 162278, loss:  0.003467082279\n",
      "training step: 162319, loss:  0.014145905152\n",
      "training step: 162360, loss:  0.009004401974\n",
      "training step: 162401, loss:  0.002975780051\n",
      "training step: 162442, loss:  0.003692979924\n",
      "training step: 162483, loss:  0.020691098645\n",
      "training step: 162524, loss:  0.002975780051\n",
      "training step: 162565, loss:  0.007752379868\n",
      "training step: 162606, loss:  0.003866109066\n",
      "training step: 162647, loss:  0.016485802829\n",
      "training step: 162688, loss:  0.003866109066\n",
      "training step: 162729, loss:  0.002930165734\n",
      "training step: 162770, loss:  0.009004401974\n",
      "training step: 162811, loss:  0.006285338197\n",
      "training step: 162852, loss:  0.005003615282\n",
      "training step: 162893, loss:  0.004087368958\n",
      "training step: 162934, loss:  0.002930165734\n",
      "training step: 162975, loss:  0.002386306180\n",
      "training step: 163016, loss:  0.004080093466\n",
      "training step: 163057, loss:  0.004087368958\n",
      "training step: 163098, loss:  0.004087368958\n",
      "training step: 163139, loss:  0.003986798227\n",
      "training step: 163180, loss:  0.003808671376\n",
      "training step: 163221, loss:  0.005003615282\n",
      "training step: 163262, loss:  0.005715856329\n",
      "training step: 163303, loss:  0.004080093466\n",
      "training step: 163344, loss:  0.025568028912\n",
      "training step: 163385, loss:  0.007752379868\n",
      "training step: 163426, loss:  0.002975780051\n",
      "training step: 163467, loss:  0.003284437582\n",
      "training step: 163508, loss:  0.024754593149\n",
      "training step: 163549, loss:  0.003002761398\n",
      "training step: 163590, loss:  0.003900909098\n",
      "training step: 163631, loss:  0.003002761398\n",
      "training step: 163672, loss:  0.004087368958\n",
      "training step: 163713, loss:  0.023757282645\n",
      "training step: 163754, loss:  0.007341358811\n",
      "training step: 163795, loss:  0.003638759954\n",
      "training step: 163836, loss:  0.005003615282\n",
      "training step: 163877, loss:  0.002396283671\n",
      "training step: 163918, loss:  0.025568028912\n",
      "training step: 163959, loss:  0.003808671376\n",
      "training step: 164000, loss:  0.002930165734\n",
      "training step: 164041, loss:  0.005715856329\n",
      "training step: 164082, loss:  0.007877553813\n",
      "training step: 164123, loss:  0.002396283671\n",
      "training step: 164164, loss:  0.009473215789\n",
      "training step: 164205, loss:  0.007341358811\n",
      "training step: 164246, loss:  0.003900909098\n",
      "training step: 164287, loss:  0.003638759954\n",
      "training step: 164328, loss:  0.007877553813\n",
      "training step: 164369, loss:  0.011776376516\n",
      "training step: 164410, loss:  0.024754593149\n",
      "training step: 164451, loss:  0.003120336682\n",
      "training step: 164492, loss:  0.009473215789\n",
      "training step: 164533, loss:  0.007752379868\n",
      "training step: 164574, loss:  0.002396283671\n",
      "training step: 164615, loss:  0.014145905152\n",
      "training step: 164656, loss:  0.006285338197\n",
      "training step: 164697, loss:  0.003011468332\n",
      "training step: 164738, loss:  0.003284437582\n",
      "training step: 164779, loss:  0.002930165734\n",
      "training step: 164820, loss:  0.007341358811\n",
      "training step: 164861, loss:  0.003011468332\n",
      "training step: 164902, loss:  0.007752379868\n",
      "training step: 164943, loss:  0.003776642960\n",
      "training step: 164984, loss:  0.009473215789\n",
      "training step: 165025, loss:  0.007877553813\n",
      "training step: 165066, loss:  0.002386306180\n",
      "training step: 165107, loss:  0.007341358811\n",
      "training step: 165148, loss:  0.003011468332\n",
      "training step: 165189, loss:  0.005715856329\n",
      "training step: 165230, loss:  0.003986798227\n",
      "training step: 165271, loss:  0.005485921632\n",
      "training step: 165312, loss:  0.003120336682\n",
      "training step: 165353, loss:  0.003011468332\n",
      "training step: 165394, loss:  0.007877553813\n",
      "training step: 165435, loss:  0.004080093466\n",
      "training step: 165476, loss:  0.025363046676\n",
      "training step: 165517, loss:  0.007877553813\n",
      "training step: 165558, loss:  0.007752379868\n",
      "training step: 165599, loss:  0.002396283671\n",
      "training step: 165640, loss:  0.003900909098\n",
      "training step: 165681, loss:  0.016485802829\n",
      "training step: 165722, loss:  0.003880497534\n",
      "training step: 165763, loss:  0.022392421961\n",
      "training step: 165804, loss:  0.005485921632\n",
      "training step: 165845, loss:  0.011776376516\n",
      "training step: 165886, loss:  0.025363046676\n",
      "training step: 165927, loss:  0.003120336682\n",
      "training step: 165968, loss:  0.022392421961\n",
      "training step: 166009, loss:  0.016485802829\n",
      "training step: 166050, loss:  0.024754593149\n",
      "training step: 166091, loss:  0.003866109066\n",
      "training step: 166132, loss:  0.025568028912\n",
      "training step: 166173, loss:  0.005715856329\n",
      "training step: 166214, loss:  0.003866109066\n",
      "training step: 166255, loss:  0.003002761398\n",
      "training step: 166296, loss:  0.003638759954\n",
      "training step: 166337, loss:  0.023757282645\n",
      "training step: 166378, loss:  0.003473303979\n",
      "training step: 166419, loss:  0.003880497534\n",
      "training step: 166460, loss:  0.003284437582\n",
      "training step: 166501, loss:  0.002930165734\n",
      "training step: 166542, loss:  0.020691098645\n",
      "training step: 166583, loss:  0.003543942003\n",
      "training step: 166624, loss:  0.003120336682\n",
      "training step: 166665, loss:  0.025363046676\n",
      "training step: 166706, loss:  0.005715856329\n",
      "training step: 166747, loss:  0.016485802829\n",
      "training step: 166788, loss:  0.003880497534\n",
      "training step: 166829, loss:  0.014145905152\n",
      "training step: 166870, loss:  0.003866109066\n",
      "training step: 166911, loss:  0.007752379868\n",
      "training step: 166952, loss:  0.002975780051\n",
      "training step: 166993, loss:  0.024754593149\n",
      "training step: 167034, loss:  0.003011468332\n",
      "training step: 167075, loss:  0.004087368958\n",
      "training step: 167116, loss:  0.003638759954\n",
      "training step: 167157, loss:  0.003638759954\n",
      "training step: 167198, loss:  0.009473215789\n",
      "training step: 167239, loss:  0.003135976614\n",
      "training step: 167280, loss:  0.007752379868\n",
      "training step: 167321, loss:  0.003467082279\n",
      "training step: 167362, loss:  0.003638759954\n",
      "training step: 167403, loss:  0.020691098645\n",
      "training step: 167444, loss:  0.007752379868\n",
      "training step: 167485, loss:  0.003692979924\n",
      "training step: 167526, loss:  0.003467082279\n",
      "training step: 167567, loss:  0.003900909098\n",
      "training step: 167608, loss:  0.003880497534\n",
      "training step: 167649, loss:  0.003638759954\n",
      "training step: 167690, loss:  0.003776642960\n",
      "training step: 167731, loss:  0.003692979924\n",
      "training step: 167772, loss:  0.007341358811\n",
      "training step: 167813, loss:  0.003638759954\n",
      "training step: 167854, loss:  0.003692979924\n",
      "training step: 167895, loss:  0.003467082279\n",
      "training step: 167936, loss:  0.009473215789\n",
      "training step: 167977, loss:  0.002396283671\n",
      "training step: 168018, loss:  0.002930165734\n",
      "training step: 168059, loss:  0.003002761398\n",
      "training step: 168100, loss:  0.003638759954\n",
      "training step: 168141, loss:  0.007752379868\n",
      "training step: 168182, loss:  0.011776376516\n",
      "training step: 168223, loss:  0.020691098645\n",
      "training step: 168264, loss:  0.016485802829\n",
      "training step: 168305, loss:  0.004080093466\n",
      "training step: 168346, loss:  0.016485802829\n",
      "training step: 168387, loss:  0.003002761398\n",
      "training step: 168428, loss:  0.002396283671\n",
      "training step: 168469, loss:  0.003692979924\n",
      "training step: 168510, loss:  0.020691098645\n",
      "training step: 168551, loss:  0.004080093466\n",
      "training step: 168592, loss:  0.006285338197\n",
      "training step: 168633, loss:  0.014145905152\n",
      "training step: 168674, loss:  0.025568028912\n",
      "training step: 168715, loss:  0.004080093466\n",
      "training step: 168756, loss:  0.003880497534\n",
      "training step: 168797, loss:  0.003002761398\n",
      "training step: 168838, loss:  0.003900909098\n",
      "training step: 168879, loss:  0.005485921632\n",
      "training step: 168920, loss:  0.003011468332\n",
      "training step: 168961, loss:  0.003011468332\n",
      "training step: 169002, loss:  0.007877553813\n",
      "training step: 169043, loss:  0.014145905152\n",
      "training step: 169084, loss:  0.003120336682\n",
      "training step: 169125, loss:  0.003135976614\n",
      "training step: 169166, loss:  0.002930165734\n",
      "training step: 169207, loss:  0.003120336682\n",
      "training step: 169248, loss:  0.003866109066\n",
      "training step: 169289, loss:  0.003986798227\n",
      "training step: 169330, loss:  0.025363046676\n",
      "training step: 169371, loss:  0.007752379868\n",
      "training step: 169412, loss:  0.004087368958\n",
      "training step: 169453, loss:  0.003866109066\n",
      "training step: 169494, loss:  0.003473303979\n",
      "training step: 169535, loss:  0.023757282645\n",
      "training step: 169576, loss:  0.023757282645\n",
      "training step: 169617, loss:  0.009004401974\n",
      "training step: 169658, loss:  0.003284437582\n",
      "training step: 169699, loss:  0.003002761398\n",
      "training step: 169740, loss:  0.003543942003\n",
      "training step: 169781, loss:  0.004087368958\n",
      "training step: 169822, loss:  0.003692979924\n",
      "training step: 169863, loss:  0.003011468332\n",
      "training step: 169904, loss:  0.003543942003\n",
      "training step: 169945, loss:  0.003692979924\n",
      "training step: 169986, loss:  0.003880497534\n",
      "training step: 170027, loss:  0.007877553813\n",
      "training step: 170068, loss:  0.005003615282\n",
      "training step: 170109, loss:  0.003900909098\n",
      "training step: 170150, loss:  0.009004401974\n",
      "training step: 170191, loss:  0.009004401974\n",
      "training step: 170232, loss:  0.003986798227\n",
      "training step: 170273, loss:  0.005003615282\n",
      "training step: 170314, loss:  0.005715856329\n",
      "training step: 170355, loss:  0.003284437582\n",
      "training step: 170396, loss:  0.003880497534\n",
      "training step: 170437, loss:  0.025568028912\n",
      "training step: 170478, loss:  0.003002761398\n",
      "training step: 170519, loss:  0.005715856329\n",
      "training step: 170560, loss:  0.003473303979\n",
      "training step: 170601, loss:  0.007752379868\n",
      "training step: 170642, loss:  0.003986798227\n",
      "training step: 170683, loss:  0.007752379868\n",
      "training step: 170724, loss:  0.003467082279\n",
      "training step: 170765, loss:  0.009004401974\n",
      "training step: 170806, loss:  0.002396283671\n",
      "training step: 170847, loss:  0.025363046676\n",
      "training step: 170888, loss:  0.011776376516\n",
      "training step: 170929, loss:  0.002386306180\n",
      "training step: 170970, loss:  0.005715856329\n",
      "training step: 171011, loss:  0.003866109066\n",
      "training step: 171052, loss:  0.005003615282\n",
      "training step: 171093, loss:  0.023757282645\n",
      "training step: 171134, loss:  0.005003615282\n",
      "training step: 171175, loss:  0.018698843196\n",
      "training step: 171216, loss:  0.003866109066\n",
      "training step: 171257, loss:  0.007341358811\n",
      "training step: 171298, loss:  0.014145905152\n",
      "training step: 171339, loss:  0.016485802829\n",
      "training step: 171380, loss:  0.023757282645\n",
      "training step: 171421, loss:  0.003011468332\n",
      "training step: 171462, loss:  0.002396283671\n",
      "training step: 171503, loss:  0.002975780051\n",
      "training step: 171544, loss:  0.002386306180\n",
      "training step: 171585, loss:  0.003986798227\n",
      "training step: 171626, loss:  0.004087368958\n",
      "training step: 171667, loss:  0.003467082279\n",
      "training step: 171708, loss:  0.004080093466\n",
      "training step: 171749, loss:  0.005715856329\n",
      "training step: 171790, loss:  0.003135976614\n",
      "training step: 171831, loss:  0.022392421961\n",
      "training step: 171872, loss:  0.003692979924\n",
      "training step: 171913, loss:  0.004080093466\n",
      "training step: 171954, loss:  0.022392421961\n",
      "training step: 171995, loss:  0.009473215789\n",
      "training step: 172036, loss:  0.003866109066\n",
      "training step: 172077, loss:  0.005715856329\n",
      "training step: 172118, loss:  0.003135976614\n",
      "training step: 172159, loss:  0.025568028912\n",
      "training step: 172200, loss:  0.003467082279\n",
      "training step: 172241, loss:  0.007877553813\n",
      "training step: 172282, loss:  0.003638759954\n",
      "training step: 172323, loss:  0.003638759954\n",
      "training step: 172364, loss:  0.023757282645\n",
      "training step: 172405, loss:  0.002930165734\n",
      "training step: 172446, loss:  0.003120336682\n",
      "training step: 172487, loss:  0.003638759954\n",
      "training step: 172528, loss:  0.003284437582\n",
      "training step: 172569, loss:  0.018698843196\n",
      "training step: 172610, loss:  0.003284437582\n",
      "training step: 172651, loss:  0.002930165734\n",
      "training step: 172692, loss:  0.009004401974\n",
      "training step: 172733, loss:  0.003011468332\n",
      "training step: 172774, loss:  0.007752379868\n",
      "training step: 172815, loss:  0.003002761398\n",
      "training step: 172856, loss:  0.007752379868\n",
      "training step: 172897, loss:  0.016485802829\n",
      "training step: 172938, loss:  0.020691098645\n",
      "training step: 172979, loss:  0.011776376516\n",
      "training step: 173020, loss:  0.002930165734\n",
      "training step: 173061, loss:  0.003135976614\n",
      "training step: 173102, loss:  0.003011468332\n",
      "training step: 173143, loss:  0.006285338197\n",
      "training step: 173184, loss:  0.023757282645\n",
      "training step: 173225, loss:  0.003543942003\n",
      "training step: 173266, loss:  0.009473215789\n",
      "training step: 173307, loss:  0.007877553813\n",
      "training step: 173348, loss:  0.007341358811\n",
      "training step: 173389, loss:  0.003473303979\n",
      "training step: 173430, loss:  0.003011468332\n",
      "training step: 173471, loss:  0.003776642960\n",
      "training step: 173512, loss:  0.005715856329\n",
      "training step: 173553, loss:  0.005485921632\n",
      "training step: 173594, loss:  0.025568028912\n",
      "training step: 173635, loss:  0.004087368958\n",
      "training step: 173676, loss:  0.003543942003\n",
      "training step: 173717, loss:  0.003692979924\n",
      "training step: 173758, loss:  0.009004401974\n",
      "training step: 173799, loss:  0.003011468332\n",
      "training step: 173840, loss:  0.018698843196\n",
      "training step: 173881, loss:  0.009473215789\n",
      "training step: 173922, loss:  0.007752379868\n",
      "training step: 173963, loss:  0.007341358811\n",
      "training step: 174004, loss:  0.024754593149\n",
      "training step: 174045, loss:  0.005715856329\n",
      "training step: 174086, loss:  0.009473215789\n",
      "training step: 174127, loss:  0.002930165734\n",
      "training step: 174168, loss:  0.003692979924\n",
      "training step: 174209, loss:  0.020691098645\n",
      "training step: 174250, loss:  0.007341358811\n",
      "training step: 174291, loss:  0.007341358811\n",
      "training step: 174332, loss:  0.003900909098\n",
      "training step: 174373, loss:  0.002975780051\n",
      "training step: 174414, loss:  0.009473215789\n",
      "training step: 174455, loss:  0.003120336682\n",
      "training step: 174496, loss:  0.003880497534\n",
      "training step: 174537, loss:  0.007341358811\n",
      "training step: 174578, loss:  0.005003615282\n",
      "training step: 174619, loss:  0.004080093466\n",
      "training step: 174660, loss:  0.005485921632\n",
      "training step: 174701, loss:  0.004087368958\n",
      "training step: 174742, loss:  0.003880497534\n",
      "training step: 174783, loss:  0.014145905152\n",
      "training step: 174824, loss:  0.003776642960\n",
      "training step: 174865, loss:  0.003880497534\n",
      "training step: 174906, loss:  0.002930165734\n",
      "training step: 174947, loss:  0.003866109066\n",
      "training step: 174988, loss:  0.020691098645\n",
      "training step: 175029, loss:  0.023757282645\n",
      "training step: 175070, loss:  0.003866109066\n",
      "training step: 175111, loss:  0.002386306180\n",
      "training step: 175152, loss:  0.009004401974\n",
      "training step: 175193, loss:  0.002396283671\n",
      "training step: 175234, loss:  0.004087368958\n",
      "training step: 175275, loss:  0.003866109066\n",
      "training step: 175316, loss:  0.003900909098\n",
      "training step: 175357, loss:  0.003638759954\n",
      "training step: 175398, loss:  0.003543942003\n",
      "training step: 175439, loss:  0.005715856329\n",
      "training step: 175480, loss:  0.009004401974\n",
      "training step: 175521, loss:  0.003692979924\n",
      "training step: 175562, loss:  0.004087368958\n",
      "training step: 175603, loss:  0.005485921632\n",
      "training step: 175644, loss:  0.007341358811\n",
      "training step: 175685, loss:  0.007341358811\n",
      "training step: 175726, loss:  0.014145905152\n",
      "training step: 175767, loss:  0.003986798227\n",
      "training step: 175808, loss:  0.009004401974\n",
      "training step: 175849, loss:  0.003473303979\n",
      "training step: 175890, loss:  0.011776376516\n",
      "training step: 175931, loss:  0.002396283671\n",
      "training step: 175972, loss:  0.003986798227\n",
      "training step: 176013, loss:  0.009473215789\n",
      "training step: 176054, loss:  0.004087368958\n",
      "training step: 176095, loss:  0.011776376516\n",
      "training step: 176136, loss:  0.002396283671\n",
      "training step: 176177, loss:  0.014145905152\n",
      "training step: 176218, loss:  0.003002761398\n",
      "training step: 176259, loss:  0.009473215789\n",
      "training step: 176300, loss:  0.009473215789\n",
      "training step: 176341, loss:  0.004087368958\n",
      "training step: 176382, loss:  0.003808671376\n",
      "training step: 176423, loss:  0.004080093466\n",
      "training step: 176464, loss:  0.002396283671\n",
      "training step: 176505, loss:  0.003776642960\n",
      "training step: 176546, loss:  0.009004401974\n",
      "training step: 176587, loss:  0.022392421961\n",
      "training step: 176628, loss:  0.023757282645\n",
      "training step: 176669, loss:  0.025363046676\n",
      "training step: 176710, loss:  0.003638759954\n",
      "training step: 176751, loss:  0.003011468332\n",
      "training step: 176792, loss:  0.002975780051\n",
      "training step: 176833, loss:  0.005485921632\n",
      "training step: 176874, loss:  0.003002761398\n",
      "training step: 176915, loss:  0.003900909098\n",
      "training step: 176956, loss:  0.007752379868\n",
      "training step: 176997, loss:  0.003776642960\n",
      "training step: 177038, loss:  0.007877553813\n",
      "training step: 177079, loss:  0.003808671376\n",
      "training step: 177120, loss:  0.006285338197\n",
      "training step: 177161, loss:  0.005003615282\n",
      "training step: 177202, loss:  0.005715856329\n",
      "training step: 177243, loss:  0.003900909098\n",
      "training step: 177284, loss:  0.025568028912\n",
      "training step: 177325, loss:  0.003135976614\n",
      "training step: 177366, loss:  0.005715856329\n",
      "training step: 177407, loss:  0.003880497534\n",
      "training step: 177448, loss:  0.003543942003\n",
      "training step: 177489, loss:  0.005715856329\n",
      "training step: 177530, loss:  0.005715856329\n",
      "training step: 177571, loss:  0.003866109066\n",
      "training step: 177612, loss:  0.005003615282\n",
      "training step: 177653, loss:  0.009004401974\n",
      "training step: 177694, loss:  0.007877553813\n",
      "training step: 177735, loss:  0.002386306180\n",
      "training step: 177776, loss:  0.003986798227\n",
      "training step: 177817, loss:  0.006285338197\n",
      "training step: 177858, loss:  0.007752379868\n",
      "training step: 177899, loss:  0.005485921632\n",
      "training step: 177940, loss:  0.025363046676\n",
      "training step: 177981, loss:  0.003986798227\n",
      "training step: 178022, loss:  0.023757282645\n",
      "training step: 178063, loss:  0.003284437582\n",
      "training step: 178104, loss:  0.003473303979\n",
      "training step: 178145, loss:  0.005715856329\n",
      "training step: 178186, loss:  0.020691098645\n",
      "training step: 178227, loss:  0.004080093466\n",
      "training step: 178268, loss:  0.022392421961\n",
      "training step: 178309, loss:  0.002975780051\n",
      "training step: 178350, loss:  0.022392421961\n",
      "training step: 178391, loss:  0.003638759954\n",
      "training step: 178432, loss:  0.005715856329\n",
      "training step: 178473, loss:  0.003638759954\n",
      "training step: 178514, loss:  0.006285338197\n",
      "training step: 178555, loss:  0.016485802829\n",
      "training step: 178596, loss:  0.003473303979\n",
      "training step: 178637, loss:  0.002396283671\n",
      "training step: 178678, loss:  0.003692979924\n",
      "training step: 178719, loss:  0.002386306180\n",
      "training step: 178760, loss:  0.002386306180\n",
      "training step: 178801, loss:  0.025363046676\n",
      "training step: 178842, loss:  0.005715856329\n",
      "training step: 178883, loss:  0.003900909098\n",
      "training step: 178924, loss:  0.003692979924\n",
      "training step: 178965, loss:  0.023757282645\n",
      "training step: 179006, loss:  0.002396283671\n",
      "training step: 179047, loss:  0.002930165734\n",
      "training step: 179088, loss:  0.003284437582\n",
      "training step: 179129, loss:  0.016485802829\n",
      "training step: 179170, loss:  0.016485802829\n",
      "training step: 179211, loss:  0.003467082279\n",
      "training step: 179252, loss:  0.023757282645\n",
      "training step: 179293, loss:  0.003120336682\n",
      "training step: 179334, loss:  0.020691098645\n",
      "training step: 179375, loss:  0.002396283671\n",
      "training step: 179416, loss:  0.003135976614\n",
      "training step: 179457, loss:  0.007752379868\n",
      "training step: 179498, loss:  0.003467082279\n",
      "training step: 179539, loss:  0.003900909098\n",
      "training step: 179580, loss:  0.002930165734\n",
      "training step: 179621, loss:  0.002386306180\n",
      "training step: 179662, loss:  0.009473215789\n",
      "training step: 179703, loss:  0.004087368958\n",
      "training step: 179744, loss:  0.016485802829\n",
      "training step: 179785, loss:  0.007752379868\n",
      "training step: 179826, loss:  0.003284437582\n",
      "training step: 179867, loss:  0.003808671376\n",
      "training step: 179908, loss:  0.003776642960\n",
      "training step: 179949, loss:  0.003011468332\n",
      "training step: 179990, loss:  0.009473215789\n",
      "training step: 180031, loss:  0.014145905152\n",
      "training step: 180072, loss:  0.003284437582\n",
      "training step: 180113, loss:  0.016485802829\n",
      "training step: 180154, loss:  0.003900909098\n",
      "training step: 180195, loss:  0.006285338197\n",
      "training step: 180236, loss:  0.003284437582\n",
      "training step: 180277, loss:  0.022392421961\n",
      "training step: 180318, loss:  0.007341358811\n",
      "training step: 180359, loss:  0.004087368958\n",
      "training step: 180400, loss:  0.023757282645\n",
      "training step: 180441, loss:  0.020691098645\n",
      "training step: 180482, loss:  0.009473215789\n",
      "training step: 180523, loss:  0.025363046676\n",
      "training step: 180564, loss:  0.002975780051\n",
      "training step: 180605, loss:  0.003467082279\n",
      "training step: 180646, loss:  0.009473215789\n",
      "training step: 180687, loss:  0.005715856329\n",
      "training step: 180728, loss:  0.002396283671\n",
      "training step: 180769, loss:  0.022392421961\n",
      "training step: 180810, loss:  0.023757282645\n",
      "training step: 180851, loss:  0.007877553813\n",
      "training step: 180892, loss:  0.014145905152\n",
      "training step: 180933, loss:  0.003776642960\n",
      "training step: 180974, loss:  0.007752379868\n",
      "training step: 181015, loss:  0.020691098645\n",
      "training step: 181056, loss:  0.016485802829\n",
      "training step: 181097, loss:  0.025568028912\n",
      "training step: 181138, loss:  0.002386306180\n",
      "training step: 181179, loss:  0.016485802829\n",
      "training step: 181220, loss:  0.003776642960\n",
      "training step: 181261, loss:  0.003284437582\n",
      "training step: 181302, loss:  0.002930165734\n",
      "training step: 181343, loss:  0.020691098645\n",
      "training step: 181384, loss:  0.005485921632\n",
      "training step: 181425, loss:  0.005715856329\n",
      "training step: 181466, loss:  0.003284437582\n",
      "training step: 181507, loss:  0.025363046676\n",
      "training step: 181548, loss:  0.003900909098\n",
      "training step: 181589, loss:  0.003543942003\n",
      "training step: 181630, loss:  0.003473303979\n",
      "training step: 181671, loss:  0.025568028912\n",
      "training step: 181712, loss:  0.004087368958\n",
      "training step: 181753, loss:  0.003986798227\n",
      "training step: 181794, loss:  0.002975780051\n",
      "training step: 181835, loss:  0.003120336682\n",
      "training step: 181876, loss:  0.003135976614\n",
      "training step: 181917, loss:  0.025568028912\n",
      "training step: 181958, loss:  0.003986798227\n",
      "training step: 181999, loss:  0.003808671376\n",
      "training step: 182040, loss:  0.003808671376\n",
      "training step: 182081, loss:  0.002396283671\n",
      "training step: 182122, loss:  0.007341358811\n",
      "training step: 182163, loss:  0.014145905152\n",
      "training step: 182204, loss:  0.016485802829\n",
      "training step: 182245, loss:  0.003473303979\n",
      "training step: 182286, loss:  0.003011468332\n",
      "training step: 182327, loss:  0.007752379868\n",
      "training step: 182368, loss:  0.024754593149\n",
      "training step: 182409, loss:  0.004087368958\n",
      "training step: 182450, loss:  0.016485802829\n",
      "training step: 182491, loss:  0.003808671376\n",
      "training step: 182532, loss:  0.018698843196\n",
      "training step: 182573, loss:  0.003135976614\n",
      "training step: 182614, loss:  0.003880497534\n",
      "training step: 182655, loss:  0.025363046676\n",
      "training step: 182696, loss:  0.005485921632\n",
      "training step: 182737, loss:  0.025568028912\n",
      "training step: 182778, loss:  0.018698843196\n",
      "training step: 182819, loss:  0.009004401974\n",
      "training step: 182860, loss:  0.002396283671\n",
      "training step: 182901, loss:  0.003002761398\n",
      "training step: 182942, loss:  0.003692979924\n",
      "training step: 182983, loss:  0.002386306180\n",
      "training step: 183024, loss:  0.003776642960\n",
      "training step: 183065, loss:  0.002975780051\n",
      "training step: 183106, loss:  0.006285338197\n",
      "training step: 183147, loss:  0.003866109066\n",
      "training step: 183188, loss:  0.002930165734\n",
      "training step: 183229, loss:  0.002975780051\n",
      "training step: 183270, loss:  0.003467082279\n",
      "training step: 183311, loss:  0.003880497534\n",
      "training step: 183352, loss:  0.005485921632\n",
      "training step: 183393, loss:  0.004080093466\n",
      "training step: 183434, loss:  0.003986798227\n",
      "training step: 183475, loss:  0.003808671376\n",
      "training step: 183516, loss:  0.003986798227\n",
      "training step: 183557, loss:  0.003638759954\n",
      "training step: 183598, loss:  0.025363046676\n",
      "training step: 183639, loss:  0.003866109066\n",
      "training step: 183680, loss:  0.003986798227\n",
      "training step: 183721, loss:  0.005715856329\n",
      "training step: 183762, loss:  0.022392421961\n",
      "training step: 183803, loss:  0.025363046676\n",
      "training step: 183844, loss:  0.025363046676\n",
      "training step: 183885, loss:  0.018698843196\n",
      "training step: 183926, loss:  0.020691098645\n",
      "training step: 183967, loss:  0.002975780051\n",
      "training step: 184008, loss:  0.002396283671\n",
      "training step: 184049, loss:  0.003638759954\n",
      "training step: 184090, loss:  0.003467082279\n",
      "training step: 184131, loss:  0.024754593149\n",
      "training step: 184172, loss:  0.005715856329\n",
      "training step: 184213, loss:  0.007341358811\n",
      "training step: 184254, loss:  0.003638759954\n",
      "training step: 184295, loss:  0.009473215789\n",
      "training step: 184336, loss:  0.003011468332\n",
      "training step: 184377, loss:  0.003473303979\n",
      "training step: 184418, loss:  0.024754593149\n",
      "training step: 184459, loss:  0.003284437582\n",
      "training step: 184500, loss:  0.005003615282\n",
      "training step: 184541, loss:  0.002396283671\n",
      "training step: 184582, loss:  0.003467082279\n",
      "training step: 184623, loss:  0.002396283671\n",
      "training step: 184664, loss:  0.005715856329\n",
      "training step: 184705, loss:  0.006285338197\n",
      "training step: 184746, loss:  0.003543942003\n",
      "training step: 184787, loss:  0.002930165734\n",
      "training step: 184828, loss:  0.003002761398\n",
      "training step: 184869, loss:  0.024754593149\n",
      "training step: 184910, loss:  0.003002761398\n",
      "training step: 184951, loss:  0.003866109066\n",
      "training step: 184992, loss:  0.003692979924\n",
      "training step: 185033, loss:  0.006285338197\n",
      "training step: 185074, loss:  0.023757282645\n",
      "training step: 185115, loss:  0.009473215789\n",
      "training step: 185156, loss:  0.004080093466\n",
      "training step: 185197, loss:  0.003866109066\n",
      "training step: 185238, loss:  0.025363046676\n",
      "training step: 185279, loss:  0.016485802829\n",
      "training step: 185320, loss:  0.003986798227\n",
      "training step: 185361, loss:  0.014145905152\n",
      "training step: 185402, loss:  0.003002761398\n",
      "training step: 185443, loss:  0.025568028912\n",
      "training step: 185484, loss:  0.003473303979\n",
      "training step: 185525, loss:  0.003880497534\n",
      "training step: 185566, loss:  0.014145905152\n",
      "training step: 185607, loss:  0.003776642960\n",
      "training step: 185648, loss:  0.003900909098\n",
      "training step: 185689, loss:  0.011776376516\n",
      "training step: 185730, loss:  0.003467082279\n",
      "training step: 185771, loss:  0.003120336682\n",
      "training step: 185812, loss:  0.003002761398\n",
      "training step: 185853, loss:  0.005485921632\n",
      "training step: 185894, loss:  0.018698843196\n",
      "training step: 185935, loss:  0.003002761398\n",
      "training step: 185976, loss:  0.002386306180\n",
      "training step: 186017, loss:  0.003284437582\n",
      "training step: 186058, loss:  0.022392421961\n",
      "training step: 186099, loss:  0.025363046676\n",
      "training step: 186140, loss:  0.020691098645\n",
      "training step: 186181, loss:  0.002386306180\n",
      "training step: 186222, loss:  0.020691098645\n",
      "training step: 186263, loss:  0.023757282645\n",
      "training step: 186304, loss:  0.005485921632\n",
      "training step: 186345, loss:  0.003866109066\n",
      "training step: 186386, loss:  0.003467082279\n",
      "training step: 186427, loss:  0.003120336682\n",
      "training step: 186468, loss:  0.009004401974\n",
      "training step: 186509, loss:  0.024754593149\n",
      "training step: 186550, loss:  0.003011468332\n",
      "training step: 186591, loss:  0.020691098645\n",
      "training step: 186632, loss:  0.007877553813\n",
      "training step: 186673, loss:  0.004087368958\n",
      "training step: 186714, loss:  0.003543942003\n",
      "training step: 186755, loss:  0.003135976614\n",
      "training step: 186796, loss:  0.022392421961\n",
      "training step: 186837, loss:  0.003900909098\n",
      "training step: 186878, loss:  0.009473215789\n",
      "training step: 186919, loss:  0.025363046676\n",
      "training step: 186960, loss:  0.003638759954\n",
      "training step: 187001, loss:  0.003776642960\n",
      "training step: 187042, loss:  0.003120336682\n",
      "training step: 187083, loss:  0.009004401974\n",
      "training step: 187124, loss:  0.005003615282\n",
      "training step: 187165, loss:  0.009473215789\n",
      "training step: 187206, loss:  0.023757282645\n",
      "training step: 187247, loss:  0.003011468332\n",
      "training step: 187288, loss:  0.025568028912\n",
      "training step: 187329, loss:  0.024754593149\n",
      "training step: 187370, loss:  0.003776642960\n",
      "training step: 187411, loss:  0.003002761398\n",
      "training step: 187452, loss:  0.003692979924\n",
      "training step: 187493, loss:  0.020691098645\n",
      "training step: 187534, loss:  0.003135976614\n",
      "training step: 187575, loss:  0.024754593149\n",
      "training step: 187616, loss:  0.005003615282\n",
      "training step: 187657, loss:  0.003692979924\n",
      "training step: 187698, loss:  0.009473215789\n",
      "training step: 187739, loss:  0.002386306180\n",
      "training step: 187780, loss:  0.003638759954\n",
      "training step: 187821, loss:  0.006285338197\n",
      "training step: 187862, loss:  0.005003615282\n",
      "training step: 187903, loss:  0.009473215789\n",
      "training step: 187944, loss:  0.002975780051\n",
      "training step: 187985, loss:  0.022392421961\n",
      "training step: 188026, loss:  0.003473303979\n",
      "training step: 188067, loss:  0.003900909098\n",
      "training step: 188108, loss:  0.014145905152\n",
      "training step: 188149, loss:  0.023757282645\n",
      "training step: 188190, loss:  0.003473303979\n",
      "training step: 188231, loss:  0.003011468332\n",
      "training step: 188272, loss:  0.007752379868\n",
      "training step: 188313, loss:  0.005003615282\n",
      "training step: 188354, loss:  0.003692979924\n",
      "training step: 188395, loss:  0.005715856329\n",
      "training step: 188436, loss:  0.022392421961\n",
      "training step: 188477, loss:  0.016485802829\n",
      "training step: 188518, loss:  0.005485921632\n",
      "training step: 188559, loss:  0.006285338197\n",
      "training step: 188600, loss:  0.004087368958\n",
      "training step: 188641, loss:  0.011776376516\n",
      "training step: 188682, loss:  0.009004401974\n",
      "training step: 188723, loss:  0.003135976614\n",
      "training step: 188764, loss:  0.003880497534\n",
      "training step: 188805, loss:  0.002396283671\n",
      "training step: 188846, loss:  0.023757282645\n",
      "training step: 188887, loss:  0.003002761398\n",
      "training step: 188928, loss:  0.024754593149\n",
      "training step: 188969, loss:  0.003011468332\n",
      "training step: 189010, loss:  0.011776376516\n",
      "training step: 189051, loss:  0.003808671376\n",
      "training step: 189092, loss:  0.022392421961\n",
      "training step: 189133, loss:  0.002386306180\n",
      "training step: 189174, loss:  0.016485802829\n",
      "training step: 189215, loss:  0.011776376516\n",
      "training step: 189256, loss:  0.003473303979\n",
      "training step: 189297, loss:  0.003880497534\n",
      "training step: 189338, loss:  0.005485921632\n",
      "training step: 189379, loss:  0.005003615282\n",
      "training step: 189420, loss:  0.006285338197\n",
      "training step: 189461, loss:  0.003467082279\n",
      "training step: 189502, loss:  0.007752379868\n",
      "training step: 189543, loss:  0.007752379868\n",
      "training step: 189584, loss:  0.025363046676\n",
      "training step: 189625, loss:  0.009473215789\n",
      "training step: 189666, loss:  0.018698843196\n",
      "training step: 189707, loss:  0.025568028912\n",
      "training step: 189748, loss:  0.003543942003\n",
      "training step: 189789, loss:  0.003002761398\n",
      "training step: 189830, loss:  0.020691098645\n",
      "training step: 189871, loss:  0.005003615282\n",
      "training step: 189912, loss:  0.011776376516\n",
      "training step: 189953, loss:  0.024754593149\n",
      "training step: 189994, loss:  0.011776376516\n",
      "training step: 190035, loss:  0.009004401974\n",
      "training step: 190076, loss:  0.002930165734\n",
      "training step: 190117, loss:  0.023757282645\n",
      "training step: 190158, loss:  0.003543942003\n",
      "training step: 190199, loss:  0.006285338197\n",
      "training step: 190240, loss:  0.003986798227\n",
      "training step: 190281, loss:  0.002975780051\n",
      "training step: 190322, loss:  0.003543942003\n",
      "training step: 190363, loss:  0.003284437582\n",
      "training step: 190404, loss:  0.003011468332\n",
      "training step: 190445, loss:  0.006285338197\n",
      "training step: 190486, loss:  0.003473303979\n",
      "training step: 190527, loss:  0.005485921632\n",
      "training step: 190568, loss:  0.009473215789\n",
      "training step: 190609, loss:  0.004080093466\n",
      "training step: 190650, loss:  0.006285338197\n",
      "training step: 190691, loss:  0.002396283671\n",
      "training step: 190732, loss:  0.022392421961\n",
      "training step: 190773, loss:  0.018698843196\n",
      "training step: 190814, loss:  0.007877553813\n",
      "training step: 190855, loss:  0.003467082279\n",
      "training step: 190896, loss:  0.023757282645\n",
      "training step: 190937, loss:  0.003776642960\n",
      "training step: 190978, loss:  0.009473215789\n",
      "training step: 191019, loss:  0.024754593149\n",
      "training step: 191060, loss:  0.007877553813\n",
      "training step: 191101, loss:  0.005003615282\n",
      "training step: 191142, loss:  0.003120336682\n",
      "training step: 191183, loss:  0.003543942003\n",
      "training step: 191224, loss:  0.003776642960\n",
      "training step: 191265, loss:  0.004087368958\n",
      "training step: 191306, loss:  0.016485802829\n",
      "training step: 191347, loss:  0.004080093466\n",
      "training step: 191388, loss:  0.003638759954\n",
      "training step: 191429, loss:  0.005003615282\n",
      "training step: 191470, loss:  0.003011468332\n",
      "training step: 191511, loss:  0.025363046676\n",
      "training step: 191552, loss:  0.003120336682\n",
      "training step: 191593, loss:  0.005485921632\n",
      "training step: 191634, loss:  0.003866109066\n",
      "training step: 191675, loss:  0.003808671376\n",
      "training step: 191716, loss:  0.005715856329\n",
      "training step: 191757, loss:  0.002975780051\n",
      "training step: 191798, loss:  0.003002761398\n",
      "training step: 191839, loss:  0.005003615282\n",
      "training step: 191880, loss:  0.003002761398\n",
      "training step: 191921, loss:  0.005003615282\n",
      "training step: 191962, loss:  0.007341358811\n",
      "training step: 192003, loss:  0.009473215789\n",
      "training step: 192044, loss:  0.003002761398\n",
      "training step: 192085, loss:  0.009473215789\n",
      "training step: 192126, loss:  0.023757282645\n",
      "training step: 192167, loss:  0.007877553813\n",
      "training step: 192208, loss:  0.003880497534\n",
      "training step: 192249, loss:  0.007752379868\n",
      "training step: 192290, loss:  0.003880497534\n",
      "training step: 192331, loss:  0.003776642960\n",
      "training step: 192372, loss:  0.003011468332\n",
      "training step: 192413, loss:  0.005003615282\n",
      "training step: 192454, loss:  0.005715856329\n",
      "training step: 192495, loss:  0.003880497534\n",
      "training step: 192536, loss:  0.018698843196\n",
      "training step: 192577, loss:  0.009473215789\n",
      "training step: 192618, loss:  0.003900909098\n",
      "training step: 192659, loss:  0.003776642960\n",
      "training step: 192700, loss:  0.003866109066\n",
      "training step: 192741, loss:  0.003120336682\n",
      "training step: 192782, loss:  0.003638759954\n",
      "training step: 192823, loss:  0.007752379868\n",
      "training step: 192864, loss:  0.003011468332\n",
      "training step: 192905, loss:  0.005003615282\n",
      "training step: 192946, loss:  0.022392421961\n",
      "training step: 192987, loss:  0.003120336682\n",
      "training step: 193028, loss:  0.020691098645\n",
      "training step: 193069, loss:  0.003880497534\n",
      "training step: 193110, loss:  0.014145905152\n",
      "training step: 193151, loss:  0.023757282645\n",
      "training step: 193192, loss:  0.007877553813\n",
      "training step: 193233, loss:  0.002396283671\n",
      "training step: 193274, loss:  0.003284437582\n",
      "training step: 193315, loss:  0.003866109066\n",
      "training step: 193356, loss:  0.023757282645\n",
      "training step: 193397, loss:  0.003467082279\n",
      "training step: 193438, loss:  0.016485802829\n",
      "training step: 193479, loss:  0.003011468332\n",
      "training step: 193520, loss:  0.023757282645\n",
      "training step: 193561, loss:  0.011776376516\n",
      "training step: 193602, loss:  0.002930165734\n",
      "training step: 193643, loss:  0.025568028912\n",
      "training step: 193684, loss:  0.025568028912\n",
      "training step: 193725, loss:  0.003900909098\n",
      "training step: 193766, loss:  0.003467082279\n",
      "training step: 193807, loss:  0.018698843196\n",
      "training step: 193848, loss:  0.003120336682\n",
      "training step: 193889, loss:  0.020691098645\n",
      "training step: 193930, loss:  0.014145905152\n",
      "training step: 193971, loss:  0.003002761398\n",
      "training step: 194012, loss:  0.011776376516\n",
      "training step: 194053, loss:  0.003473303979\n",
      "training step: 194094, loss:  0.022392421961\n",
      "training step: 194135, loss:  0.003135976614\n",
      "training step: 194176, loss:  0.023757282645\n",
      "training step: 194217, loss:  0.014145905152\n",
      "training step: 194258, loss:  0.003808671376\n",
      "training step: 194299, loss:  0.004087368958\n",
      "training step: 194340, loss:  0.016485802829\n",
      "training step: 194381, loss:  0.005003615282\n",
      "training step: 194422, loss:  0.007752379868\n",
      "training step: 194463, loss:  0.003866109066\n",
      "training step: 194504, loss:  0.003986798227\n",
      "training step: 194545, loss:  0.005715856329\n",
      "training step: 194586, loss:  0.022392421961\n",
      "training step: 194627, loss:  0.003986798227\n",
      "training step: 194668, loss:  0.005485921632\n",
      "training step: 194709, loss:  0.005715856329\n",
      "training step: 194750, loss:  0.002386306180\n",
      "training step: 194791, loss:  0.022392421961\n",
      "training step: 194832, loss:  0.003473303979\n",
      "training step: 194873, loss:  0.025363046676\n",
      "training step: 194914, loss:  0.025568028912\n",
      "training step: 194955, loss:  0.003543942003\n",
      "training step: 194996, loss:  0.003135976614\n",
      "training step: 195037, loss:  0.003120336682\n",
      "training step: 195078, loss:  0.003011468332\n",
      "training step: 195119, loss:  0.006285338197\n",
      "training step: 195160, loss:  0.002930165734\n",
      "training step: 195201, loss:  0.007877553813\n",
      "training step: 195242, loss:  0.005485921632\n",
      "training step: 195283, loss:  0.005485921632\n",
      "training step: 195324, loss:  0.003120336682\n",
      "training step: 195365, loss:  0.011776376516\n",
      "training step: 195406, loss:  0.009473215789\n",
      "training step: 195447, loss:  0.003866109066\n",
      "training step: 195488, loss:  0.002396283671\n",
      "training step: 195529, loss:  0.005715856329\n",
      "training step: 195570, loss:  0.007877553813\n",
      "training step: 195611, loss:  0.005003615282\n",
      "training step: 195652, loss:  0.003638759954\n",
      "training step: 195693, loss:  0.018698843196\n",
      "training step: 195734, loss:  0.018698843196\n",
      "training step: 195775, loss:  0.025568028912\n",
      "training step: 195816, loss:  0.003986798227\n",
      "training step: 195857, loss:  0.002975780051\n",
      "training step: 195898, loss:  0.005485921632\n",
      "training step: 195939, loss:  0.016485802829\n",
      "training step: 195980, loss:  0.003638759954\n",
      "training step: 196021, loss:  0.005715856329\n",
      "training step: 196062, loss:  0.003002761398\n",
      "training step: 196103, loss:  0.002386306180\n",
      "training step: 196144, loss:  0.003986798227\n",
      "training step: 196185, loss:  0.009473215789\n",
      "training step: 196226, loss:  0.009004401974\n",
      "training step: 196267, loss:  0.002386306180\n",
      "training step: 196308, loss:  0.003692979924\n",
      "training step: 196349, loss:  0.007341358811\n",
      "training step: 196390, loss:  0.002386306180\n",
      "training step: 196431, loss:  0.003866109066\n",
      "training step: 196472, loss:  0.004087368958\n",
      "training step: 196513, loss:  0.003900909098\n",
      "training step: 196554, loss:  0.009004401974\n",
      "training step: 196595, loss:  0.018698843196\n",
      "training step: 196636, loss:  0.002386306180\n",
      "training step: 196677, loss:  0.003011468332\n",
      "training step: 196718, loss:  0.003986798227\n",
      "training step: 196759, loss:  0.024754593149\n",
      "training step: 196800, loss:  0.016485802829\n",
      "training step: 196841, loss:  0.003638759954\n",
      "training step: 196882, loss:  0.016485802829\n",
      "training step: 196923, loss:  0.018698843196\n",
      "training step: 196964, loss:  0.025363046676\n",
      "training step: 197005, loss:  0.003986798227\n",
      "training step: 197046, loss:  0.003467082279\n",
      "training step: 197087, loss:  0.003120336682\n",
      "training step: 197128, loss:  0.020691098645\n",
      "training step: 197169, loss:  0.006285338197\n",
      "training step: 197210, loss:  0.011776376516\n",
      "training step: 197251, loss:  0.024754593149\n",
      "training step: 197292, loss:  0.003866109066\n",
      "training step: 197333, loss:  0.018698843196\n",
      "training step: 197374, loss:  0.003808671376\n",
      "training step: 197415, loss:  0.007752379868\n",
      "training step: 197456, loss:  0.003692979924\n",
      "training step: 197497, loss:  0.003543942003\n",
      "training step: 197538, loss:  0.002930165734\n",
      "training step: 197579, loss:  0.003866109066\n",
      "training step: 197620, loss:  0.025363046676\n",
      "training step: 197661, loss:  0.003880497534\n",
      "training step: 197702, loss:  0.016485802829\n",
      "training step: 197743, loss:  0.025363046676\n",
      "training step: 197784, loss:  0.003002761398\n",
      "training step: 197825, loss:  0.003002761398\n",
      "training step: 197866, loss:  0.024754593149\n",
      "training step: 197907, loss:  0.022392421961\n",
      "training step: 197948, loss:  0.003900909098\n",
      "training step: 197989, loss:  0.005715856329\n",
      "training step: 198030, loss:  0.023757282645\n",
      "training step: 198071, loss:  0.003473303979\n",
      "training step: 198112, loss:  0.007341358811\n",
      "training step: 198153, loss:  0.005715856329\n",
      "training step: 198194, loss:  0.006285338197\n",
      "training step: 198235, loss:  0.003284437582\n",
      "training step: 198276, loss:  0.007341358811\n",
      "training step: 198317, loss:  0.024754593149\n",
      "training step: 198358, loss:  0.005715856329\n",
      "training step: 198399, loss:  0.025363046676\n",
      "training step: 198440, loss:  0.003135976614\n",
      "training step: 198481, loss:  0.003120336682\n",
      "training step: 198522, loss:  0.018698843196\n",
      "training step: 198563, loss:  0.016485802829\n",
      "training step: 198604, loss:  0.007877553813\n",
      "training step: 198645, loss:  0.003692979924\n",
      "training step: 198686, loss:  0.009004401974\n",
      "training step: 198727, loss:  0.003120336682\n",
      "training step: 198768, loss:  0.005485921632\n",
      "training step: 198809, loss:  0.014145905152\n",
      "training step: 198850, loss:  0.006285338197\n",
      "training step: 198891, loss:  0.022392421961\n",
      "training step: 198932, loss:  0.003002761398\n",
      "training step: 198973, loss:  0.004080093466\n",
      "training step: 199014, loss:  0.002975780051\n",
      "training step: 199055, loss:  0.009004401974\n",
      "training step: 199096, loss:  0.003986798227\n",
      "training step: 199137, loss:  0.003986798227\n",
      "training step: 199178, loss:  0.002396283671\n",
      "training step: 199219, loss:  0.003692979924\n",
      "training step: 199260, loss:  0.004080093466\n",
      "training step: 199301, loss:  0.009004401974\n",
      "training step: 199342, loss:  0.018698843196\n",
      "training step: 199383, loss:  0.003808671376\n",
      "training step: 199424, loss:  0.004080093466\n",
      "training step: 199465, loss:  0.003473303979\n",
      "training step: 199506, loss:  0.025568028912\n",
      "training step: 199547, loss:  0.009473215789\n",
      "training step: 199588, loss:  0.003986798227\n",
      "training step: 199629, loss:  0.003986798227\n",
      "training step: 199670, loss:  0.003120336682\n",
      "training step: 199711, loss:  0.003135976614\n",
      "training step: 199752, loss:  0.025363046676\n",
      "training step: 199793, loss:  0.023757282645\n",
      "training step: 199834, loss:  0.004087368958\n",
      "training step: 199875, loss:  0.018698843196\n",
      "training step: 199916, loss:  0.002975780051\n",
      "training step: 199957, loss:  0.003638759954\n",
      "training step: 199998, loss:  0.002975780051\n",
      "training step: 200039, loss:  0.009473215789\n",
      "training step: 200080, loss:  0.005003615282\n",
      "training step: 200121, loss:  0.003808671376\n",
      "training step: 200162, loss:  0.003120336682\n",
      "training step: 200203, loss:  0.009004401974\n",
      "training step: 200244, loss:  0.005715856329\n",
      "training step: 200285, loss:  0.024754593149\n",
      "training step: 200326, loss:  0.002396283671\n",
      "training step: 200367, loss:  0.023757282645\n",
      "training step: 200408, loss:  0.004087368958\n",
      "training step: 200449, loss:  0.014145905152\n",
      "training step: 200490, loss:  0.003284437582\n",
      "training step: 200531, loss:  0.009473215789\n",
      "training step: 200572, loss:  0.003135976614\n",
      "training step: 200613, loss:  0.003135976614\n",
      "training step: 200654, loss:  0.007877553813\n",
      "training step: 200695, loss:  0.020691098645\n",
      "training step: 200736, loss:  0.003543942003\n",
      "training step: 200777, loss:  0.011776376516\n",
      "training step: 200818, loss:  0.003473303979\n",
      "training step: 200859, loss:  0.002930165734\n",
      "training step: 200900, loss:  0.003467082279\n",
      "training step: 200941, loss:  0.022392421961\n",
      "training step: 200982, loss:  0.005003615282\n",
      "training step: 201023, loss:  0.003808671376\n",
      "training step: 201064, loss:  0.009473215789\n",
      "training step: 201105, loss:  0.022392421961\n",
      "training step: 201146, loss:  0.004080093466\n",
      "training step: 201187, loss:  0.025568028912\n",
      "training step: 201228, loss:  0.016485802829\n",
      "training step: 201269, loss:  0.002975780051\n",
      "training step: 201310, loss:  0.002386306180\n",
      "training step: 201351, loss:  0.004087368958\n",
      "training step: 201392, loss:  0.003692979924\n",
      "training step: 201433, loss:  0.009004401974\n",
      "training step: 201474, loss:  0.003135976614\n",
      "training step: 201515, loss:  0.003473303979\n",
      "training step: 201556, loss:  0.003638759954\n",
      "training step: 201597, loss:  0.003900909098\n",
      "training step: 201638, loss:  0.002975780051\n",
      "training step: 201679, loss:  0.003986798227\n",
      "training step: 201720, loss:  0.005485921632\n",
      "training step: 201761, loss:  0.014145905152\n",
      "training step: 201802, loss:  0.005485921632\n",
      "training step: 201843, loss:  0.003638759954\n",
      "training step: 201884, loss:  0.018698843196\n",
      "training step: 201925, loss:  0.003692979924\n",
      "training step: 201966, loss:  0.007752379868\n",
      "training step: 202007, loss:  0.018698843196\n",
      "training step: 202048, loss:  0.003880497534\n",
      "training step: 202089, loss:  0.003900909098\n",
      "training step: 202130, loss:  0.018698843196\n",
      "training step: 202171, loss:  0.003692979924\n",
      "training step: 202212, loss:  0.018698843196\n",
      "training step: 202253, loss:  0.005003615282\n",
      "training step: 202294, loss:  0.005715856329\n",
      "training step: 202335, loss:  0.022392421961\n",
      "training step: 202376, loss:  0.007877553813\n",
      "training step: 202417, loss:  0.025363046676\n",
      "training step: 202458, loss:  0.005485921632\n",
      "training step: 202499, loss:  0.004087368958\n",
      "training step: 202540, loss:  0.009473215789\n",
      "training step: 202581, loss:  0.005715856329\n",
      "training step: 202622, loss:  0.014145905152\n",
      "training step: 202663, loss:  0.011776376516\n",
      "training step: 202704, loss:  0.009004401974\n",
      "training step: 202745, loss:  0.002386306180\n",
      "training step: 202786, loss:  0.006285338197\n",
      "training step: 202827, loss:  0.022392421961\n",
      "training step: 202868, loss:  0.011776376516\n",
      "training step: 202909, loss:  0.002386306180\n",
      "training step: 202950, loss:  0.025363046676\n",
      "training step: 202991, loss:  0.014145905152\n",
      "training step: 203032, loss:  0.009473215789\n",
      "training step: 203073, loss:  0.003866109066\n",
      "training step: 203114, loss:  0.003002761398\n",
      "training step: 203155, loss:  0.009473215789\n",
      "training step: 203196, loss:  0.003808671376\n",
      "training step: 203237, loss:  0.016485802829\n",
      "training step: 203278, loss:  0.009004401974\n",
      "training step: 203319, loss:  0.023757282645\n",
      "training step: 203360, loss:  0.004080093466\n",
      "training step: 203401, loss:  0.009004401974\n",
      "training step: 203442, loss:  0.023757282645\n",
      "training step: 203483, loss:  0.025568028912\n",
      "training step: 203524, loss:  0.003284437582\n",
      "training step: 203565, loss:  0.002396283671\n",
      "training step: 203606, loss:  0.006285338197\n",
      "training step: 203647, loss:  0.011776376516\n",
      "training step: 203688, loss:  0.023757282645\n",
      "training step: 203729, loss:  0.003808671376\n",
      "training step: 203770, loss:  0.009473215789\n",
      "training step: 203811, loss:  0.016485802829\n",
      "training step: 203852, loss:  0.025568028912\n",
      "training step: 203893, loss:  0.003808671376\n",
      "training step: 203934, loss:  0.003135976614\n",
      "training step: 203975, loss:  0.005003615282\n",
      "training step: 204016, loss:  0.003900909098\n",
      "training step: 204057, loss:  0.003543942003\n",
      "training step: 204098, loss:  0.003002761398\n",
      "training step: 204139, loss:  0.016485802829\n",
      "training step: 204180, loss:  0.003284437582\n",
      "training step: 204221, loss:  0.003986798227\n",
      "training step: 204262, loss:  0.007752379868\n",
      "training step: 204303, loss:  0.025568028912\n",
      "training step: 204344, loss:  0.002930165734\n",
      "training step: 204385, loss:  0.003011468332\n",
      "training step: 204426, loss:  0.025363046676\n",
      "training step: 204467, loss:  0.007877553813\n",
      "training step: 204508, loss:  0.009473215789\n",
      "training step: 204549, loss:  0.023757282645\n",
      "training step: 204590, loss:  0.003011468332\n",
      "training step: 204631, loss:  0.004080093466\n",
      "training step: 204672, loss:  0.002396283671\n",
      "training step: 204713, loss:  0.006285338197\n",
      "training step: 204754, loss:  0.023757282645\n",
      "training step: 204795, loss:  0.003543942003\n",
      "training step: 204836, loss:  0.003120336682\n",
      "training step: 204877, loss:  0.003880497534\n",
      "training step: 204918, loss:  0.023757282645\n",
      "training step: 204959, loss:  0.025568028912\n",
      "training step: 205000, loss:  0.006285338197\n",
      "training step: 205041, loss:  0.003880497534\n",
      "training step: 205082, loss:  0.011776376516\n",
      "training step: 205123, loss:  0.018698843196\n",
      "training step: 205164, loss:  0.009473215789\n",
      "training step: 205205, loss:  0.009473215789\n",
      "training step: 205246, loss:  0.003808671376\n",
      "training step: 205287, loss:  0.003467082279\n",
      "training step: 205328, loss:  0.003135976614\n",
      "training step: 205369, loss:  0.024754593149\n",
      "training step: 205410, loss:  0.003543942003\n",
      "training step: 205451, loss:  0.009004401974\n",
      "training step: 205492, loss:  0.003692979924\n",
      "training step: 205533, loss:  0.003284437582\n",
      "training step: 205574, loss:  0.014145905152\n",
      "training step: 205615, loss:  0.007341358811\n",
      "training step: 205656, loss:  0.004087368958\n",
      "training step: 205697, loss:  0.003284437582\n",
      "training step: 205738, loss:  0.011776376516\n",
      "training step: 205779, loss:  0.005003615282\n",
      "training step: 205820, loss:  0.003473303979\n",
      "training step: 205861, loss:  0.023757282645\n",
      "training step: 205902, loss:  0.003866109066\n",
      "training step: 205943, loss:  0.003880497534\n",
      "training step: 205984, loss:  0.003900909098\n",
      "training step: 206025, loss:  0.004080093466\n",
      "training step: 206066, loss:  0.003284437582\n",
      "training step: 206107, loss:  0.003866109066\n",
      "training step: 206148, loss:  0.003880497534\n",
      "training step: 206189, loss:  0.003638759954\n",
      "training step: 206230, loss:  0.007877553813\n",
      "training step: 206271, loss:  0.003002761398\n",
      "training step: 206312, loss:  0.016485802829\n",
      "training step: 206353, loss:  0.007752379868\n",
      "training step: 206394, loss:  0.003467082279\n",
      "training step: 206435, loss:  0.007877553813\n",
      "training step: 206476, loss:  0.007341358811\n",
      "training step: 206517, loss:  0.020691098645\n",
      "training step: 206558, loss:  0.023757282645\n",
      "training step: 206599, loss:  0.002396283671\n",
      "training step: 206640, loss:  0.007752379868\n",
      "training step: 206681, loss:  0.020691098645\n",
      "training step: 206722, loss:  0.003543942003\n",
      "training step: 206763, loss:  0.005003615282\n",
      "training step: 206804, loss:  0.025363046676\n",
      "training step: 206845, loss:  0.024754593149\n",
      "training step: 206886, loss:  0.023757282645\n",
      "training step: 206927, loss:  0.025568028912\n",
      "training step: 206968, loss:  0.003986798227\n",
      "training step: 207009, loss:  0.024754593149\n",
      "training step: 207050, loss:  0.003120336682\n",
      "training step: 207091, loss:  0.002386306180\n",
      "training step: 207132, loss:  0.004087368958\n",
      "training step: 207173, loss:  0.003880497534\n",
      "training step: 207214, loss:  0.002386306180\n",
      "training step: 207255, loss:  0.016485802829\n",
      "training step: 207296, loss:  0.002975780051\n",
      "training step: 207337, loss:  0.007341358811\n",
      "training step: 207378, loss:  0.018698843196\n",
      "training step: 207419, loss:  0.009473215789\n",
      "training step: 207460, loss:  0.003002761398\n",
      "training step: 207501, loss:  0.007877553813\n",
      "training step: 207542, loss:  0.009004401974\n",
      "training step: 207583, loss:  0.002975780051\n",
      "training step: 207624, loss:  0.003776642960\n",
      "training step: 207665, loss:  0.004080093466\n",
      "training step: 207706, loss:  0.003120336682\n",
      "training step: 207747, loss:  0.003120336682\n",
      "training step: 207788, loss:  0.002396283671\n",
      "training step: 207829, loss:  0.014145905152\n",
      "training step: 207870, loss:  0.002396283671\n",
      "training step: 207911, loss:  0.024754593149\n",
      "training step: 207952, loss:  0.003900909098\n",
      "training step: 207993, loss:  0.002386306180\n",
      "training step: 208034, loss:  0.003638759954\n",
      "training step: 208075, loss:  0.003638759954\n",
      "training step: 208116, loss:  0.003135976614\n",
      "training step: 208157, loss:  0.025363046676\n",
      "training step: 208198, loss:  0.025568028912\n",
      "training step: 208239, loss:  0.016485802829\n",
      "training step: 208280, loss:  0.022392421961\n",
      "training step: 208321, loss:  0.003120336682\n",
      "training step: 208362, loss:  0.009004401974\n",
      "training step: 208403, loss:  0.003900909098\n",
      "training step: 208444, loss:  0.006285338197\n",
      "training step: 208485, loss:  0.003986798227\n",
      "training step: 208526, loss:  0.025363046676\n",
      "training step: 208567, loss:  0.003284437582\n",
      "training step: 208608, loss:  0.003880497534\n",
      "training step: 208649, loss:  0.003135976614\n",
      "training step: 208690, loss:  0.006285338197\n",
      "training step: 208731, loss:  0.011776376516\n",
      "training step: 208772, loss:  0.004080093466\n",
      "training step: 208813, loss:  0.007752379868\n",
      "training step: 208854, loss:  0.003473303979\n",
      "training step: 208895, loss:  0.018698843196\n",
      "training step: 208936, loss:  0.009004401974\n",
      "training step: 208977, loss:  0.002396283671\n",
      "training step: 209018, loss:  0.002975780051\n",
      "training step: 209059, loss:  0.003808671376\n",
      "training step: 209100, loss:  0.005485921632\n",
      "training step: 209141, loss:  0.004087368958\n",
      "training step: 209182, loss:  0.002975780051\n",
      "training step: 209223, loss:  0.020691098645\n",
      "training step: 209264, loss:  0.020691098645\n",
      "training step: 209305, loss:  0.016485802829\n",
      "training step: 209346, loss:  0.005485921632\n",
      "training step: 209387, loss:  0.003638759954\n",
      "training step: 209428, loss:  0.005715856329\n",
      "training step: 209469, loss:  0.003866109066\n",
      "training step: 209510, loss:  0.002930165734\n",
      "training step: 209551, loss:  0.007877553813\n",
      "training step: 209592, loss:  0.009473215789\n",
      "training step: 209633, loss:  0.016485802829\n",
      "training step: 209674, loss:  0.023757282645\n",
      "training step: 209715, loss:  0.025568028912\n",
      "training step: 209756, loss:  0.005485921632\n",
      "training step: 209797, loss:  0.002386306180\n",
      "training step: 209838, loss:  0.002396283671\n",
      "training step: 209879, loss:  0.002930165734\n",
      "training step: 209920, loss:  0.003900909098\n",
      "training step: 209961, loss:  0.003692979924\n",
      "training step: 210002, loss:  0.003002761398\n",
      "training step: 210043, loss:  0.003638759954\n",
      "training step: 210084, loss:  0.003776642960\n",
      "training step: 210125, loss:  0.025568028912\n",
      "training step: 210166, loss:  0.004087368958\n",
      "training step: 210207, loss:  0.009473215789\n",
      "training step: 210248, loss:  0.004087368958\n",
      "training step: 210289, loss:  0.002975780051\n",
      "training step: 210330, loss:  0.014145905152\n",
      "training step: 210371, loss:  0.003692979924\n",
      "training step: 210412, loss:  0.003467082279\n",
      "training step: 210453, loss:  0.003002761398\n",
      "training step: 210494, loss:  0.003776642960\n",
      "training step: 210535, loss:  0.004087368958\n",
      "training step: 210576, loss:  0.003284437582\n",
      "training step: 210617, loss:  0.003692979924\n",
      "training step: 210658, loss:  0.024754593149\n",
      "training step: 210699, loss:  0.004087368958\n",
      "training step: 210740, loss:  0.003880497534\n",
      "training step: 210781, loss:  0.004080093466\n",
      "training step: 210822, loss:  0.007341358811\n",
      "training step: 210863, loss:  0.025363046676\n",
      "training step: 210904, loss:  0.003986798227\n",
      "training step: 210945, loss:  0.003808671376\n",
      "training step: 210986, loss:  0.003900909098\n",
      "training step: 211027, loss:  0.014145905152\n",
      "training step: 211068, loss:  0.003467082279\n",
      "training step: 211109, loss:  0.025363046676\n",
      "training step: 211150, loss:  0.009473215789\n",
      "training step: 211191, loss:  0.003284437582\n",
      "training step: 211232, loss:  0.009004401974\n",
      "training step: 211273, loss:  0.004087368958\n",
      "training step: 211314, loss:  0.018698843196\n",
      "training step: 211355, loss:  0.009004401974\n",
      "training step: 211396, loss:  0.025568028912\n",
      "training step: 211437, loss:  0.003002761398\n",
      "training step: 211478, loss:  0.005003615282\n",
      "training step: 211519, loss:  0.003473303979\n",
      "training step: 211560, loss:  0.003011468332\n",
      "training step: 211601, loss:  0.007341358811\n",
      "training step: 211642, loss:  0.020691098645\n",
      "training step: 211683, loss:  0.002975780051\n",
      "training step: 211724, loss:  0.009473215789\n",
      "training step: 211765, loss:  0.003011468332\n",
      "training step: 211806, loss:  0.014145905152\n",
      "training step: 211847, loss:  0.003120336682\n",
      "training step: 211888, loss:  0.018698843196\n",
      "training step: 211929, loss:  0.025568028912\n",
      "training step: 211970, loss:  0.007752379868\n",
      "training step: 212011, loss:  0.005715856329\n",
      "training step: 212052, loss:  0.003011468332\n",
      "training step: 212093, loss:  0.005485921632\n",
      "training step: 212134, loss:  0.002930165734\n",
      "training step: 212175, loss:  0.002396283671\n",
      "training step: 212216, loss:  0.003880497534\n",
      "training step: 212257, loss:  0.009004401974\n",
      "training step: 212298, loss:  0.023757282645\n",
      "training step: 212339, loss:  0.003900909098\n",
      "training step: 212380, loss:  0.023757282645\n",
      "training step: 212421, loss:  0.005715856329\n",
      "training step: 212462, loss:  0.023757282645\n",
      "training step: 212503, loss:  0.003880497534\n",
      "training step: 212544, loss:  0.003284437582\n",
      "training step: 212585, loss:  0.002975780051\n",
      "training step: 212626, loss:  0.024754593149\n",
      "training step: 212667, loss:  0.003638759954\n",
      "training step: 212708, loss:  0.006285338197\n",
      "training step: 212749, loss:  0.025363046676\n",
      "training step: 212790, loss:  0.003986798227\n",
      "training step: 212831, loss:  0.003002761398\n",
      "training step: 212872, loss:  0.020691098645\n",
      "training step: 212913, loss:  0.003467082279\n",
      "training step: 212954, loss:  0.005003615282\n",
      "training step: 212995, loss:  0.004087368958\n",
      "training step: 213036, loss:  0.007341358811\n",
      "training step: 213077, loss:  0.020691098645\n",
      "training step: 213118, loss:  0.009473215789\n",
      "training step: 213159, loss:  0.003135976614\n",
      "training step: 213200, loss:  0.003986798227\n",
      "training step: 213241, loss:  0.009473215789\n",
      "training step: 213282, loss:  0.003284437582\n",
      "training step: 213323, loss:  0.004087368958\n",
      "training step: 213364, loss:  0.023757282645\n",
      "training step: 213405, loss:  0.007877553813\n",
      "training step: 213446, loss:  0.024754593149\n",
      "training step: 213487, loss:  0.025568028912\n",
      "training step: 213528, loss:  0.011776376516\n",
      "training step: 213569, loss:  0.003120336682\n",
      "training step: 213610, loss:  0.005715856329\n",
      "training step: 213651, loss:  0.003543942003\n",
      "training step: 213692, loss:  0.020691098645\n",
      "training step: 213733, loss:  0.003135976614\n",
      "training step: 213774, loss:  0.005003615282\n",
      "training step: 213815, loss:  0.003135976614\n",
      "training step: 213856, loss:  0.003467082279\n",
      "training step: 213897, loss:  0.020691098645\n",
      "training step: 213938, loss:  0.003880497534\n",
      "training step: 213979, loss:  0.007341358811\n",
      "training step: 214020, loss:  0.020691098645\n",
      "training step: 214061, loss:  0.003543942003\n",
      "training step: 214102, loss:  0.003473303979\n",
      "training step: 214143, loss:  0.023757282645\n",
      "training step: 214184, loss:  0.024754593149\n",
      "training step: 214225, loss:  0.003776642960\n",
      "training step: 214266, loss:  0.023757282645\n",
      "training step: 214307, loss:  0.003467082279\n",
      "training step: 214348, loss:  0.024754593149\n",
      "training step: 214389, loss:  0.007752379868\n",
      "training step: 214430, loss:  0.003880497534\n",
      "training step: 214471, loss:  0.003543942003\n",
      "training step: 214512, loss:  0.025363046676\n",
      "training step: 214553, loss:  0.002930165734\n",
      "training step: 214594, loss:  0.003638759954\n",
      "training step: 214635, loss:  0.004080093466\n",
      "training step: 214676, loss:  0.003002761398\n",
      "training step: 214717, loss:  0.003986798227\n",
      "training step: 214758, loss:  0.016485802829\n",
      "training step: 214799, loss:  0.011776376516\n",
      "training step: 214840, loss:  0.005485921632\n",
      "training step: 214881, loss:  0.014145905152\n",
      "training step: 214922, loss:  0.003473303979\n",
      "training step: 214963, loss:  0.009473215789\n",
      "training step: 215004, loss:  0.003473303979\n",
      "training step: 215045, loss:  0.002975780051\n",
      "training step: 215086, loss:  0.009473215789\n",
      "training step: 215127, loss:  0.009004401974\n",
      "training step: 215168, loss:  0.002396283671\n",
      "training step: 215209, loss:  0.009004401974\n",
      "training step: 215250, loss:  0.003011468332\n",
      "training step: 215291, loss:  0.003120336682\n",
      "training step: 215332, loss:  0.011776376516\n",
      "training step: 215373, loss:  0.022392421961\n",
      "training step: 215414, loss:  0.002930165734\n",
      "training step: 215455, loss:  0.002396283671\n",
      "training step: 215496, loss:  0.002386306180\n",
      "training step: 215537, loss:  0.018698843196\n",
      "training step: 215578, loss:  0.003880497534\n",
      "training step: 215619, loss:  0.009473215789\n",
      "training step: 215660, loss:  0.003467082279\n",
      "training step: 215701, loss:  0.022392421961\n",
      "training step: 215742, loss:  0.003011468332\n",
      "training step: 215783, loss:  0.003284437582\n",
      "training step: 215824, loss:  0.003467082279\n",
      "training step: 215865, loss:  0.003467082279\n",
      "training step: 215906, loss:  0.022392421961\n",
      "training step: 215947, loss:  0.003473303979\n",
      "training step: 215988, loss:  0.004087368958\n",
      "training step: 216029, loss:  0.024754593149\n",
      "training step: 216070, loss:  0.007341358811\n",
      "training step: 216111, loss:  0.005485921632\n",
      "training step: 216152, loss:  0.003986798227\n",
      "training step: 216193, loss:  0.009004401974\n",
      "training step: 216234, loss:  0.003692979924\n",
      "training step: 216275, loss:  0.003692979924\n",
      "training step: 216316, loss:  0.020691098645\n",
      "training step: 216357, loss:  0.007752379868\n",
      "training step: 216398, loss:  0.004080093466\n",
      "training step: 216439, loss:  0.007752379868\n",
      "training step: 216480, loss:  0.003135976614\n",
      "training step: 216521, loss:  0.003135976614\n",
      "training step: 216562, loss:  0.020691098645\n",
      "training step: 216603, loss:  0.002930165734\n",
      "training step: 216644, loss:  0.005715856329\n",
      "training step: 216685, loss:  0.002975780051\n",
      "training step: 216726, loss:  0.002975780051\n",
      "training step: 216767, loss:  0.003002761398\n",
      "training step: 216808, loss:  0.009004401974\n",
      "training step: 216849, loss:  0.011776376516\n",
      "training step: 216890, loss:  0.003808671376\n",
      "training step: 216931, loss:  0.003135976614\n",
      "training step: 216972, loss:  0.003467082279\n",
      "training step: 217013, loss:  0.003866109066\n",
      "training step: 217054, loss:  0.003011468332\n",
      "training step: 217095, loss:  0.016485802829\n",
      "training step: 217136, loss:  0.025363046676\n",
      "training step: 217177, loss:  0.003135976614\n",
      "training step: 217218, loss:  0.003011468332\n",
      "training step: 217259, loss:  0.004087368958\n",
      "training step: 217300, loss:  0.003120336682\n",
      "training step: 217341, loss:  0.005715856329\n",
      "training step: 217382, loss:  0.004087368958\n",
      "training step: 217423, loss:  0.018698843196\n",
      "training step: 217464, loss:  0.003808671376\n",
      "training step: 217505, loss:  0.024754593149\n",
      "training step: 217546, loss:  0.022392421961\n",
      "training step: 217587, loss:  0.003808671376\n",
      "training step: 217628, loss:  0.004080093466\n",
      "training step: 217669, loss:  0.009004401974\n",
      "training step: 217710, loss:  0.003473303979\n",
      "training step: 217751, loss:  0.025568028912\n",
      "training step: 217792, loss:  0.003467082279\n",
      "training step: 217833, loss:  0.006285338197\n",
      "training step: 217874, loss:  0.011776376516\n",
      "training step: 217915, loss:  0.003900909098\n",
      "training step: 217956, loss:  0.007877553813\n",
      "training step: 217997, loss:  0.024754593149\n",
      "training step: 218038, loss:  0.003473303979\n",
      "training step: 218079, loss:  0.006285338197\n",
      "training step: 218120, loss:  0.003900909098\n",
      "training step: 218161, loss:  0.003638759954\n",
      "training step: 218202, loss:  0.003011468332\n",
      "training step: 218243, loss:  0.003543942003\n",
      "training step: 218284, loss:  0.014145905152\n",
      "training step: 218325, loss:  0.018698843196\n",
      "training step: 218366, loss:  0.002386306180\n",
      "training step: 218407, loss:  0.002975780051\n",
      "training step: 218448, loss:  0.003543942003\n",
      "training step: 218489, loss:  0.018698843196\n",
      "training step: 218530, loss:  0.007752379868\n",
      "training step: 218571, loss:  0.003284437582\n",
      "training step: 218612, loss:  0.009004401974\n",
      "training step: 218653, loss:  0.003284437582\n",
      "training step: 218694, loss:  0.003473303979\n",
      "training step: 218735, loss:  0.025363046676\n",
      "training step: 218776, loss:  0.020691098645\n",
      "training step: 218817, loss:  0.025568028912\n",
      "training step: 218858, loss:  0.003808671376\n",
      "training step: 218899, loss:  0.002396283671\n",
      "training step: 218940, loss:  0.002386306180\n",
      "training step: 218981, loss:  0.018698843196\n",
      "training step: 219022, loss:  0.003473303979\n",
      "training step: 219063, loss:  0.003543942003\n",
      "training step: 219104, loss:  0.023757282645\n",
      "training step: 219145, loss:  0.003284437582\n",
      "training step: 219186, loss:  0.024754593149\n",
      "training step: 219227, loss:  0.003543942003\n",
      "training step: 219268, loss:  0.003284437582\n",
      "training step: 219309, loss:  0.003473303979\n",
      "training step: 219350, loss:  0.003011468332\n",
      "training step: 219391, loss:  0.003120336682\n",
      "training step: 219432, loss:  0.009004401974\n",
      "training step: 219473, loss:  0.005485921632\n",
      "training step: 219514, loss:  0.003473303979\n",
      "training step: 219555, loss:  0.004080093466\n",
      "training step: 219596, loss:  0.002396283671\n",
      "training step: 219637, loss:  0.002386306180\n",
      "training step: 219678, loss:  0.003002761398\n",
      "training step: 219719, loss:  0.004087368958\n",
      "training step: 219760, loss:  0.003011468332\n",
      "training step: 219801, loss:  0.024754593149\n",
      "training step: 219842, loss:  0.006285338197\n",
      "training step: 219883, loss:  0.003011468332\n",
      "training step: 219924, loss:  0.011776376516\n",
      "training step: 219965, loss:  0.002386306180\n",
      "training step: 220006, loss:  0.003467082279\n",
      "training step: 220047, loss:  0.003002761398\n",
      "training step: 220088, loss:  0.003692979924\n",
      "training step: 220129, loss:  0.003543942003\n",
      "training step: 220170, loss:  0.007877553813\n",
      "training step: 220211, loss:  0.006285338197\n",
      "training step: 220252, loss:  0.003776642960\n",
      "training step: 220293, loss:  0.003638759954\n",
      "training step: 220334, loss:  0.014145905152\n",
      "training step: 220375, loss:  0.003011468332\n",
      "training step: 220416, loss:  0.006285338197\n",
      "training step: 220457, loss:  0.003638759954\n",
      "training step: 220498, loss:  0.003543942003\n",
      "training step: 220539, loss:  0.005003615282\n",
      "training step: 220580, loss:  0.005003615282\n",
      "training step: 220621, loss:  0.002975780051\n",
      "training step: 220662, loss:  0.005485921632\n",
      "training step: 220703, loss:  0.003866109066\n",
      "training step: 220744, loss:  0.003808671376\n",
      "training step: 220785, loss:  0.003986798227\n",
      "training step: 220826, loss:  0.005715856329\n",
      "training step: 220867, loss:  0.006285338197\n",
      "training step: 220908, loss:  0.024754593149\n",
      "training step: 220949, loss:  0.003135976614\n",
      "training step: 220990, loss:  0.005715856329\n",
      "training step: 221031, loss:  0.002975780051\n",
      "training step: 221072, loss:  0.023757282645\n",
      "training step: 221113, loss:  0.003120336682\n",
      "training step: 221154, loss:  0.022392421961\n",
      "training step: 221195, loss:  0.003880497534\n",
      "training step: 221236, loss:  0.007877553813\n",
      "training step: 221277, loss:  0.003002761398\n",
      "training step: 221318, loss:  0.003986798227\n",
      "training step: 221359, loss:  0.016485802829\n",
      "training step: 221400, loss:  0.002930165734\n",
      "training step: 221441, loss:  0.002975780051\n",
      "training step: 221482, loss:  0.002396283671\n",
      "training step: 221523, loss:  0.003776642960\n",
      "training step: 221564, loss:  0.018698843196\n",
      "training step: 221605, loss:  0.011776376516\n",
      "training step: 221646, loss:  0.003880497534\n",
      "training step: 221687, loss:  0.020691098645\n",
      "training step: 221728, loss:  0.003543942003\n",
      "training step: 221769, loss:  0.025363046676\n",
      "training step: 221810, loss:  0.005485921632\n",
      "training step: 221851, loss:  0.002930165734\n",
      "training step: 221892, loss:  0.003866109066\n",
      "training step: 221933, loss:  0.009004401974\n",
      "training step: 221974, loss:  0.003866109066\n",
      "training step: 222015, loss:  0.005485921632\n",
      "training step: 222056, loss:  0.007752379868\n",
      "training step: 222097, loss:  0.005715856329\n",
      "training step: 222138, loss:  0.024754593149\n",
      "training step: 222179, loss:  0.025568028912\n",
      "training step: 222220, loss:  0.004080093466\n",
      "training step: 222261, loss:  0.020691098645\n",
      "training step: 222302, loss:  0.025363046676\n",
      "training step: 222343, loss:  0.025363046676\n",
      "training step: 222384, loss:  0.002386306180\n",
      "training step: 222425, loss:  0.009004401974\n",
      "training step: 222466, loss:  0.009473215789\n",
      "training step: 222507, loss:  0.003900909098\n",
      "training step: 222548, loss:  0.007877553813\n",
      "training step: 222589, loss:  0.018698843196\n",
      "training step: 222630, loss:  0.007341358811\n",
      "training step: 222671, loss:  0.002975780051\n",
      "training step: 222712, loss:  0.003986798227\n",
      "training step: 222753, loss:  0.003866109066\n",
      "training step: 222794, loss:  0.003467082279\n",
      "training step: 222835, loss:  0.003284437582\n",
      "training step: 222876, loss:  0.020691098645\n",
      "training step: 222917, loss:  0.003692979924\n",
      "training step: 222958, loss:  0.003002761398\n",
      "training step: 222999, loss:  0.003135976614\n",
      "training step: 223040, loss:  0.007752379868\n",
      "training step: 223081, loss:  0.007877553813\n",
      "training step: 223122, loss:  0.003284437582\n",
      "training step: 223163, loss:  0.022392421961\n",
      "training step: 223204, loss:  0.014145905152\n",
      "training step: 223245, loss:  0.004087368958\n",
      "training step: 223286, loss:  0.024754593149\n",
      "training step: 223327, loss:  0.003473303979\n",
      "training step: 223368, loss:  0.007877553813\n",
      "training step: 223409, loss:  0.003986798227\n",
      "training step: 223450, loss:  0.005715856329\n",
      "training step: 223491, loss:  0.003543942003\n",
      "training step: 223532, loss:  0.016485802829\n",
      "training step: 223573, loss:  0.003880497534\n",
      "training step: 223614, loss:  0.009473215789\n",
      "training step: 223655, loss:  0.009473215789\n",
      "training step: 223696, loss:  0.002975780051\n",
      "training step: 223737, loss:  0.003776642960\n",
      "training step: 223778, loss:  0.003776642960\n",
      "training step: 223819, loss:  0.003900909098\n",
      "training step: 223860, loss:  0.003880497534\n",
      "training step: 223901, loss:  0.007877553813\n",
      "training step: 223942, loss:  0.016485802829\n",
      "training step: 223983, loss:  0.003473303979\n",
      "training step: 224024, loss:  0.009004401974\n",
      "training step: 224065, loss:  0.004087368958\n",
      "training step: 224106, loss:  0.006285338197\n",
      "training step: 224147, loss:  0.003473303979\n",
      "training step: 224188, loss:  0.014145905152\n",
      "training step: 224229, loss:  0.003900909098\n",
      "training step: 224270, loss:  0.023757282645\n",
      "training step: 224311, loss:  0.007341358811\n",
      "training step: 224352, loss:  0.020691098645\n",
      "training step: 224393, loss:  0.003900909098\n",
      "training step: 224434, loss:  0.003135976614\n",
      "training step: 224475, loss:  0.003692979924\n",
      "training step: 224516, loss:  0.003467082279\n",
      "training step: 224557, loss:  0.003543942003\n",
      "training step: 224598, loss:  0.003692979924\n",
      "training step: 224639, loss:  0.006285338197\n",
      "training step: 224680, loss:  0.002386306180\n",
      "training step: 224721, loss:  0.018698843196\n",
      "training step: 224762, loss:  0.007752379868\n",
      "training step: 224803, loss:  0.004080093466\n",
      "training step: 224844, loss:  0.003467082279\n",
      "training step: 224885, loss:  0.003284437582\n",
      "training step: 224926, loss:  0.003776642960\n",
      "training step: 224967, loss:  0.016485802829\n",
      "training step: 225008, loss:  0.005715856329\n",
      "training step: 225049, loss:  0.003543942003\n",
      "training step: 225090, loss:  0.005003615282\n",
      "training step: 225131, loss:  0.005003615282\n",
      "training step: 225172, loss:  0.024754593149\n",
      "training step: 225213, loss:  0.025363046676\n",
      "training step: 225254, loss:  0.005715856329\n",
      "training step: 225295, loss:  0.011776376516\n",
      "training step: 225336, loss:  0.002975780051\n",
      "training step: 225377, loss:  0.003543942003\n",
      "training step: 225418, loss:  0.003880497534\n",
      "training step: 225459, loss:  0.023757282645\n",
      "training step: 225500, loss:  0.002975780051\n",
      "training step: 225541, loss:  0.005485921632\n",
      "training step: 225582, loss:  0.004080093466\n",
      "training step: 225623, loss:  0.009004401974\n",
      "training step: 225664, loss:  0.009004401974\n",
      "training step: 225705, loss:  0.003866109066\n",
      "training step: 225746, loss:  0.024754593149\n",
      "training step: 225787, loss:  0.003880497534\n",
      "training step: 225828, loss:  0.007341358811\n",
      "training step: 225869, loss:  0.003002761398\n",
      "training step: 225910, loss:  0.009473215789\n",
      "training step: 225951, loss:  0.003808671376\n",
      "training step: 225992, loss:  0.007341358811\n",
      "training step: 226033, loss:  0.002930165734\n",
      "training step: 226074, loss:  0.024754593149\n",
      "training step: 226115, loss:  0.002396283671\n",
      "training step: 226156, loss:  0.005003615282\n",
      "training step: 226197, loss:  0.004080093466\n",
      "training step: 226238, loss:  0.025568028912\n",
      "training step: 226279, loss:  0.007752379868\n",
      "training step: 226320, loss:  0.002975780051\n",
      "training step: 226361, loss:  0.007752379868\n",
      "training step: 226402, loss:  0.025363046676\n",
      "training step: 226443, loss:  0.003011468332\n",
      "training step: 226484, loss:  0.003135976614\n",
      "training step: 226525, loss:  0.025363046676\n",
      "training step: 226566, loss:  0.009473215789\n",
      "training step: 226607, loss:  0.023757282645\n",
      "training step: 226648, loss:  0.003284437582\n",
      "training step: 226689, loss:  0.005715856329\n",
      "training step: 226730, loss:  0.003692979924\n",
      "training step: 226771, loss:  0.003002761398\n",
      "training step: 226812, loss:  0.003135976614\n",
      "training step: 226853, loss:  0.009004401974\n",
      "training step: 226894, loss:  0.003002761398\n",
      "training step: 226935, loss:  0.009004401974\n",
      "training step: 226976, loss:  0.025568028912\n",
      "training step: 227017, loss:  0.002930165734\n",
      "training step: 227058, loss:  0.011776376516\n",
      "training step: 227099, loss:  0.003002761398\n",
      "training step: 227140, loss:  0.003866109066\n",
      "training step: 227181, loss:  0.004080093466\n",
      "training step: 227222, loss:  0.025363046676\n",
      "training step: 227263, loss:  0.003692979924\n",
      "training step: 227304, loss:  0.003776642960\n",
      "training step: 227345, loss:  0.003543942003\n",
      "training step: 227386, loss:  0.003120336682\n",
      "training step: 227427, loss:  0.022392421961\n",
      "training step: 227468, loss:  0.005485921632\n",
      "training step: 227509, loss:  0.016485802829\n",
      "training step: 227550, loss:  0.009004401974\n",
      "training step: 227591, loss:  0.014145905152\n",
      "training step: 227632, loss:  0.003002761398\n",
      "training step: 227673, loss:  0.002396283671\n",
      "training step: 227714, loss:  0.002386306180\n",
      "training step: 227755, loss:  0.003866109066\n",
      "training step: 227796, loss:  0.020691098645\n",
      "training step: 227837, loss:  0.003866109066\n",
      "training step: 227878, loss:  0.005003615282\n",
      "training step: 227919, loss:  0.014145905152\n",
      "training step: 227960, loss:  0.003284437582\n",
      "training step: 228001, loss:  0.007341358811\n",
      "training step: 228042, loss:  0.003011468332\n",
      "training step: 228083, loss:  0.009004401974\n",
      "training step: 228124, loss:  0.003467082279\n",
      "training step: 228165, loss:  0.002386306180\n",
      "training step: 228206, loss:  0.018698843196\n",
      "training step: 228247, loss:  0.009004401974\n",
      "training step: 228288, loss:  0.003638759954\n",
      "training step: 228329, loss:  0.025363046676\n",
      "training step: 228370, loss:  0.007341358811\n",
      "training step: 228411, loss:  0.003880497534\n",
      "training step: 228452, loss:  0.011776376516\n",
      "training step: 228493, loss:  0.003120336682\n",
      "training step: 228534, loss:  0.016485802829\n",
      "training step: 228575, loss:  0.005485921632\n",
      "training step: 228616, loss:  0.014145905152\n",
      "training step: 228657, loss:  0.025568028912\n",
      "training step: 228698, loss:  0.003011468332\n",
      "training step: 228739, loss:  0.003638759954\n",
      "training step: 228780, loss:  0.003135976614\n",
      "training step: 228821, loss:  0.003473303979\n",
      "training step: 228862, loss:  0.022392421961\n",
      "training step: 228903, loss:  0.009004401974\n",
      "training step: 228944, loss:  0.007877553813\n",
      "training step: 228985, loss:  0.003866109066\n",
      "training step: 229026, loss:  0.005715856329\n",
      "training step: 229067, loss:  0.016485802829\n",
      "training step: 229108, loss:  0.003900909098\n",
      "training step: 229149, loss:  0.025568028912\n",
      "training step: 229190, loss:  0.025363046676\n",
      "training step: 229231, loss:  0.025568028912\n",
      "training step: 229272, loss:  0.003135976614\n",
      "training step: 229313, loss:  0.009004401974\n",
      "training step: 229354, loss:  0.020691098645\n",
      "training step: 229395, loss:  0.023757282645\n",
      "training step: 229436, loss:  0.003900909098\n",
      "training step: 229477, loss:  0.009473215789\n",
      "training step: 229518, loss:  0.003692979924\n",
      "training step: 229559, loss:  0.004087368958\n",
      "training step: 229600, loss:  0.025568028912\n",
      "training step: 229641, loss:  0.003011468332\n",
      "training step: 229682, loss:  0.022392421961\n",
      "training step: 229723, loss:  0.025568028912\n",
      "training step: 229764, loss:  0.003284437582\n",
      "training step: 229805, loss:  0.005715856329\n",
      "training step: 229846, loss:  0.022392421961\n",
      "training step: 229887, loss:  0.003002761398\n",
      "training step: 229928, loss:  0.002396283671\n",
      "training step: 229969, loss:  0.007752379868\n",
      "training step: 230010, loss:  0.003986798227\n",
      "training step: 230051, loss:  0.007877553813\n",
      "training step: 230092, loss:  0.016485802829\n",
      "training step: 230133, loss:  0.005003615282\n",
      "training step: 230174, loss:  0.022392421961\n",
      "training step: 230215, loss:  0.022392421961\n",
      "training step: 230256, loss:  0.003776642960\n",
      "training step: 230297, loss:  0.005715856329\n",
      "training step: 230338, loss:  0.003284437582\n",
      "training step: 230379, loss:  0.003473303979\n",
      "training step: 230420, loss:  0.003011468332\n",
      "training step: 230461, loss:  0.005003615282\n",
      "training step: 230502, loss:  0.003638759954\n",
      "training step: 230543, loss:  0.006285338197\n",
      "training step: 230584, loss:  0.025568028912\n",
      "training step: 230625, loss:  0.003638759954\n",
      "training step: 230666, loss:  0.003692979924\n",
      "training step: 230707, loss:  0.025363046676\n",
      "training step: 230748, loss:  0.003692979924\n",
      "training step: 230789, loss:  0.020691098645\n",
      "training step: 230830, loss:  0.023757282645\n",
      "training step: 230871, loss:  0.007752379868\n",
      "training step: 230912, loss:  0.006285338197\n",
      "training step: 230953, loss:  0.003808671376\n",
      "training step: 230994, loss:  0.018698843196\n",
      "training step: 231035, loss:  0.002975780051\n",
      "training step: 231076, loss:  0.016485802829\n",
      "training step: 231117, loss:  0.014145905152\n",
      "training step: 231158, loss:  0.005485921632\n",
      "training step: 231199, loss:  0.003011468332\n",
      "training step: 231240, loss:  0.004087368958\n",
      "training step: 231281, loss:  0.003986798227\n",
      "training step: 231322, loss:  0.003011468332\n",
      "training step: 231363, loss:  0.003638759954\n",
      "training step: 231404, loss:  0.006285338197\n",
      "training step: 231445, loss:  0.003473303979\n",
      "training step: 231486, loss:  0.003900909098\n",
      "training step: 231527, loss:  0.003543942003\n",
      "training step: 231568, loss:  0.003120336682\n",
      "training step: 231609, loss:  0.003467082279\n",
      "training step: 231650, loss:  0.007877553813\n",
      "training step: 231691, loss:  0.005485921632\n",
      "training step: 231732, loss:  0.003900909098\n",
      "training step: 231773, loss:  0.009473215789\n",
      "training step: 231814, loss:  0.003002761398\n",
      "training step: 231855, loss:  0.003467082279\n",
      "training step: 231896, loss:  0.004087368958\n",
      "training step: 231937, loss:  0.003135976614\n",
      "training step: 231978, loss:  0.003986798227\n",
      "training step: 232019, loss:  0.003880497534\n",
      "training step: 232060, loss:  0.022392421961\n",
      "training step: 232101, loss:  0.003473303979\n",
      "training step: 232142, loss:  0.003808671376\n",
      "training step: 232183, loss:  0.005003615282\n",
      "training step: 232224, loss:  0.005715856329\n",
      "training step: 232265, loss:  0.014145905152\n",
      "training step: 232306, loss:  0.003002761398\n",
      "training step: 232347, loss:  0.003120336682\n",
      "training step: 232388, loss:  0.005003615282\n",
      "training step: 232429, loss:  0.024754593149\n",
      "training step: 232470, loss:  0.003692979924\n",
      "training step: 232511, loss:  0.005715856329\n",
      "training step: 232552, loss:  0.002396283671\n",
      "training step: 232593, loss:  0.016485802829\n",
      "training step: 232634, loss:  0.003467082279\n",
      "training step: 232675, loss:  0.002975780051\n",
      "training step: 232716, loss:  0.009473215789\n",
      "training step: 232757, loss:  0.018698843196\n",
      "training step: 232798, loss:  0.020691098645\n",
      "training step: 232839, loss:  0.003638759954\n",
      "training step: 232880, loss:  0.025568028912\n",
      "training step: 232921, loss:  0.014145905152\n",
      "training step: 232962, loss:  0.004087368958\n",
      "training step: 233003, loss:  0.014145905152\n",
      "training step: 233044, loss:  0.006285338197\n",
      "training step: 233085, loss:  0.018698843196\n",
      "training step: 233126, loss:  0.003284437582\n",
      "training step: 233167, loss:  0.002930165734\n",
      "training step: 233208, loss:  0.025363046676\n",
      "training step: 233249, loss:  0.020691098645\n",
      "training step: 233290, loss:  0.004087368958\n",
      "training step: 233331, loss:  0.002930165734\n",
      "training step: 233372, loss:  0.002396283671\n",
      "training step: 233413, loss:  0.003638759954\n",
      "training step: 233454, loss:  0.004080093466\n",
      "training step: 233495, loss:  0.003135976614\n",
      "training step: 233536, loss:  0.003986798227\n",
      "training step: 233577, loss:  0.007877553813\n",
      "training step: 233618, loss:  0.003543942003\n",
      "training step: 233659, loss:  0.003473303979\n",
      "training step: 233700, loss:  0.023757282645\n",
      "training step: 233741, loss:  0.003900909098\n",
      "training step: 233782, loss:  0.003638759954\n",
      "training step: 233823, loss:  0.003808671376\n",
      "training step: 233864, loss:  0.024754593149\n",
      "training step: 233905, loss:  0.003120336682\n",
      "training step: 233946, loss:  0.003986798227\n",
      "training step: 233987, loss:  0.005715856329\n",
      "training step: 234028, loss:  0.024754593149\n",
      "training step: 234069, loss:  0.003473303979\n",
      "training step: 234110, loss:  0.007341358811\n",
      "training step: 234151, loss:  0.009473215789\n",
      "training step: 234192, loss:  0.009473215789\n",
      "training step: 234233, loss:  0.005715856329\n",
      "training step: 234274, loss:  0.003473303979\n",
      "training step: 234315, loss:  0.018698843196\n",
      "training step: 234356, loss:  0.023757282645\n",
      "training step: 234397, loss:  0.002930165734\n",
      "training step: 234438, loss:  0.011776376516\n",
      "training step: 234479, loss:  0.005715856329\n",
      "training step: 234520, loss:  0.002396283671\n",
      "training step: 234561, loss:  0.002930165734\n",
      "training step: 234602, loss:  0.003467082279\n",
      "training step: 234643, loss:  0.003543942003\n",
      "training step: 234684, loss:  0.002386306180\n",
      "training step: 234725, loss:  0.014145905152\n",
      "training step: 234766, loss:  0.003986798227\n",
      "training step: 234807, loss:  0.002975780051\n",
      "training step: 234848, loss:  0.003543942003\n",
      "training step: 234889, loss:  0.003900909098\n",
      "training step: 234930, loss:  0.003473303979\n",
      "training step: 234971, loss:  0.007752379868\n",
      "training step: 235012, loss:  0.004087368958\n",
      "training step: 235053, loss:  0.002930165734\n",
      "training step: 235094, loss:  0.003543942003\n",
      "training step: 235135, loss:  0.003900909098\n",
      "training step: 235176, loss:  0.003638759954\n",
      "training step: 235217, loss:  0.003638759954\n",
      "training step: 235258, loss:  0.016485802829\n",
      "training step: 235299, loss:  0.003120336682\n",
      "training step: 235340, loss:  0.005715856329\n",
      "training step: 235381, loss:  0.009473215789\n",
      "training step: 235422, loss:  0.002930165734\n",
      "training step: 235463, loss:  0.003120336682\n",
      "training step: 235504, loss:  0.007752379868\n",
      "training step: 235545, loss:  0.005715856329\n",
      "training step: 235586, loss:  0.002396283671\n",
      "training step: 235627, loss:  0.003986798227\n",
      "training step: 235668, loss:  0.025568028912\n",
      "training step: 235709, loss:  0.003543942003\n",
      "training step: 235750, loss:  0.003284437582\n",
      "training step: 235791, loss:  0.003120336682\n",
      "training step: 235832, loss:  0.003866109066\n",
      "training step: 235873, loss:  0.018698843196\n",
      "training step: 235914, loss:  0.002396283671\n",
      "training step: 235955, loss:  0.003880497534\n",
      "training step: 235996, loss:  0.003776642960\n",
      "training step: 236037, loss:  0.005715856329\n",
      "training step: 236078, loss:  0.024754593149\n",
      "training step: 236119, loss:  0.003011468332\n",
      "training step: 236160, loss:  0.016485802829\n",
      "training step: 236201, loss:  0.002396283671\n",
      "training step: 236242, loss:  0.003986798227\n",
      "training step: 236283, loss:  0.003900909098\n",
      "training step: 236324, loss:  0.003866109066\n",
      "training step: 236365, loss:  0.003776642960\n",
      "training step: 236406, loss:  0.004080093466\n",
      "training step: 236447, loss:  0.005485921632\n",
      "training step: 236488, loss:  0.007752379868\n",
      "training step: 236529, loss:  0.025363046676\n",
      "training step: 236570, loss:  0.003866109066\n",
      "training step: 236611, loss:  0.003776642960\n",
      "training step: 236652, loss:  0.003002761398\n",
      "training step: 236693, loss:  0.003135976614\n",
      "training step: 236734, loss:  0.003776642960\n",
      "training step: 236775, loss:  0.003880497534\n",
      "training step: 236816, loss:  0.020691098645\n",
      "training step: 236857, loss:  0.002930165734\n",
      "training step: 236898, loss:  0.002930165734\n",
      "training step: 236939, loss:  0.002930165734\n",
      "training step: 236980, loss:  0.005485921632\n",
      "training step: 237021, loss:  0.004080093466\n",
      "training step: 237062, loss:  0.009004401974\n",
      "training step: 237103, loss:  0.003692979924\n",
      "training step: 237144, loss:  0.023757282645\n",
      "training step: 237185, loss:  0.003808671376\n",
      "training step: 237226, loss:  0.007341358811\n",
      "training step: 237267, loss:  0.023757282645\n",
      "training step: 237308, loss:  0.003866109066\n",
      "training step: 237349, loss:  0.002386306180\n",
      "training step: 237390, loss:  0.005003615282\n",
      "training step: 237431, loss:  0.007341358811\n",
      "training step: 237472, loss:  0.022392421961\n",
      "training step: 237513, loss:  0.002396283671\n",
      "training step: 237554, loss:  0.003473303979\n",
      "training step: 237595, loss:  0.003011468332\n",
      "training step: 237636, loss:  0.018698843196\n",
      "training step: 237677, loss:  0.002396283671\n",
      "training step: 237718, loss:  0.003002761398\n",
      "training step: 237759, loss:  0.003638759954\n",
      "training step: 237800, loss:  0.007752379868\n",
      "training step: 237841, loss:  0.003900909098\n",
      "training step: 237882, loss:  0.020691098645\n",
      "training step: 237923, loss:  0.003543942003\n",
      "training step: 237964, loss:  0.003120336682\n",
      "training step: 238005, loss:  0.004080093466\n",
      "training step: 238046, loss:  0.002930165734\n",
      "training step: 238087, loss:  0.025363046676\n",
      "training step: 238128, loss:  0.009473215789\n",
      "training step: 238169, loss:  0.005003615282\n",
      "training step: 238210, loss:  0.009004401974\n",
      "training step: 238251, loss:  0.003120336682\n",
      "training step: 238292, loss:  0.007341358811\n",
      "training step: 238333, loss:  0.022392421961\n",
      "training step: 238374, loss:  0.003692979924\n",
      "training step: 238415, loss:  0.003776642960\n",
      "training step: 238456, loss:  0.002975780051\n",
      "training step: 238497, loss:  0.011776376516\n",
      "training step: 238538, loss:  0.003120336682\n",
      "training step: 238579, loss:  0.003011468332\n",
      "training step: 238620, loss:  0.003638759954\n",
      "training step: 238661, loss:  0.025363046676\n",
      "training step: 238702, loss:  0.003776642960\n",
      "training step: 238743, loss:  0.002396283671\n",
      "training step: 238784, loss:  0.004087368958\n",
      "training step: 238825, loss:  0.007877553813\n",
      "training step: 238866, loss:  0.003900909098\n",
      "training step: 238907, loss:  0.002930165734\n",
      "training step: 238948, loss:  0.003135976614\n",
      "training step: 238989, loss:  0.025363046676\n",
      "training step: 239030, loss:  0.007752379868\n",
      "training step: 239071, loss:  0.011776376516\n",
      "training step: 239112, loss:  0.003135976614\n",
      "training step: 239153, loss:  0.003986798227\n",
      "training step: 239194, loss:  0.003011468332\n",
      "training step: 239235, loss:  0.003473303979\n",
      "training step: 239276, loss:  0.007341358811\n",
      "training step: 239317, loss:  0.005715856329\n",
      "training step: 239358, loss:  0.003002761398\n",
      "training step: 239399, loss:  0.003638759954\n",
      "training step: 239440, loss:  0.003011468332\n",
      "training step: 239481, loss:  0.004087368958\n",
      "training step: 239522, loss:  0.003467082279\n",
      "training step: 239563, loss:  0.014145905152\n",
      "training step: 239604, loss:  0.003284437582\n",
      "training step: 239645, loss:  0.003808671376\n",
      "training step: 239686, loss:  0.004080093466\n",
      "training step: 239727, loss:  0.002386306180\n",
      "training step: 239768, loss:  0.005485921632\n",
      "training step: 239809, loss:  0.005003615282\n",
      "training step: 239850, loss:  0.003866109066\n",
      "training step: 239891, loss:  0.016485802829\n",
      "training step: 239932, loss:  0.005003615282\n",
      "training step: 239973, loss:  0.003638759954\n",
      "training step: 240014, loss:  0.003120336682\n",
      "training step: 240055, loss:  0.002386306180\n",
      "training step: 240096, loss:  0.003900909098\n",
      "training step: 240137, loss:  0.002386306180\n",
      "training step: 240178, loss:  0.004087368958\n",
      "training step: 240219, loss:  0.009004401974\n",
      "training step: 240260, loss:  0.003808671376\n",
      "training step: 240301, loss:  0.003467082279\n",
      "training step: 240342, loss:  0.002396283671\n",
      "training step: 240383, loss:  0.003638759954\n",
      "training step: 240424, loss:  0.002396283671\n",
      "training step: 240465, loss:  0.003986798227\n",
      "training step: 240506, loss:  0.003776642960\n",
      "training step: 240547, loss:  0.003473303979\n",
      "training step: 240588, loss:  0.003638759954\n",
      "training step: 240629, loss:  0.005003615282\n",
      "training step: 240670, loss:  0.005003615282\n",
      "training step: 240711, loss:  0.002386306180\n",
      "training step: 240752, loss:  0.003011468332\n",
      "training step: 240793, loss:  0.003986798227\n",
      "training step: 240834, loss:  0.003002761398\n",
      "training step: 240875, loss:  0.007877553813\n",
      "training step: 240916, loss:  0.003880497534\n",
      "training step: 240957, loss:  0.002930165734\n",
      "training step: 240998, loss:  0.020691098645\n",
      "training step: 241039, loss:  0.003120336682\n",
      "training step: 241080, loss:  0.003776642960\n",
      "training step: 241121, loss:  0.022392421961\n",
      "training step: 241162, loss:  0.023757282645\n",
      "training step: 241203, loss:  0.003866109066\n",
      "training step: 241244, loss:  0.022392421961\n",
      "training step: 241285, loss:  0.024754593149\n",
      "training step: 241326, loss:  0.006285338197\n",
      "training step: 241367, loss:  0.003900909098\n",
      "training step: 241408, loss:  0.003135976614\n",
      "training step: 241449, loss:  0.003986798227\n",
      "training step: 241490, loss:  0.007752379868\n",
      "training step: 241531, loss:  0.005715856329\n",
      "training step: 241572, loss:  0.023757282645\n",
      "training step: 241613, loss:  0.002386306180\n",
      "training step: 241654, loss:  0.025363046676\n",
      "training step: 241695, loss:  0.003543942003\n",
      "training step: 241736, loss:  0.005485921632\n",
      "training step: 241777, loss:  0.003011468332\n",
      "training step: 241818, loss:  0.003011468332\n",
      "training step: 241859, loss:  0.020691098645\n",
      "training step: 241900, loss:  0.003866109066\n",
      "training step: 241941, loss:  0.007341358811\n",
      "training step: 241982, loss:  0.006285338197\n",
      "training step: 242023, loss:  0.006285338197\n",
      "training step: 242064, loss:  0.018698843196\n",
      "training step: 242105, loss:  0.003638759954\n",
      "training step: 242146, loss:  0.003638759954\n",
      "training step: 242187, loss:  0.003135976614\n",
      "training step: 242228, loss:  0.003866109066\n",
      "training step: 242269, loss:  0.003986798227\n",
      "training step: 242310, loss:  0.007341358811\n",
      "training step: 242351, loss:  0.002930165734\n",
      "training step: 242392, loss:  0.003135976614\n",
      "training step: 242433, loss:  0.016485802829\n",
      "training step: 242474, loss:  0.003866109066\n",
      "training step: 242515, loss:  0.004087368958\n",
      "training step: 242556, loss:  0.005715856329\n",
      "training step: 242597, loss:  0.002386306180\n",
      "training step: 242638, loss:  0.002396283671\n",
      "training step: 242679, loss:  0.018698843196\n",
      "training step: 242720, loss:  0.014145905152\n",
      "training step: 242761, loss:  0.002396283671\n",
      "training step: 242802, loss:  0.018698843196\n",
      "training step: 242843, loss:  0.014145905152\n",
      "training step: 242884, loss:  0.003011468332\n",
      "training step: 242925, loss:  0.009473215789\n",
      "training step: 242966, loss:  0.002396283671\n",
      "training step: 243007, loss:  0.002930165734\n",
      "training step: 243048, loss:  0.003120336682\n",
      "training step: 243089, loss:  0.014145905152\n",
      "training step: 243130, loss:  0.023757282645\n",
      "training step: 243171, loss:  0.003002761398\n",
      "training step: 243212, loss:  0.003120336682\n",
      "training step: 243253, loss:  0.002386306180\n",
      "training step: 243294, loss:  0.011776376516\n",
      "training step: 243335, loss:  0.004087368958\n",
      "training step: 243376, loss:  0.003543942003\n",
      "training step: 243417, loss:  0.025363046676\n",
      "training step: 243458, loss:  0.003543942003\n",
      "training step: 243499, loss:  0.009004401974\n",
      "training step: 243540, loss:  0.003776642960\n",
      "training step: 243581, loss:  0.003135976614\n",
      "training step: 243622, loss:  0.009473215789\n",
      "training step: 243663, loss:  0.023757282645\n",
      "training step: 243704, loss:  0.018698843196\n",
      "training step: 243745, loss:  0.023757282645\n",
      "training step: 243786, loss:  0.009473215789\n",
      "training step: 243827, loss:  0.002386306180\n",
      "training step: 243868, loss:  0.016485802829\n",
      "training step: 243909, loss:  0.024754593149\n",
      "training step: 243950, loss:  0.003638759954\n",
      "training step: 243991, loss:  0.002930165734\n",
      "training step: 244032, loss:  0.003011468332\n",
      "training step: 244073, loss:  0.022392421961\n",
      "training step: 244114, loss:  0.007341358811\n",
      "training step: 244155, loss:  0.002396283671\n",
      "training step: 244196, loss:  0.004087368958\n",
      "training step: 244237, loss:  0.003011468332\n",
      "training step: 244278, loss:  0.003543942003\n",
      "training step: 244319, loss:  0.018698843196\n",
      "training step: 244360, loss:  0.018698843196\n",
      "training step: 244401, loss:  0.003284437582\n",
      "training step: 244442, loss:  0.003776642960\n",
      "training step: 244483, loss:  0.003543942003\n",
      "training step: 244524, loss:  0.003011468332\n",
      "training step: 244565, loss:  0.025363046676\n",
      "training step: 244606, loss:  0.004087368958\n",
      "training step: 244647, loss:  0.002975780051\n",
      "training step: 244688, loss:  0.011776376516\n",
      "training step: 244729, loss:  0.003120336682\n",
      "training step: 244770, loss:  0.009004401974\n",
      "training step: 244811, loss:  0.022392421961\n",
      "training step: 244852, loss:  0.020691098645\n",
      "training step: 244893, loss:  0.004080093466\n",
      "training step: 244934, loss:  0.020691098645\n",
      "training step: 244975, loss:  0.007752379868\n",
      "training step: 245016, loss:  0.022392421961\n",
      "training step: 245057, loss:  0.014145905152\n",
      "training step: 245098, loss:  0.003776642960\n",
      "training step: 245139, loss:  0.003638759954\n",
      "training step: 245180, loss:  0.014145905152\n",
      "training step: 245221, loss:  0.002930165734\n",
      "training step: 245262, loss:  0.025363046676\n",
      "training step: 245303, loss:  0.003002761398\n",
      "training step: 245344, loss:  0.005485921632\n",
      "training step: 245385, loss:  0.003986798227\n",
      "training step: 245426, loss:  0.007341358811\n",
      "training step: 245467, loss:  0.003011468332\n",
      "training step: 245508, loss:  0.002930165734\n",
      "training step: 245549, loss:  0.003002761398\n",
      "training step: 245590, loss:  0.007752379868\n",
      "training step: 245631, loss:  0.018698843196\n",
      "training step: 245672, loss:  0.009473215789\n",
      "training step: 245713, loss:  0.003543942003\n",
      "training step: 245754, loss:  0.003692979924\n",
      "training step: 245795, loss:  0.003284437582\n",
      "training step: 245836, loss:  0.025568028912\n",
      "training step: 245877, loss:  0.003543942003\n",
      "training step: 245918, loss:  0.002396283671\n",
      "training step: 245959, loss:  0.011776376516\n",
      "training step: 246000, loss:  0.004087368958\n",
      "training step: 246041, loss:  0.022392421961\n",
      "training step: 246082, loss:  0.003011468332\n",
      "training step: 246123, loss:  0.002386306180\n",
      "training step: 246164, loss:  0.025568028912\n",
      "training step: 246205, loss:  0.003473303979\n",
      "training step: 246246, loss:  0.003692979924\n",
      "training step: 246287, loss:  0.003880497534\n",
      "training step: 246328, loss:  0.002396283671\n",
      "training step: 246369, loss:  0.016485802829\n",
      "training step: 246410, loss:  0.003638759954\n",
      "training step: 246451, loss:  0.023757282645\n",
      "training step: 246492, loss:  0.025568028912\n",
      "training step: 246533, loss:  0.007752379868\n",
      "training step: 246574, loss:  0.003900909098\n",
      "training step: 246615, loss:  0.004087368958\n",
      "training step: 246656, loss:  0.007877553813\n",
      "training step: 246697, loss:  0.003284437582\n",
      "training step: 246738, loss:  0.003638759954\n",
      "training step: 246779, loss:  0.003120336682\n",
      "training step: 246820, loss:  0.022392421961\n",
      "training step: 246861, loss:  0.005485921632\n",
      "training step: 246902, loss:  0.003866109066\n",
      "training step: 246943, loss:  0.003467082279\n",
      "training step: 246984, loss:  0.003808671376\n",
      "training step: 247025, loss:  0.020691098645\n",
      "training step: 247066, loss:  0.002975780051\n",
      "training step: 247107, loss:  0.003467082279\n",
      "training step: 247148, loss:  0.003120336682\n",
      "training step: 247189, loss:  0.007752379868\n",
      "training step: 247230, loss:  0.003986798227\n",
      "training step: 247271, loss:  0.002396283671\n",
      "training step: 247312, loss:  0.002386306180\n",
      "training step: 247353, loss:  0.003776642960\n",
      "training step: 247394, loss:  0.003880497534\n",
      "training step: 247435, loss:  0.003808671376\n",
      "training step: 247476, loss:  0.003467082279\n",
      "training step: 247517, loss:  0.005003615282\n",
      "training step: 247558, loss:  0.007341358811\n",
      "training step: 247599, loss:  0.025363046676\n",
      "training step: 247640, loss:  0.003776642960\n",
      "training step: 247681, loss:  0.007341358811\n",
      "training step: 247722, loss:  0.005715856329\n",
      "training step: 247763, loss:  0.025363046676\n",
      "training step: 247804, loss:  0.003900909098\n",
      "training step: 247845, loss:  0.011776376516\n",
      "training step: 247886, loss:  0.003467082279\n",
      "training step: 247927, loss:  0.002386306180\n",
      "training step: 247968, loss:  0.003473303979\n",
      "training step: 248009, loss:  0.009004401974\n",
      "training step: 248050, loss:  0.003638759954\n",
      "training step: 248091, loss:  0.003900909098\n",
      "training step: 248132, loss:  0.009004401974\n",
      "training step: 248173, loss:  0.014145905152\n",
      "training step: 248214, loss:  0.011776376516\n",
      "training step: 248255, loss:  0.003011468332\n",
      "training step: 248296, loss:  0.003776642960\n",
      "training step: 248337, loss:  0.007341358811\n",
      "training step: 248378, loss:  0.003692979924\n",
      "training step: 248419, loss:  0.003011468332\n",
      "training step: 248460, loss:  0.004080093466\n",
      "training step: 248501, loss:  0.003776642960\n",
      "training step: 248542, loss:  0.002386306180\n",
      "training step: 248583, loss:  0.005485921632\n",
      "training step: 248624, loss:  0.004087368958\n",
      "training step: 248665, loss:  0.025568028912\n",
      "training step: 248706, loss:  0.003866109066\n",
      "training step: 248747, loss:  0.005485921632\n",
      "training step: 248788, loss:  0.003986798227\n",
      "training step: 248829, loss:  0.003808671376\n",
      "training step: 248870, loss:  0.011776376516\n",
      "training step: 248911, loss:  0.016485802829\n",
      "training step: 248952, loss:  0.006285338197\n",
      "training step: 248993, loss:  0.003473303979\n",
      "training step: 249034, loss:  0.024754593149\n",
      "training step: 249075, loss:  0.011776376516\n",
      "training step: 249116, loss:  0.003002761398\n",
      "training step: 249157, loss:  0.005715856329\n",
      "training step: 249198, loss:  0.003002761398\n",
      "training step: 249239, loss:  0.025363046676\n",
      "training step: 249280, loss:  0.003284437582\n",
      "training step: 249321, loss:  0.003866109066\n",
      "training step: 249362, loss:  0.024754593149\n",
      "training step: 249403, loss:  0.007877553813\n",
      "training step: 249444, loss:  0.025363046676\n",
      "training step: 249485, loss:  0.024754593149\n",
      "training step: 249526, loss:  0.025363046676\n",
      "training step: 249567, loss:  0.006285338197\n",
      "training step: 249608, loss:  0.005485921632\n",
      "training step: 249649, loss:  0.003900909098\n",
      "training step: 249690, loss:  0.003002761398\n",
      "training step: 249731, loss:  0.003900909098\n",
      "training step: 249772, loss:  0.003120336682\n",
      "training step: 249813, loss:  0.003880497534\n",
      "training step: 249854, loss:  0.003776642960\n",
      "training step: 249895, loss:  0.006285338197\n",
      "training step: 249936, loss:  0.025363046676\n",
      "training step: 249977, loss:  0.003473303979\n",
      "training step: 250018, loss:  0.003011468332\n",
      "training step: 250059, loss:  0.004080093466\n",
      "training step: 250100, loss:  0.003002761398\n",
      "training step: 250141, loss:  0.007752379868\n",
      "training step: 250182, loss:  0.003808671376\n",
      "training step: 250223, loss:  0.003284437582\n",
      "training step: 250264, loss:  0.025363046676\n",
      "training step: 250305, loss:  0.003808671376\n",
      "training step: 250346, loss:  0.009004401974\n",
      "training step: 250387, loss:  0.009004401974\n",
      "training step: 250428, loss:  0.024754593149\n",
      "training step: 250469, loss:  0.004080093466\n",
      "training step: 250510, loss:  0.022392421961\n",
      "training step: 250551, loss:  0.003776642960\n",
      "training step: 250592, loss:  0.005715856329\n",
      "training step: 250633, loss:  0.025363046676\n",
      "training step: 250674, loss:  0.022392421961\n",
      "training step: 250715, loss:  0.014145905152\n",
      "training step: 250756, loss:  0.003011468332\n",
      "training step: 250797, loss:  0.003011468332\n",
      "training step: 250838, loss:  0.003880497534\n",
      "training step: 250879, loss:  0.002975780051\n",
      "training step: 250920, loss:  0.003467082279\n",
      "training step: 250961, loss:  0.018698843196\n",
      "training step: 251002, loss:  0.003880497534\n",
      "training step: 251043, loss:  0.003880497534\n",
      "training step: 251084, loss:  0.005715856329\n",
      "training step: 251125, loss:  0.011776376516\n",
      "training step: 251166, loss:  0.003135976614\n",
      "training step: 251207, loss:  0.016485802829\n",
      "training step: 251248, loss:  0.024754593149\n",
      "training step: 251289, loss:  0.003467082279\n",
      "training step: 251330, loss:  0.014145905152\n",
      "training step: 251371, loss:  0.003011468332\n",
      "training step: 251412, loss:  0.003467082279\n",
      "training step: 251453, loss:  0.002930165734\n",
      "training step: 251494, loss:  0.004087368958\n",
      "training step: 251535, loss:  0.004087368958\n",
      "training step: 251576, loss:  0.003900909098\n",
      "training step: 251617, loss:  0.025363046676\n",
      "training step: 251658, loss:  0.003900909098\n",
      "training step: 251699, loss:  0.003776642960\n",
      "training step: 251740, loss:  0.002930165734\n",
      "training step: 251781, loss:  0.005485921632\n",
      "training step: 251822, loss:  0.002396283671\n",
      "training step: 251863, loss:  0.007877553813\n",
      "training step: 251904, loss:  0.003120336682\n",
      "training step: 251945, loss:  0.025568028912\n",
      "training step: 251986, loss:  0.002975780051\n",
      "training step: 252027, loss:  0.005715856329\n",
      "training step: 252068, loss:  0.025568028912\n",
      "training step: 252109, loss:  0.003900909098\n",
      "training step: 252150, loss:  0.002386306180\n",
      "training step: 252191, loss:  0.003002761398\n",
      "training step: 252232, loss:  0.003467082279\n",
      "training step: 252273, loss:  0.023757282645\n",
      "training step: 252314, loss:  0.024754593149\n",
      "training step: 252355, loss:  0.025363046676\n",
      "training step: 252396, loss:  0.023757282645\n",
      "training step: 252437, loss:  0.020691098645\n",
      "training step: 252478, loss:  0.003120336682\n",
      "training step: 252519, loss:  0.005485921632\n",
      "training step: 252560, loss:  0.005003615282\n",
      "training step: 252601, loss:  0.003473303979\n",
      "training step: 252642, loss:  0.011776376516\n",
      "training step: 252683, loss:  0.005715856329\n",
      "training step: 252724, loss:  0.003473303979\n",
      "training step: 252765, loss:  0.002930165734\n",
      "training step: 252806, loss:  0.016485802829\n",
      "training step: 252847, loss:  0.006285338197\n",
      "training step: 252888, loss:  0.003467082279\n",
      "training step: 252929, loss:  0.002386306180\n",
      "training step: 252970, loss:  0.005715856329\n",
      "training step: 253011, loss:  0.014145905152\n",
      "training step: 253052, loss:  0.003880497534\n",
      "training step: 253093, loss:  0.018698843196\n",
      "training step: 253134, loss:  0.003900909098\n",
      "training step: 253175, loss:  0.018698843196\n",
      "training step: 253216, loss:  0.003135976614\n",
      "training step: 253257, loss:  0.003135976614\n",
      "training step: 253298, loss:  0.003986798227\n",
      "training step: 253339, loss:  0.009473215789\n",
      "training step: 253380, loss:  0.022392421961\n",
      "training step: 253421, loss:  0.004087368958\n",
      "training step: 253462, loss:  0.002386306180\n",
      "training step: 253503, loss:  0.003120336682\n",
      "training step: 253544, loss:  0.003135976614\n",
      "training step: 253585, loss:  0.002975780051\n",
      "training step: 253626, loss:  0.007341358811\n",
      "training step: 253667, loss:  0.005715856329\n",
      "training step: 253708, loss:  0.005485921632\n",
      "training step: 253749, loss:  0.007752379868\n",
      "training step: 253790, loss:  0.002930165734\n",
      "training step: 253831, loss:  0.003692979924\n",
      "training step: 253872, loss:  0.005715856329\n",
      "training step: 253913, loss:  0.018698843196\n",
      "training step: 253954, loss:  0.022392421961\n",
      "training step: 253995, loss:  0.009473215789\n",
      "training step: 254036, loss:  0.009004401974\n",
      "training step: 254077, loss:  0.009473215789\n",
      "training step: 254118, loss:  0.003866109066\n",
      "training step: 254159, loss:  0.004080093466\n",
      "training step: 254200, loss:  0.003866109066\n",
      "training step: 254241, loss:  0.003135976614\n",
      "training step: 254282, loss:  0.018698843196\n",
      "training step: 254323, loss:  0.003808671376\n",
      "training step: 254364, loss:  0.005485921632\n",
      "training step: 254405, loss:  0.003692979924\n",
      "training step: 254446, loss:  0.005715856329\n",
      "training step: 254487, loss:  0.003866109066\n",
      "training step: 254528, loss:  0.005715856329\n",
      "training step: 254569, loss:  0.005715856329\n",
      "training step: 254610, loss:  0.020691098645\n",
      "training step: 254651, loss:  0.003880497534\n",
      "training step: 254692, loss:  0.003638759954\n",
      "training step: 254733, loss:  0.009473215789\n",
      "training step: 254774, loss:  0.009473215789\n",
      "training step: 254815, loss:  0.003986798227\n",
      "training step: 254856, loss:  0.002930165734\n",
      "training step: 254897, loss:  0.003473303979\n",
      "training step: 254938, loss:  0.003776642960\n",
      "training step: 254979, loss:  0.003900909098\n",
      "training step: 255020, loss:  0.003543942003\n",
      "training step: 255061, loss:  0.002975780051\n",
      "training step: 255102, loss:  0.009004401974\n",
      "training step: 255143, loss:  0.023757282645\n",
      "training step: 255184, loss:  0.007341358811\n",
      "training step: 255225, loss:  0.007877553813\n",
      "training step: 255266, loss:  0.005715856329\n",
      "training step: 255307, loss:  0.003473303979\n",
      "training step: 255348, loss:  0.002930165734\n",
      "training step: 255389, loss:  0.006285338197\n",
      "training step: 255430, loss:  0.005485921632\n",
      "training step: 255471, loss:  0.003135976614\n",
      "training step: 255512, loss:  0.018698843196\n",
      "training step: 255553, loss:  0.025363046676\n",
      "training step: 255594, loss:  0.018698843196\n",
      "training step: 255635, loss:  0.003120336682\n",
      "training step: 255676, loss:  0.003900909098\n",
      "training step: 255717, loss:  0.025568028912\n",
      "training step: 255758, loss:  0.002930165734\n",
      "training step: 255799, loss:  0.003467082279\n",
      "training step: 255840, loss:  0.009004401974\n",
      "training step: 255881, loss:  0.003808671376\n",
      "training step: 255922, loss:  0.025363046676\n",
      "training step: 255963, loss:  0.024754593149\n",
      "training step: 256004, loss:  0.018698843196\n",
      "training step: 256045, loss:  0.003467082279\n",
      "training step: 256086, loss:  0.003638759954\n",
      "training step: 256127, loss:  0.005485921632\n",
      "training step: 256168, loss:  0.009473215789\n",
      "training step: 256209, loss:  0.007341358811\n",
      "training step: 256250, loss:  0.016485802829\n",
      "training step: 256291, loss:  0.002396283671\n",
      "training step: 256332, loss:  0.003543942003\n",
      "training step: 256373, loss:  0.005003615282\n",
      "training step: 256414, loss:  0.003638759954\n",
      "training step: 256455, loss:  0.003638759954\n",
      "training step: 256496, loss:  0.018698843196\n",
      "training step: 256537, loss:  0.002975780051\n",
      "training step: 256578, loss:  0.003135976614\n",
      "training step: 256619, loss:  0.024754593149\n",
      "training step: 256660, loss:  0.005003615282\n",
      "training step: 256701, loss:  0.003135976614\n",
      "training step: 256742, loss:  0.020691098645\n",
      "training step: 256783, loss:  0.024754593149\n",
      "training step: 256824, loss:  0.025363046676\n",
      "training step: 256865, loss:  0.003473303979\n",
      "training step: 256906, loss:  0.003473303979\n",
      "training step: 256947, loss:  0.016485802829\n",
      "training step: 256988, loss:  0.025363046676\n",
      "training step: 257029, loss:  0.004080093466\n",
      "training step: 257070, loss:  0.022392421961\n",
      "training step: 257111, loss:  0.002386306180\n",
      "training step: 257152, loss:  0.002930165734\n",
      "training step: 257193, loss:  0.007341358811\n",
      "training step: 257234, loss:  0.007341358811\n",
      "training step: 257275, loss:  0.007341358811\n",
      "training step: 257316, loss:  0.005485921632\n",
      "training step: 257357, loss:  0.007877553813\n",
      "training step: 257398, loss:  0.014145905152\n",
      "training step: 257439, loss:  0.002975780051\n",
      "training step: 257480, loss:  0.003692979924\n",
      "training step: 257521, loss:  0.004087368958\n",
      "training step: 257562, loss:  0.002975780051\n",
      "training step: 257603, loss:  0.016485802829\n",
      "training step: 257644, loss:  0.003776642960\n",
      "training step: 257685, loss:  0.003135976614\n",
      "training step: 257726, loss:  0.007341358811\n",
      "training step: 257767, loss:  0.004087368958\n",
      "training step: 257808, loss:  0.009004401974\n",
      "training step: 257849, loss:  0.006285338197\n",
      "training step: 257890, loss:  0.003692979924\n",
      "training step: 257931, loss:  0.016485802829\n",
      "training step: 257972, loss:  0.003120336682\n",
      "training step: 258013, loss:  0.005715856329\n",
      "training step: 258054, loss:  0.004080093466\n",
      "training step: 258095, loss:  0.014145905152\n",
      "training step: 258136, loss:  0.007752379868\n",
      "training step: 258177, loss:  0.014145905152\n",
      "training step: 258218, loss:  0.003808671376\n",
      "training step: 258259, loss:  0.003880497534\n",
      "training step: 258300, loss:  0.007877553813\n",
      "training step: 258341, loss:  0.011776376516\n",
      "training step: 258382, loss:  0.003692979924\n",
      "training step: 258423, loss:  0.002386306180\n",
      "training step: 258464, loss:  0.025568028912\n",
      "training step: 258505, loss:  0.004087368958\n",
      "training step: 258546, loss:  0.006285338197\n",
      "training step: 258587, loss:  0.003692979924\n",
      "training step: 258628, loss:  0.003135976614\n",
      "training step: 258669, loss:  0.003808671376\n",
      "training step: 258710, loss:  0.014145905152\n",
      "training step: 258751, loss:  0.003284437582\n",
      "training step: 258792, loss:  0.016485802829\n",
      "training step: 258833, loss:  0.002386306180\n",
      "training step: 258874, loss:  0.003900909098\n",
      "training step: 258915, loss:  0.003986798227\n",
      "training step: 258956, loss:  0.003808671376\n",
      "training step: 258997, loss:  0.003900909098\n",
      "training step: 259038, loss:  0.020691098645\n",
      "training step: 259079, loss:  0.003284437582\n",
      "training step: 259120, loss:  0.003692979924\n",
      "training step: 259161, loss:  0.002396283671\n",
      "training step: 259202, loss:  0.002386306180\n",
      "training step: 259243, loss:  0.005485921632\n",
      "training step: 259284, loss:  0.014145905152\n",
      "training step: 259325, loss:  0.004080093466\n",
      "training step: 259366, loss:  0.024754593149\n",
      "training step: 259407, loss:  0.024754593149\n",
      "training step: 259448, loss:  0.003543942003\n",
      "training step: 259489, loss:  0.003002761398\n",
      "training step: 259530, loss:  0.003467082279\n",
      "training step: 259571, loss:  0.003692979924\n",
      "training step: 259612, loss:  0.025363046676\n",
      "training step: 259653, loss:  0.023757282645\n",
      "training step: 259694, loss:  0.003002761398\n",
      "training step: 259735, loss:  0.003880497534\n",
      "training step: 259776, loss:  0.003692979924\n",
      "training step: 259817, loss:  0.003638759954\n",
      "training step: 259858, loss:  0.004087368958\n",
      "training step: 259899, loss:  0.003638759954\n",
      "training step: 259940, loss:  0.003467082279\n",
      "training step: 259981, loss:  0.003776642960\n",
      "training step: 260022, loss:  0.003473303979\n",
      "training step: 260063, loss:  0.003467082279\n",
      "training step: 260104, loss:  0.003543942003\n",
      "training step: 260145, loss:  0.003638759954\n",
      "training step: 260186, loss:  0.003986798227\n",
      "training step: 260227, loss:  0.003120336682\n",
      "training step: 260268, loss:  0.003467082279\n",
      "training step: 260309, loss:  0.002386306180\n",
      "training step: 260350, loss:  0.003473303979\n",
      "training step: 260391, loss:  0.007877553813\n",
      "training step: 260432, loss:  0.002396283671\n",
      "training step: 260473, loss:  0.023757282645\n",
      "training step: 260514, loss:  0.003002761398\n",
      "training step: 260555, loss:  0.002386306180\n",
      "training step: 260596, loss:  0.003986798227\n",
      "training step: 260637, loss:  0.005003615282\n",
      "training step: 260678, loss:  0.005715856329\n",
      "training step: 260719, loss:  0.007752379868\n",
      "training step: 260760, loss:  0.003284437582\n",
      "training step: 260801, loss:  0.005485921632\n",
      "training step: 260842, loss:  0.003002761398\n",
      "training step: 260883, loss:  0.003986798227\n",
      "training step: 260924, loss:  0.025568028912\n",
      "training step: 260965, loss:  0.007341358811\n",
      "training step: 261006, loss:  0.004080093466\n",
      "training step: 261047, loss:  0.002975780051\n",
      "training step: 261088, loss:  0.009473215789\n",
      "training step: 261129, loss:  0.009004401974\n",
      "training step: 261170, loss:  0.023757282645\n",
      "training step: 261211, loss:  0.004080093466\n",
      "training step: 261252, loss:  0.002930165734\n",
      "training step: 261293, loss:  0.022392421961\n",
      "training step: 261334, loss:  0.014145905152\n",
      "training step: 261375, loss:  0.004080093466\n",
      "training step: 261416, loss:  0.003473303979\n",
      "training step: 261457, loss:  0.003776642960\n",
      "training step: 261498, loss:  0.014145905152\n",
      "training step: 261539, loss:  0.007341358811\n",
      "training step: 261580, loss:  0.007877553813\n",
      "training step: 261621, loss:  0.006285338197\n",
      "training step: 261662, loss:  0.018698843196\n",
      "training step: 261703, loss:  0.011776376516\n",
      "training step: 261744, loss:  0.003002761398\n",
      "training step: 261785, loss:  0.002386306180\n",
      "training step: 261826, loss:  0.003986798227\n",
      "training step: 261867, loss:  0.003473303979\n",
      "training step: 261908, loss:  0.006285338197\n",
      "training step: 261949, loss:  0.009473215789\n",
      "training step: 261990, loss:  0.003135976614\n",
      "training step: 262031, loss:  0.004080093466\n",
      "training step: 262072, loss:  0.018698843196\n",
      "training step: 262113, loss:  0.003135976614\n",
      "training step: 262154, loss:  0.003638759954\n",
      "training step: 262195, loss:  0.007752379868\n",
      "training step: 262236, loss:  0.002396283671\n",
      "training step: 262277, loss:  0.014145905152\n",
      "training step: 262318, loss:  0.003467082279\n",
      "training step: 262359, loss:  0.009473215789\n",
      "training step: 262400, loss:  0.003135976614\n",
      "training step: 262441, loss:  0.024754593149\n",
      "training step: 262482, loss:  0.005485921632\n",
      "training step: 262523, loss:  0.009473215789\n",
      "training step: 262564, loss:  0.025568028912\n",
      "training step: 262605, loss:  0.025363046676\n",
      "training step: 262646, loss:  0.024754593149\n",
      "training step: 262687, loss:  0.003543942003\n",
      "training step: 262728, loss:  0.004080093466\n",
      "training step: 262769, loss:  0.003467082279\n",
      "training step: 262810, loss:  0.004087368958\n",
      "training step: 262851, loss:  0.003120336682\n",
      "training step: 262892, loss:  0.003986798227\n",
      "training step: 262933, loss:  0.022392421961\n",
      "training step: 262974, loss:  0.003638759954\n",
      "training step: 263015, loss:  0.003473303979\n",
      "training step: 263056, loss:  0.007341358811\n",
      "training step: 263097, loss:  0.018698843196\n",
      "training step: 263138, loss:  0.003120336682\n",
      "training step: 263179, loss:  0.005485921632\n",
      "training step: 263220, loss:  0.003866109066\n",
      "training step: 263261, loss:  0.003776642960\n",
      "training step: 263302, loss:  0.014145905152\n",
      "training step: 263343, loss:  0.023757282645\n",
      "training step: 263384, loss:  0.007341358811\n",
      "training step: 263425, loss:  0.002386306180\n",
      "training step: 263466, loss:  0.006285338197\n",
      "training step: 263507, loss:  0.003467082279\n",
      "training step: 263548, loss:  0.007877553813\n",
      "training step: 263589, loss:  0.009473215789\n",
      "training step: 263630, loss:  0.020691098645\n",
      "training step: 263671, loss:  0.007341358811\n",
      "training step: 263712, loss:  0.003692979924\n",
      "training step: 263753, loss:  0.023757282645\n",
      "training step: 263794, loss:  0.003776642960\n",
      "training step: 263835, loss:  0.020691098645\n",
      "training step: 263876, loss:  0.002396283671\n",
      "training step: 263917, loss:  0.009473215789\n",
      "training step: 263958, loss:  0.003986798227\n",
      "training step: 263999, loss:  0.003776642960\n",
      "training step: 264040, loss:  0.024754593149\n",
      "training step: 264081, loss:  0.005485921632\n",
      "training step: 264122, loss:  0.002975780051\n",
      "training step: 264163, loss:  0.003692979924\n",
      "training step: 264204, loss:  0.014145905152\n",
      "training step: 264245, loss:  0.020691098645\n",
      "training step: 264286, loss:  0.003692979924\n",
      "training step: 264327, loss:  0.005485921632\n",
      "training step: 264368, loss:  0.009004401974\n",
      "training step: 264409, loss:  0.003692979924\n",
      "training step: 264450, loss:  0.003284437582\n",
      "training step: 264491, loss:  0.024754593149\n",
      "training step: 264532, loss:  0.014145905152\n",
      "training step: 264573, loss:  0.007752379868\n",
      "training step: 264614, loss:  0.007341358811\n",
      "training step: 264655, loss:  0.003776642960\n",
      "training step: 264696, loss:  0.020691098645\n",
      "training step: 264737, loss:  0.024754593149\n",
      "training step: 264778, loss:  0.022392421961\n",
      "training step: 264819, loss:  0.002930165734\n",
      "training step: 264860, loss:  0.009473215789\n",
      "training step: 264901, loss:  0.020691098645\n",
      "training step: 264942, loss:  0.007877553813\n",
      "training step: 264983, loss:  0.004080093466\n",
      "training step: 265024, loss:  0.007877553813\n",
      "training step: 265065, loss:  0.016485802829\n",
      "training step: 265106, loss:  0.011776376516\n",
      "training step: 265147, loss:  0.003880497534\n",
      "training step: 265188, loss:  0.003467082279\n",
      "training step: 265229, loss:  0.003135976614\n",
      "training step: 265270, loss:  0.003808671376\n",
      "training step: 265311, loss:  0.003808671376\n",
      "training step: 265352, loss:  0.022392421961\n",
      "training step: 265393, loss:  0.004087368958\n",
      "training step: 265434, loss:  0.020691098645\n",
      "training step: 265475, loss:  0.003776642960\n",
      "training step: 265516, loss:  0.003880497534\n",
      "training step: 265557, loss:  0.003543942003\n",
      "training step: 265598, loss:  0.023757282645\n",
      "training step: 265639, loss:  0.003120336682\n",
      "training step: 265680, loss:  0.003776642960\n",
      "training step: 265721, loss:  0.003543942003\n",
      "training step: 265762, loss:  0.005485921632\n",
      "training step: 265803, loss:  0.002396283671\n",
      "training step: 265844, loss:  0.011776376516\n",
      "training step: 265885, loss:  0.014145905152\n",
      "training step: 265926, loss:  0.004087368958\n",
      "training step: 265967, loss:  0.005003615282\n",
      "training step: 266008, loss:  0.003284437582\n",
      "training step: 266049, loss:  0.009473215789\n",
      "training step: 266090, loss:  0.006285338197\n",
      "training step: 266131, loss:  0.024754593149\n",
      "training step: 266172, loss:  0.009004401974\n",
      "training step: 266213, loss:  0.002396283671\n",
      "training step: 266254, loss:  0.009004401974\n",
      "training step: 266295, loss:  0.005715856329\n",
      "training step: 266336, loss:  0.003467082279\n",
      "training step: 266377, loss:  0.011776376516\n",
      "training step: 266418, loss:  0.025568028912\n",
      "training step: 266459, loss:  0.003473303979\n",
      "training step: 266500, loss:  0.003900909098\n",
      "training step: 266541, loss:  0.011776376516\n",
      "training step: 266582, loss:  0.006285338197\n",
      "training step: 270928, loss:  0.003002761398\n",
      "training step: 270969, loss:  0.024754593149\n",
      "training step: 271010, loss:  0.014145905152\n",
      "training step: 271051, loss:  0.003284437582\n",
      "training step: 271092, loss:  0.014145905152\n",
      "training step: 271133, loss:  0.003692979924\n",
      "training step: 271174, loss:  0.003011468332\n",
      "training step: 271215, loss:  0.009473215789\n",
      "training step: 271256, loss:  0.003900909098\n",
      "training step: 271297, loss:  0.003135976614\n",
      "training step: 271338, loss:  0.022392421961\n",
      "training step: 271379, loss:  0.007877553813\n",
      "training step: 271420, loss:  0.020691098645\n",
      "training step: 271461, loss:  0.003467082279\n",
      "training step: 271502, loss:  0.025363046676\n",
      "training step: 271543, loss:  0.003120336682\n",
      "training step: 271584, loss:  0.003986798227\n",
      "training step: 271625, loss:  0.009004401974\n",
      "training step: 271666, loss:  0.005715856329\n",
      "training step: 271707, loss:  0.003866109066\n",
      "training step: 271748, loss:  0.003776642960\n",
      "training step: 271789, loss:  0.004087368958\n",
      "training step: 271830, loss:  0.003900909098\n",
      "training step: 271871, loss:  0.006285338197\n",
      "training step: 271912, loss:  0.003986798227\n",
      "training step: 271953, loss:  0.024754593149\n",
      "training step: 271994, loss:  0.024754593149\n",
      "training step: 272035, loss:  0.003900909098\n",
      "training step: 272076, loss:  0.018698843196\n",
      "training step: 272117, loss:  0.003900909098\n",
      "training step: 272158, loss:  0.016485802829\n",
      "training step: 272199, loss:  0.003692979924\n",
      "training step: 272240, loss:  0.004087368958\n",
      "training step: 272281, loss:  0.014145905152\n",
      "training step: 272322, loss:  0.003808671376\n",
      "training step: 272363, loss:  0.004080093466\n",
      "training step: 272404, loss:  0.025568028912\n",
      "training step: 272445, loss:  0.007341358811\n",
      "training step: 272486, loss:  0.002386306180\n",
      "training step: 272527, loss:  0.005485921632\n",
      "training step: 272568, loss:  0.009473215789\n",
      "training step: 272609, loss:  0.009473215789\n",
      "training step: 272650, loss:  0.002930165734\n",
      "training step: 272691, loss:  0.007752379868\n",
      "training step: 272732, loss:  0.011776376516\n",
      "training step: 272773, loss:  0.020691098645\n",
      "training step: 272814, loss:  0.014145905152\n",
      "training step: 272855, loss:  0.003284437582\n",
      "training step: 272896, loss:  0.025363046676\n",
      "training step: 272937, loss:  0.024754593149\n",
      "training step: 272978, loss:  0.002930165734\n",
      "training step: 273019, loss:  0.009004401974\n",
      "training step: 273060, loss:  0.003002761398\n",
      "training step: 273101, loss:  0.003880497534\n",
      "training step: 273142, loss:  0.025363046676\n",
      "training step: 273183, loss:  0.003866109066\n",
      "training step: 273224, loss:  0.005003615282\n",
      "training step: 273265, loss:  0.005003615282\n",
      "training step: 273306, loss:  0.005715856329\n",
      "training step: 273347, loss:  0.018698843196\n",
      "training step: 273388, loss:  0.018698843196\n",
      "training step: 273429, loss:  0.003135976614\n",
      "training step: 273470, loss:  0.003120336682\n",
      "training step: 273511, loss:  0.014145905152\n",
      "training step: 273552, loss:  0.004080093466\n",
      "training step: 273593, loss:  0.003986798227\n",
      "training step: 273634, loss:  0.009004401974\n",
      "training step: 273675, loss:  0.002930165734\n",
      "training step: 273716, loss:  0.011776376516\n",
      "training step: 273757, loss:  0.002386306180\n",
      "training step: 273798, loss:  0.023757282645\n",
      "training step: 273839, loss:  0.003866109066\n",
      "training step: 273880, loss:  0.003135976614\n",
      "training step: 273921, loss:  0.003986798227\n",
      "training step: 273962, loss:  0.003473303979\n",
      "training step: 274003, loss:  0.004080093466\n",
      "training step: 274044, loss:  0.003866109066\n",
      "training step: 274085, loss:  0.023757282645\n",
      "training step: 274126, loss:  0.003776642960\n",
      "training step: 274167, loss:  0.009004401974\n",
      "training step: 274208, loss:  0.016485802829\n",
      "training step: 274249, loss:  0.002386306180\n",
      "training step: 274290, loss:  0.005715856329\n",
      "training step: 274331, loss:  0.025363046676\n",
      "training step: 274372, loss:  0.016485802829\n",
      "training step: 274413, loss:  0.024754593149\n",
      "training step: 274454, loss:  0.002930165734\n",
      "training step: 274495, loss:  0.020691098645\n",
      "training step: 274536, loss:  0.003776642960\n",
      "training step: 274577, loss:  0.003002761398\n",
      "training step: 274618, loss:  0.023757282645\n",
      "training step: 274659, loss:  0.003866109066\n",
      "training step: 274700, loss:  0.016485802829\n",
      "training step: 274741, loss:  0.005715856329\n",
      "training step: 274782, loss:  0.007877553813\n",
      "training step: 274823, loss:  0.003776642960\n",
      "training step: 274864, loss:  0.011776376516\n",
      "training step: 274905, loss:  0.003284437582\n",
      "training step: 274946, loss:  0.003120336682\n",
      "training step: 274987, loss:  0.003473303979\n",
      "training step: 275028, loss:  0.020691098645\n",
      "training step: 275069, loss:  0.005715856329\n",
      "training step: 275110, loss:  0.002930165734\n",
      "training step: 275151, loss:  0.002386306180\n",
      "training step: 275192, loss:  0.003900909098\n",
      "training step: 275233, loss:  0.002396283671\n",
      "training step: 275274, loss:  0.009473215789\n",
      "training step: 275315, loss:  0.007877553813\n",
      "training step: 275356, loss:  0.003011468332\n",
      "training step: 275397, loss:  0.005003615282\n",
      "training step: 275438, loss:  0.003467082279\n",
      "training step: 275479, loss:  0.002975780051\n",
      "training step: 275520, loss:  0.003011468332\n",
      "training step: 275561, loss:  0.003638759954\n",
      "training step: 275602, loss:  0.025363046676\n",
      "training step: 275643, loss:  0.003002761398\n",
      "training step: 275684, loss:  0.007752379868\n",
      "training step: 275725, loss:  0.006285338197\n",
      "training step: 275766, loss:  0.024754593149\n",
      "training step: 275807, loss:  0.003776642960\n",
      "training step: 275848, loss:  0.018698843196\n",
      "training step: 275889, loss:  0.025568028912\n",
      "training step: 275930, loss:  0.003900909098\n",
      "training step: 275971, loss:  0.002396283671\n",
      "training step: 276012, loss:  0.007752379868\n",
      "training step: 276053, loss:  0.007752379868\n",
      "training step: 276094, loss:  0.003880497534\n",
      "training step: 276135, loss:  0.003900909098\n",
      "training step: 276176, loss:  0.003120336682\n",
      "training step: 276217, loss:  0.006285338197\n",
      "training step: 276258, loss:  0.009473215789\n",
      "training step: 276299, loss:  0.003692979924\n",
      "training step: 276340, loss:  0.022392421961\n",
      "training step: 276381, loss:  0.003135976614\n",
      "training step: 276422, loss:  0.005003615282\n",
      "training step: 276463, loss:  0.003002761398\n",
      "training step: 276504, loss:  0.004087368958\n",
      "training step: 276545, loss:  0.024754593149\n",
      "training step: 276586, loss:  0.003002761398\n",
      "training step: 276627, loss:  0.003120336682\n",
      "training step: 276668, loss:  0.004080093466\n",
      "training step: 276709, loss:  0.005485921632\n",
      "training step: 276750, loss:  0.003543942003\n",
      "training step: 276791, loss:  0.003543942003\n",
      "training step: 276832, loss:  0.018698843196\n",
      "training step: 276873, loss:  0.002396283671\n",
      "training step: 276914, loss:  0.007341358811\n",
      "training step: 276955, loss:  0.002975780051\n",
      "training step: 276996, loss:  0.003638759954\n",
      "training step: 277037, loss:  0.003900909098\n",
      "training step: 277078, loss:  0.003011468332\n",
      "training step: 277119, loss:  0.004087368958\n",
      "training step: 277160, loss:  0.003467082279\n",
      "training step: 277201, loss:  0.003135976614\n",
      "training step: 277242, loss:  0.002930165734\n",
      "training step: 277283, loss:  0.016485802829\n",
      "training step: 277324, loss:  0.003880497534\n",
      "training step: 277365, loss:  0.007341358811\n",
      "training step: 277406, loss:  0.003986798227\n",
      "training step: 277447, loss:  0.003866109066\n",
      "training step: 277488, loss:  0.002396283671\n",
      "training step: 277529, loss:  0.003467082279\n",
      "training step: 277570, loss:  0.025568028912\n",
      "training step: 277611, loss:  0.003986798227\n",
      "training step: 277652, loss:  0.007752379868\n",
      "training step: 277693, loss:  0.002386306180\n",
      "training step: 277734, loss:  0.002386306180\n",
      "training step: 277775, loss:  0.006285338197\n",
      "training step: 277816, loss:  0.002396283671\n",
      "training step: 277857, loss:  0.005485921632\n",
      "training step: 277898, loss:  0.005715856329\n",
      "training step: 277939, loss:  0.002975780051\n",
      "training step: 277980, loss:  0.003543942003\n",
      "training step: 278021, loss:  0.007752379868\n",
      "training step: 278062, loss:  0.009473215789\n",
      "training step: 278103, loss:  0.025363046676\n",
      "training step: 278144, loss:  0.003120336682\n",
      "training step: 278185, loss:  0.014145905152\n",
      "training step: 278226, loss:  0.007752379868\n",
      "training step: 278267, loss:  0.005003615282\n",
      "training step: 278308, loss:  0.003011468332\n",
      "training step: 278349, loss:  0.003467082279\n",
      "training step: 278390, loss:  0.005003615282\n",
      "training step: 278431, loss:  0.025568028912\n",
      "training step: 278472, loss:  0.025363046676\n",
      "training step: 278513, loss:  0.003692979924\n",
      "training step: 278554, loss:  0.003002761398\n",
      "training step: 278595, loss:  0.005485921632\n",
      "training step: 278636, loss:  0.002396283671\n",
      "training step: 278677, loss:  0.003543942003\n",
      "training step: 278718, loss:  0.007341358811\n",
      "training step: 278759, loss:  0.006285338197\n",
      "training step: 278800, loss:  0.003543942003\n",
      "training step: 278841, loss:  0.003900909098\n",
      "training step: 278882, loss:  0.003011468332\n",
      "training step: 278923, loss:  0.007752379868\n",
      "training step: 278964, loss:  0.023757282645\n",
      "training step: 279005, loss:  0.003900909098\n",
      "training step: 279046, loss:  0.003776642960\n",
      "training step: 279087, loss:  0.024754593149\n",
      "training step: 279128, loss:  0.002396283671\n",
      "training step: 279169, loss:  0.003002761398\n",
      "training step: 279210, loss:  0.003543942003\n",
      "training step: 279251, loss:  0.002396283671\n",
      "training step: 279292, loss:  0.020691098645\n",
      "training step: 279333, loss:  0.018698843196\n",
      "training step: 279374, loss:  0.007877553813\n",
      "training step: 279415, loss:  0.002975780051\n",
      "training step: 279456, loss:  0.006285338197\n",
      "training step: 279497, loss:  0.005003615282\n",
      "training step: 279538, loss:  0.018698843196\n",
      "training step: 279579, loss:  0.002975780051\n",
      "training step: 279620, loss:  0.024754593149\n",
      "training step: 279661, loss:  0.007877553813\n",
      "training step: 279702, loss:  0.003880497534\n",
      "training step: 279743, loss:  0.009473215789\n",
      "training step: 279784, loss:  0.002975780051\n",
      "training step: 279825, loss:  0.002975780051\n",
      "training step: 279866, loss:  0.016485802829\n",
      "training step: 279907, loss:  0.025568028912\n",
      "training step: 279948, loss:  0.003880497534\n",
      "training step: 279989, loss:  0.003692979924\n",
      "training step: 280030, loss:  0.002930165734\n",
      "training step: 280071, loss:  0.003473303979\n",
      "training step: 280112, loss:  0.006285338197\n",
      "training step: 280153, loss:  0.003692979924\n",
      "training step: 280194, loss:  0.003135976614\n",
      "training step: 280235, loss:  0.020691098645\n",
      "training step: 280276, loss:  0.006285338197\n",
      "training step: 280317, loss:  0.024754593149\n",
      "training step: 280358, loss:  0.006285338197\n",
      "training step: 280399, loss:  0.003638759954\n",
      "training step: 280440, loss:  0.023757282645\n",
      "training step: 280481, loss:  0.022392421961\n",
      "training step: 280522, loss:  0.003900909098\n",
      "training step: 280563, loss:  0.025363046676\n",
      "training step: 280604, loss:  0.003467082279\n",
      "training step: 280645, loss:  0.020691098645\n",
      "training step: 280686, loss:  0.005003615282\n",
      "training step: 280727, loss:  0.018698843196\n",
      "training step: 280768, loss:  0.025363046676\n",
      "training step: 280809, loss:  0.003002761398\n",
      "training step: 280850, loss:  0.003135976614\n",
      "training step: 280891, loss:  0.003011468332\n",
      "training step: 280932, loss:  0.005485921632\n",
      "training step: 280973, loss:  0.003808671376\n",
      "training step: 281014, loss:  0.003135976614\n",
      "training step: 281055, loss:  0.004080093466\n",
      "training step: 281096, loss:  0.006285338197\n",
      "training step: 281137, loss:  0.005485921632\n",
      "training step: 281178, loss:  0.025568028912\n",
      "training step: 281219, loss:  0.009004401974\n",
      "training step: 281260, loss:  0.020691098645\n",
      "training step: 281301, loss:  0.022392421961\n",
      "training step: 281342, loss:  0.020691098645\n",
      "training step: 281383, loss:  0.004087368958\n",
      "training step: 281424, loss:  0.009004401974\n",
      "training step: 281465, loss:  0.003638759954\n",
      "training step: 281506, loss:  0.003002761398\n",
      "training step: 281547, loss:  0.003011468332\n",
      "training step: 281588, loss:  0.004080093466\n",
      "training step: 281629, loss:  0.003880497534\n",
      "training step: 281670, loss:  0.006285338197\n",
      "training step: 281711, loss:  0.002930165734\n",
      "training step: 281752, loss:  0.002975780051\n",
      "training step: 281793, loss:  0.002386306180\n",
      "training step: 281834, loss:  0.003808671376\n",
      "training step: 281875, loss:  0.003900909098\n",
      "training step: 281916, loss:  0.002386306180\n",
      "training step: 281957, loss:  0.006285338197\n",
      "training step: 281998, loss:  0.003002761398\n",
      "training step: 282039, loss:  0.006285338197\n",
      "training step: 282080, loss:  0.003467082279\n",
      "training step: 282121, loss:  0.003638759954\n",
      "training step: 282162, loss:  0.007752379868\n",
      "training step: 282203, loss:  0.018698843196\n",
      "training step: 282244, loss:  0.004087368958\n",
      "training step: 282285, loss:  0.004080093466\n",
      "training step: 282326, loss:  0.002930165734\n",
      "training step: 282367, loss:  0.002930165734\n",
      "training step: 282408, loss:  0.006285338197\n",
      "training step: 282449, loss:  0.003638759954\n",
      "training step: 282490, loss:  0.007341358811\n",
      "training step: 282531, loss:  0.006285338197\n",
      "training step: 282572, loss:  0.003900909098\n",
      "training step: 282613, loss:  0.007752379868\n",
      "training step: 282654, loss:  0.003692979924\n",
      "training step: 282695, loss:  0.009473215789\n",
      "training step: 282736, loss:  0.004087368958\n",
      "training step: 282777, loss:  0.024754593149\n",
      "training step: 282818, loss:  0.024754593149\n",
      "training step: 282859, loss:  0.025363046676\n",
      "training step: 282900, loss:  0.004087368958\n",
      "training step: 282941, loss:  0.004087368958\n",
      "training step: 282982, loss:  0.011776376516\n",
      "training step: 283023, loss:  0.011776376516\n",
      "training step: 283064, loss:  0.003467082279\n",
      "training step: 283105, loss:  0.004087368958\n",
      "training step: 283146, loss:  0.025363046676\n",
      "training step: 283187, loss:  0.003473303979\n",
      "training step: 283228, loss:  0.003002761398\n",
      "training step: 283269, loss:  0.003638759954\n",
      "training step: 283310, loss:  0.003011468332\n",
      "training step: 283351, loss:  0.025568028912\n",
      "training step: 283392, loss:  0.003135976614\n",
      "training step: 283433, loss:  0.025568028912\n",
      "training step: 283474, loss:  0.022392421961\n",
      "training step: 283515, loss:  0.003900909098\n",
      "training step: 283556, loss:  0.009473215789\n",
      "training step: 283597, loss:  0.005003615282\n",
      "training step: 283638, loss:  0.023757282645\n",
      "training step: 283679, loss:  0.005715856329\n",
      "training step: 283720, loss:  0.003866109066\n",
      "training step: 283761, loss:  0.007877553813\n",
      "training step: 283802, loss:  0.003284437582\n",
      "training step: 283843, loss:  0.024754593149\n",
      "training step: 283884, loss:  0.007752379868\n",
      "training step: 283925, loss:  0.003808671376\n",
      "training step: 283966, loss:  0.005003615282\n",
      "training step: 284007, loss:  0.003120336682\n",
      "training step: 284048, loss:  0.003473303979\n",
      "training step: 284089, loss:  0.003011468332\n",
      "training step: 284130, loss:  0.002930165734\n",
      "training step: 284171, loss:  0.004080093466\n",
      "training step: 284212, loss:  0.002975780051\n",
      "training step: 284253, loss:  0.020691098645\n",
      "training step: 284294, loss:  0.003880497534\n",
      "training step: 284335, loss:  0.020691098645\n",
      "training step: 284376, loss:  0.007752379868\n",
      "training step: 284417, loss:  0.004087368958\n",
      "training step: 284458, loss:  0.003135976614\n",
      "training step: 284499, loss:  0.003135976614\n",
      "training step: 284540, loss:  0.003900909098\n",
      "training step: 284581, loss:  0.007752379868\n",
      "training step: 284622, loss:  0.005485921632\n",
      "training step: 284663, loss:  0.025568028912\n",
      "training step: 284704, loss:  0.023757282645\n",
      "training step: 284745, loss:  0.024754593149\n",
      "training step: 284786, loss:  0.007877553813\n",
      "training step: 284827, loss:  0.007752379868\n",
      "training step: 284868, loss:  0.003638759954\n",
      "training step: 284909, loss:  0.016485802829\n",
      "training step: 284950, loss:  0.005485921632\n",
      "training step: 284991, loss:  0.018698843196\n",
      "training step: 285032, loss:  0.018698843196\n",
      "training step: 285073, loss:  0.003880497534\n",
      "training step: 285114, loss:  0.003135976614\n",
      "training step: 285155, loss:  0.003002761398\n",
      "training step: 285196, loss:  0.009473215789\n",
      "training step: 285237, loss:  0.003986798227\n",
      "training step: 285278, loss:  0.005003615282\n",
      "training step: 285319, loss:  0.005003615282\n",
      "training step: 285360, loss:  0.003135976614\n",
      "training step: 285401, loss:  0.003135976614\n",
      "training step: 285442, loss:  0.022392421961\n",
      "training step: 285483, loss:  0.006285338197\n",
      "training step: 285524, loss:  0.011776376516\n",
      "training step: 285565, loss:  0.005003615282\n",
      "training step: 285606, loss:  0.025568028912\n",
      "training step: 285647, loss:  0.009004401974\n",
      "training step: 285688, loss:  0.002396283671\n",
      "training step: 285729, loss:  0.025363046676\n",
      "training step: 285770, loss:  0.003135976614\n",
      "training step: 285811, loss:  0.022392421961\n",
      "training step: 285852, loss:  0.003467082279\n",
      "training step: 285893, loss:  0.002975780051\n",
      "training step: 285934, loss:  0.003692979924\n",
      "training step: 285975, loss:  0.003467082279\n",
      "training step: 286016, loss:  0.004080093466\n",
      "training step: 286057, loss:  0.003473303979\n",
      "training step: 286098, loss:  0.022392421961\n",
      "training step: 286139, loss:  0.002975780051\n",
      "training step: 286180, loss:  0.014145905152\n",
      "training step: 286221, loss:  0.003284437582\n",
      "training step: 286262, loss:  0.003986798227\n",
      "training step: 286303, loss:  0.003776642960\n",
      "training step: 286344, loss:  0.014145905152\n",
      "training step: 286385, loss:  0.003002761398\n",
      "training step: 286426, loss:  0.003120336682\n",
      "training step: 286467, loss:  0.002396283671\n",
      "training step: 286508, loss:  0.009004401974\n",
      "training step: 286549, loss:  0.020691098645\n",
      "training step: 286590, loss:  0.003638759954\n",
      "training step: 286631, loss:  0.003808671376\n",
      "training step: 286672, loss:  0.002930165734\n",
      "training step: 286713, loss:  0.023757282645\n",
      "training step: 286754, loss:  0.005715856329\n",
      "training step: 286795, loss:  0.004087368958\n",
      "training step: 286836, loss:  0.009004401974\n",
      "training step: 286877, loss:  0.003900909098\n",
      "training step: 286918, loss:  0.002975780051\n",
      "training step: 286959, loss:  0.003692979924\n",
      "training step: 287000, loss:  0.003986798227\n",
      "training step: 287041, loss:  0.003880497534\n",
      "training step: 287082, loss:  0.005003615282\n",
      "training step: 287123, loss:  0.024754593149\n",
      "training step: 287164, loss:  0.005003615282\n",
      "training step: 287205, loss:  0.004087368958\n",
      "training step: 287246, loss:  0.003467082279\n",
      "training step: 287287, loss:  0.025568028912\n",
      "training step: 287328, loss:  0.003776642960\n",
      "training step: 287369, loss:  0.002930165734\n",
      "training step: 287410, loss:  0.003638759954\n",
      "training step: 287451, loss:  0.023757282645\n",
      "training step: 287492, loss:  0.006285338197\n",
      "training step: 287533, loss:  0.014145905152\n",
      "training step: 287574, loss:  0.003120336682\n",
      "training step: 287615, loss:  0.003284437582\n",
      "training step: 287656, loss:  0.005485921632\n",
      "training step: 287697, loss:  0.003011468332\n",
      "training step: 287738, loss:  0.020691098645\n",
      "training step: 287779, loss:  0.004087368958\n",
      "training step: 287820, loss:  0.024754593149\n",
      "training step: 287861, loss:  0.007877553813\n",
      "training step: 287902, loss:  0.018698843196\n",
      "training step: 287943, loss:  0.006285338197\n",
      "training step: 287984, loss:  0.006285338197\n",
      "training step: 288025, loss:  0.003473303979\n",
      "training step: 288066, loss:  0.014145905152\n",
      "training step: 288107, loss:  0.003986798227\n",
      "training step: 288148, loss:  0.003900909098\n",
      "training step: 288189, loss:  0.003135976614\n",
      "training step: 288230, loss:  0.025363046676\n",
      "training step: 288271, loss:  0.018698843196\n",
      "training step: 288312, loss:  0.005003615282\n",
      "training step: 288353, loss:  0.002396283671\n",
      "training step: 288394, loss:  0.002386306180\n",
      "training step: 288435, loss:  0.003120336682\n",
      "training step: 288476, loss:  0.002386306180\n",
      "training step: 288517, loss:  0.003284437582\n",
      "training step: 288558, loss:  0.003880497534\n",
      "training step: 288599, loss:  0.003692979924\n",
      "training step: 288640, loss:  0.003866109066\n",
      "training step: 288681, loss:  0.003866109066\n",
      "training step: 288722, loss:  0.014145905152\n",
      "training step: 288763, loss:  0.023757282645\n",
      "training step: 288804, loss:  0.004080093466\n",
      "training step: 288845, loss:  0.003866109066\n",
      "training step: 288886, loss:  0.009473215789\n",
      "training step: 288927, loss:  0.003284437582\n",
      "training step: 288968, loss:  0.004087368958\n",
      "training step: 289009, loss:  0.018698843196\n",
      "training step: 289050, loss:  0.020691098645\n",
      "training step: 289091, loss:  0.009004401974\n",
      "training step: 289132, loss:  0.006285338197\n",
      "training step: 289173, loss:  0.025568028912\n",
      "training step: 289214, loss:  0.002930165734\n",
      "training step: 289255, loss:  0.005485921632\n",
      "training step: 289296, loss:  0.002386306180\n",
      "training step: 289337, loss:  0.020691098645\n",
      "training step: 289378, loss:  0.002930165734\n",
      "training step: 289419, loss:  0.005715856329\n",
      "training step: 289460, loss:  0.002930165734\n",
      "training step: 289501, loss:  0.003467082279\n",
      "training step: 289542, loss:  0.007341358811\n",
      "training step: 289583, loss:  0.003866109066\n",
      "training step: 289624, loss:  0.016485802829\n",
      "training step: 289665, loss:  0.007877553813\n",
      "training step: 289706, loss:  0.003866109066\n",
      "training step: 289747, loss:  0.009473215789\n",
      "training step: 289788, loss:  0.003543942003\n",
      "training step: 289829, loss:  0.009004401974\n",
      "training step: 289870, loss:  0.004087368958\n",
      "training step: 289911, loss:  0.003808671376\n",
      "training step: 289952, loss:  0.024754593149\n",
      "training step: 289993, loss:  0.009004401974\n",
      "training step: 290034, loss:  0.004080093466\n",
      "training step: 290075, loss:  0.003120336682\n",
      "training step: 290116, loss:  0.003543942003\n",
      "training step: 290157, loss:  0.024754593149\n",
      "training step: 290198, loss:  0.003692979924\n",
      "training step: 290239, loss:  0.002975780051\n",
      "training step: 290280, loss:  0.016485802829\n",
      "training step: 290321, loss:  0.025568028912\n",
      "training step: 290362, loss:  0.003467082279\n",
      "training step: 290403, loss:  0.003135976614\n",
      "training step: 290444, loss:  0.003135976614\n",
      "training step: 290485, loss:  0.003135976614\n",
      "training step: 290526, loss:  0.003135976614\n",
      "training step: 290567, loss:  0.003011468332\n",
      "training step: 290608, loss:  0.005715856329\n",
      "training step: 290649, loss:  0.007877553813\n",
      "training step: 290690, loss:  0.003776642960\n",
      "training step: 290731, loss:  0.003135976614\n",
      "training step: 290772, loss:  0.003002761398\n",
      "training step: 290813, loss:  0.003120336682\n",
      "training step: 290854, loss:  0.003880497534\n",
      "training step: 290895, loss:  0.003120336682\n",
      "training step: 290936, loss:  0.003900909098\n",
      "training step: 290977, loss:  0.003638759954\n",
      "training step: 291018, loss:  0.022392421961\n",
      "training step: 291059, loss:  0.003808671376\n",
      "training step: 291100, loss:  0.005003615282\n",
      "training step: 291141, loss:  0.003880497534\n",
      "training step: 291182, loss:  0.007341358811\n",
      "training step: 291223, loss:  0.007752379868\n",
      "training step: 291264, loss:  0.024754593149\n",
      "training step: 291305, loss:  0.003776642960\n",
      "training step: 291346, loss:  0.005003615282\n",
      "training step: 291387, loss:  0.009473215789\n",
      "training step: 291428, loss:  0.011776376516\n",
      "training step: 291469, loss:  0.003473303979\n",
      "training step: 291510, loss:  0.020691098645\n",
      "training step: 291551, loss:  0.018698843196\n",
      "training step: 291592, loss:  0.003467082279\n",
      "training step: 291633, loss:  0.003473303979\n",
      "training step: 291674, loss:  0.025568028912\n",
      "training step: 291715, loss:  0.020691098645\n",
      "training step: 291756, loss:  0.018698843196\n",
      "training step: 291797, loss:  0.003776642960\n",
      "training step: 291838, loss:  0.020691098645\n",
      "training step: 291879, loss:  0.003543942003\n",
      "training step: 291920, loss:  0.014145905152\n",
      "training step: 291961, loss:  0.004080093466\n",
      "training step: 292002, loss:  0.024754593149\n",
      "training step: 292043, loss:  0.009004401974\n",
      "training step: 292084, loss:  0.002396283671\n",
      "training step: 292125, loss:  0.004087368958\n",
      "training step: 292166, loss:  0.003986798227\n",
      "training step: 292207, loss:  0.007341358811\n",
      "training step: 292248, loss:  0.003284437582\n",
      "training step: 292289, loss:  0.003467082279\n",
      "training step: 292330, loss:  0.025568028912\n",
      "training step: 292371, loss:  0.002386306180\n",
      "training step: 292412, loss:  0.003120336682\n",
      "training step: 292453, loss:  0.016485802829\n",
      "training step: 292494, loss:  0.003284437582\n",
      "training step: 292535, loss:  0.007877553813\n",
      "training step: 292576, loss:  0.018698843196\n",
      "training step: 292617, loss:  0.006285338197\n",
      "training step: 292658, loss:  0.002396283671\n",
      "training step: 292699, loss:  0.025568028912\n",
      "training step: 292740, loss:  0.022392421961\n",
      "training step: 292781, loss:  0.003776642960\n",
      "training step: 292822, loss:  0.002930165734\n",
      "training step: 292863, loss:  0.003120336682\n",
      "training step: 292904, loss:  0.003467082279\n",
      "training step: 292945, loss:  0.011776376516\n",
      "training step: 292986, loss:  0.003467082279\n",
      "training step: 293027, loss:  0.003543942003\n",
      "training step: 293068, loss:  0.007877553813\n",
      "training step: 293109, loss:  0.002386306180\n",
      "training step: 293150, loss:  0.009473215789\n",
      "training step: 293191, loss:  0.002386306180\n",
      "training step: 293232, loss:  0.002396283671\n",
      "training step: 293273, loss:  0.003473303979\n",
      "training step: 293314, loss:  0.025568028912\n",
      "training step: 293355, loss:  0.003011468332\n",
      "training step: 293396, loss:  0.007341358811\n",
      "training step: 293437, loss:  0.006285338197\n",
      "training step: 293478, loss:  0.003473303979\n",
      "training step: 293519, loss:  0.002386306180\n",
      "training step: 293560, loss:  0.003986798227\n",
      "training step: 293601, loss:  0.002396283671\n",
      "training step: 293642, loss:  0.024754593149\n",
      "training step: 293683, loss:  0.009004401974\n",
      "training step: 293724, loss:  0.020691098645\n",
      "training step: 293765, loss:  0.003467082279\n",
      "training step: 293806, loss:  0.009473215789\n",
      "training step: 293847, loss:  0.009473215789\n",
      "training step: 293888, loss:  0.003776642960\n",
      "training step: 293929, loss:  0.020691098645\n",
      "training step: 293970, loss:  0.003135976614\n",
      "training step: 294011, loss:  0.003638759954\n",
      "training step: 294052, loss:  0.005485921632\n",
      "training step: 294093, loss:  0.009004401974\n",
      "training step: 294134, loss:  0.003692979924\n",
      "training step: 294175, loss:  0.025568028912\n",
      "training step: 294216, loss:  0.016485802829\n",
      "training step: 294257, loss:  0.003776642960\n",
      "training step: 294298, loss:  0.007877553813\n",
      "training step: 294339, loss:  0.011776376516\n",
      "training step: 294380, loss:  0.003866109066\n",
      "training step: 294421, loss:  0.006285338197\n",
      "training step: 294462, loss:  0.003638759954\n",
      "training step: 294503, loss:  0.003473303979\n",
      "training step: 294544, loss:  0.016485802829\n",
      "training step: 294585, loss:  0.003011468332\n",
      "training step: 294626, loss:  0.003284437582\n",
      "training step: 294667, loss:  0.018698843196\n",
      "training step: 294708, loss:  0.016485802829\n",
      "training step: 294749, loss:  0.003135976614\n",
      "training step: 294790, loss:  0.002930165734\n",
      "training step: 294831, loss:  0.003473303979\n",
      "training step: 294872, loss:  0.003467082279\n",
      "training step: 294913, loss:  0.003808671376\n",
      "training step: 294954, loss:  0.003543942003\n",
      "training step: 294995, loss:  0.004087368958\n",
      "training step: 295036, loss:  0.002975780051\n",
      "training step: 295077, loss:  0.003880497534\n",
      "training step: 295118, loss:  0.002930165734\n",
      "training step: 295159, loss:  0.022392421961\n",
      "training step: 295200, loss:  0.002396283671\n",
      "training step: 295241, loss:  0.004080093466\n",
      "training step: 295282, loss:  0.003011468332\n",
      "training step: 295323, loss:  0.009473215789\n",
      "training step: 295364, loss:  0.004087368958\n",
      "training step: 295405, loss:  0.007341358811\n",
      "training step: 295446, loss:  0.005003615282\n",
      "training step: 295487, loss:  0.003543942003\n",
      "training step: 295528, loss:  0.003880497534\n",
      "training step: 295569, loss:  0.002930165734\n",
      "training step: 295610, loss:  0.003467082279\n",
      "training step: 295651, loss:  0.018698843196\n",
      "training step: 295692, loss:  0.003900909098\n",
      "training step: 295733, loss:  0.023757282645\n",
      "training step: 295774, loss:  0.023757282645\n",
      "training step: 295815, loss:  0.002396283671\n",
      "training step: 295856, loss:  0.018698843196\n",
      "training step: 295897, loss:  0.018698843196\n",
      "training step: 295938, loss:  0.025568028912\n",
      "training step: 295979, loss:  0.007341358811\n",
      "training step: 296020, loss:  0.003776642960\n",
      "training step: 296061, loss:  0.020691098645\n",
      "training step: 296102, loss:  0.005485921632\n",
      "training step: 296143, loss:  0.003120336682\n",
      "training step: 296184, loss:  0.005003615282\n",
      "training step: 296225, loss:  0.003808671376\n",
      "training step: 296266, loss:  0.025568028912\n",
      "training step: 296307, loss:  0.003880497534\n",
      "training step: 296348, loss:  0.004080093466\n",
      "training step: 296389, loss:  0.007752379868\n",
      "training step: 296430, loss:  0.009004401974\n",
      "training step: 296471, loss:  0.002396283671\n",
      "training step: 296512, loss:  0.003473303979\n",
      "training step: 296553, loss:  0.018698843196\n",
      "training step: 296594, loss:  0.003002761398\n",
      "training step: 296635, loss:  0.004087368958\n",
      "training step: 296676, loss:  0.003467082279\n",
      "training step: 296717, loss:  0.002396283671\n",
      "training step: 296758, loss:  0.005715856329\n",
      "training step: 296799, loss:  0.002386306180\n",
      "training step: 296840, loss:  0.003638759954\n",
      "training step: 296881, loss:  0.009004401974\n",
      "training step: 296922, loss:  0.002930165734\n",
      "training step: 296963, loss:  0.003002761398\n",
      "training step: 297004, loss:  0.003284437582\n",
      "training step: 297045, loss:  0.018698843196\n",
      "training step: 297086, loss:  0.002975780051\n",
      "training step: 297127, loss:  0.002386306180\n",
      "training step: 297168, loss:  0.020691098645\n",
      "training step: 297209, loss:  0.020691098645\n",
      "training step: 297250, loss:  0.011776376516\n",
      "training step: 297291, loss:  0.003011468332\n",
      "training step: 297332, loss:  0.024754593149\n",
      "training step: 297373, loss:  0.003011468332\n",
      "training step: 297414, loss:  0.005715856329\n",
      "training step: 297455, loss:  0.003135976614\n",
      "training step: 297496, loss:  0.002975780051\n",
      "training step: 297537, loss:  0.023757282645\n",
      "training step: 297578, loss:  0.004087368958\n",
      "training step: 297619, loss:  0.018698843196\n",
      "training step: 297660, loss:  0.025568028912\n",
      "training step: 297701, loss:  0.003284437582\n",
      "training step: 297742, loss:  0.006285338197\n",
      "training step: 297783, loss:  0.003467082279\n",
      "training step: 297824, loss:  0.002386306180\n",
      "training step: 297865, loss:  0.024754593149\n",
      "training step: 297906, loss:  0.003880497534\n",
      "training step: 297947, loss:  0.003776642960\n",
      "training step: 297988, loss:  0.007877553813\n",
      "training step: 298029, loss:  0.007752379868\n",
      "training step: 298070, loss:  0.003284437582\n",
      "training step: 298111, loss:  0.004080093466\n",
      "training step: 298152, loss:  0.023757282645\n",
      "training step: 298193, loss:  0.006285338197\n",
      "training step: 298234, loss:  0.005485921632\n",
      "training step: 298275, loss:  0.003011468332\n",
      "training step: 298316, loss:  0.025568028912\n",
      "training step: 298357, loss:  0.005715856329\n",
      "training step: 298398, loss:  0.025363046676\n",
      "training step: 298439, loss:  0.006285338197\n",
      "training step: 298480, loss:  0.003692979924\n",
      "training step: 298521, loss:  0.003135976614\n",
      "training step: 298562, loss:  0.024754593149\n",
      "training step: 298603, loss:  0.024754593149\n",
      "training step: 298644, loss:  0.003473303979\n",
      "training step: 298685, loss:  0.003284437582\n",
      "training step: 298726, loss:  0.003692979924\n",
      "training step: 298767, loss:  0.006285338197\n",
      "training step: 298808, loss:  0.003011468332\n",
      "training step: 298849, loss:  0.018698843196\n",
      "training step: 298890, loss:  0.007877553813\n",
      "training step: 298931, loss:  0.003467082279\n",
      "training step: 298972, loss:  0.004087368958\n",
      "training step: 299013, loss:  0.020691098645\n",
      "training step: 299054, loss:  0.004080093466\n",
      "training step: 299095, loss:  0.003467082279\n",
      "training step: 299136, loss:  0.003808671376\n",
      "training step: 299177, loss:  0.003284437582\n",
      "training step: 299218, loss:  0.014145905152\n",
      "training step: 299259, loss:  0.003467082279\n",
      "training step: 299300, loss:  0.003986798227\n",
      "training step: 299341, loss:  0.002396283671\n",
      "training step: 299382, loss:  0.005003615282\n",
      "training step: 299423, loss:  0.016485802829\n",
      "training step: 299464, loss:  0.003002761398\n",
      "training step: 299505, loss:  0.023757282645\n",
      "training step: 299546, loss:  0.009473215789\n",
      "training step: 299587, loss:  0.007341358811\n",
      "training step: 299628, loss:  0.005715856329\n",
      "training step: 299669, loss:  0.025363046676\n",
      "training step: 299710, loss:  0.006285338197\n",
      "training step: 299751, loss:  0.003880497534\n",
      "training step: 299792, loss:  0.005715856329\n",
      "training step: 299833, loss:  0.004087368958\n",
      "training step: 299874, loss:  0.004087368958\n",
      "training step: 299915, loss:  0.009473215789\n",
      "training step: 299956, loss:  0.011776376516\n",
      "training step: 299997, loss:  0.003638759954\n",
      "training step: 300038, loss:  0.003135976614\n",
      "training step: 300079, loss:  0.025363046676\n",
      "training step: 300120, loss:  0.007877553813\n",
      "training step: 300161, loss:  0.003808671376\n",
      "training step: 300202, loss:  0.014145905152\n",
      "training step: 300243, loss:  0.004087368958\n",
      "training step: 300284, loss:  0.006285338197\n",
      "training step: 300325, loss:  0.003638759954\n",
      "training step: 300366, loss:  0.003866109066\n",
      "training step: 300407, loss:  0.003284437582\n",
      "training step: 300448, loss:  0.003011468332\n",
      "training step: 300489, loss:  0.018698843196\n",
      "training step: 300530, loss:  0.003002761398\n",
      "training step: 300571, loss:  0.005485921632\n",
      "training step: 300612, loss:  0.009004401974\n",
      "training step: 300653, loss:  0.003135976614\n",
      "training step: 300694, loss:  0.006285338197\n",
      "training step: 300735, loss:  0.025568028912\n",
      "training step: 300776, loss:  0.003135976614\n",
      "training step: 300817, loss:  0.002975780051\n",
      "training step: 300858, loss:  0.007752379868\n",
      "training step: 300899, loss:  0.007341358811\n",
      "training step: 300940, loss:  0.003900909098\n",
      "training step: 300981, loss:  0.003880497534\n",
      "training step: 301022, loss:  0.003986798227\n",
      "training step: 301063, loss:  0.004080093466\n",
      "training step: 301104, loss:  0.023757282645\n",
      "training step: 301145, loss:  0.003135976614\n",
      "training step: 301186, loss:  0.002975780051\n",
      "training step: 301227, loss:  0.005003615282\n",
      "training step: 301268, loss:  0.022392421961\n",
      "training step: 301309, loss:  0.003011468332\n",
      "training step: 301350, loss:  0.004087368958\n",
      "training step: 301391, loss:  0.003880497534\n",
      "training step: 301432, loss:  0.014145905152\n",
      "training step: 301473, loss:  0.002975780051\n",
      "training step: 301514, loss:  0.003002761398\n",
      "training step: 301555, loss:  0.014145905152\n",
      "training step: 301596, loss:  0.018698843196\n",
      "training step: 301637, loss:  0.007752379868\n",
      "training step: 301678, loss:  0.018698843196\n",
      "training step: 301719, loss:  0.004080093466\n",
      "training step: 301760, loss:  0.018698843196\n",
      "training step: 301801, loss:  0.002930165734\n",
      "training step: 301842, loss:  0.002396283671\n",
      "training step: 301883, loss:  0.003808671376\n",
      "training step: 301924, loss:  0.025363046676\n",
      "training step: 301965, loss:  0.004080093466\n",
      "training step: 302006, loss:  0.006285338197\n",
      "training step: 302047, loss:  0.006285338197\n",
      "training step: 302088, loss:  0.020691098645\n",
      "training step: 302129, loss:  0.003120336682\n",
      "training step: 302170, loss:  0.022392421961\n",
      "training step: 302211, loss:  0.003692979924\n",
      "training step: 302252, loss:  0.003900909098\n",
      "training step: 302293, loss:  0.005715856329\n",
      "training step: 302334, loss:  0.003002761398\n",
      "training step: 302375, loss:  0.007877553813\n",
      "training step: 302416, loss:  0.002930165734\n",
      "training step: 302457, loss:  0.002930165734\n",
      "training step: 302498, loss:  0.005485921632\n",
      "training step: 302539, loss:  0.016485802829\n",
      "training step: 302580, loss:  0.005715856329\n",
      "training step: 302621, loss:  0.002975780051\n",
      "training step: 302662, loss:  0.003543942003\n",
      "training step: 302703, loss:  0.003900909098\n",
      "training step: 302744, loss:  0.002396283671\n",
      "training step: 302785, loss:  0.004087368958\n",
      "training step: 302826, loss:  0.005485921632\n",
      "training step: 302867, loss:  0.016485802829\n",
      "training step: 302908, loss:  0.002930165734\n",
      "training step: 302949, loss:  0.002975780051\n",
      "training step: 302990, loss:  0.003808671376\n",
      "training step: 303031, loss:  0.005003615282\n",
      "training step: 303072, loss:  0.025568028912\n",
      "training step: 303113, loss:  0.005485921632\n",
      "training step: 303154, loss:  0.024754593149\n",
      "training step: 303195, loss:  0.004087368958\n",
      "training step: 303236, loss:  0.023757282645\n",
      "training step: 303277, loss:  0.006285338197\n",
      "training step: 303318, loss:  0.002930165734\n",
      "training step: 303359, loss:  0.007341358811\n",
      "training step: 303400, loss:  0.004087368958\n",
      "training step: 303441, loss:  0.009004401974\n",
      "training step: 303482, loss:  0.003543942003\n",
      "training step: 303523, loss:  0.024754593149\n",
      "training step: 303564, loss:  0.004087368958\n",
      "training step: 303605, loss:  0.006285338197\n",
      "training step: 303646, loss:  0.002386306180\n",
      "training step: 303687, loss:  0.003467082279\n",
      "training step: 303728, loss:  0.003011468332\n",
      "training step: 303769, loss:  0.005485921632\n",
      "training step: 303810, loss:  0.003866109066\n",
      "training step: 303851, loss:  0.009473215789\n",
      "training step: 303892, loss:  0.003467082279\n",
      "training step: 303933, loss:  0.002386306180\n",
      "training step: 303974, loss:  0.003135976614\n",
      "training step: 304015, loss:  0.003808671376\n",
      "training step: 304056, loss:  0.007752379868\n",
      "training step: 304097, loss:  0.016485802829\n",
      "training step: 304138, loss:  0.003135976614\n",
      "training step: 304179, loss:  0.004080093466\n",
      "training step: 304220, loss:  0.003866109066\n",
      "training step: 304261, loss:  0.002386306180\n",
      "training step: 304302, loss:  0.003880497534\n",
      "training step: 304343, loss:  0.005485921632\n",
      "training step: 304384, loss:  0.023757282645\n",
      "training step: 304425, loss:  0.003808671376\n",
      "training step: 304466, loss:  0.025363046676\n",
      "training step: 304507, loss:  0.005715856329\n",
      "training step: 304548, loss:  0.003692979924\n",
      "training step: 304589, loss:  0.003135976614\n",
      "training step: 304630, loss:  0.002396283671\n",
      "training step: 304671, loss:  0.003543942003\n",
      "training step: 304712, loss:  0.014145905152\n",
      "training step: 304753, loss:  0.016485802829\n",
      "training step: 304794, loss:  0.009004401974\n",
      "training step: 304835, loss:  0.004087368958\n",
      "training step: 304876, loss:  0.007341358811\n",
      "training step: 304917, loss:  0.024754593149\n",
      "training step: 304958, loss:  0.004080093466\n",
      "training step: 304999, loss:  0.009004401974\n",
      "training step: 305040, loss:  0.009473215789\n",
      "training step: 305081, loss:  0.022392421961\n",
      "training step: 305122, loss:  0.003473303979\n",
      "training step: 305163, loss:  0.002386306180\n",
      "training step: 305204, loss:  0.005715856329\n",
      "training step: 305245, loss:  0.003638759954\n",
      "training step: 305286, loss:  0.009473215789\n",
      "training step: 305327, loss:  0.003011468332\n",
      "training step: 305368, loss:  0.003467082279\n",
      "training step: 305409, loss:  0.025363046676\n",
      "training step: 305450, loss:  0.020691098645\n",
      "training step: 305491, loss:  0.003900909098\n",
      "training step: 305532, loss:  0.004080093466\n",
      "training step: 305573, loss:  0.025363046676\n",
      "training step: 305614, loss:  0.009004401974\n",
      "training step: 305655, loss:  0.005003615282\n",
      "training step: 305696, loss:  0.003638759954\n",
      "training step: 305737, loss:  0.003638759954\n",
      "training step: 305778, loss:  0.003002761398\n",
      "training step: 305819, loss:  0.009004401974\n",
      "training step: 305860, loss:  0.003638759954\n",
      "training step: 305901, loss:  0.002396283671\n",
      "training step: 305942, loss:  0.007341358811\n",
      "training step: 305983, loss:  0.005003615282\n",
      "training step: 306024, loss:  0.020691098645\n",
      "training step: 306065, loss:  0.005715856329\n",
      "training step: 306106, loss:  0.003473303979\n",
      "training step: 306147, loss:  0.003638759954\n",
      "training step: 306188, loss:  0.025363046676\n",
      "training step: 306229, loss:  0.003692979924\n",
      "training step: 306270, loss:  0.004080093466\n",
      "training step: 306311, loss:  0.011776376516\n",
      "training step: 306352, loss:  0.003692979924\n",
      "training step: 306393, loss:  0.003880497534\n",
      "training step: 306434, loss:  0.022392421961\n",
      "training step: 306475, loss:  0.009004401974\n",
      "training step: 306516, loss:  0.023757282645\n",
      "training step: 306557, loss:  0.003473303979\n",
      "training step: 306598, loss:  0.007341358811\n",
      "training step: 306639, loss:  0.003986798227\n",
      "training step: 306680, loss:  0.018698843196\n",
      "training step: 306721, loss:  0.004080093466\n",
      "training step: 306762, loss:  0.002975780051\n",
      "training step: 306803, loss:  0.003866109066\n",
      "training step: 306844, loss:  0.003467082279\n",
      "training step: 306885, loss:  0.007877553813\n",
      "training step: 306926, loss:  0.003473303979\n",
      "training step: 306967, loss:  0.003543942003\n",
      "training step: 307008, loss:  0.020691098645\n",
      "training step: 307049, loss:  0.003135976614\n",
      "training step: 307090, loss:  0.016485802829\n",
      "training step: 307131, loss:  0.003002761398\n",
      "training step: 307172, loss:  0.003135976614\n",
      "training step: 307213, loss:  0.005715856329\n",
      "training step: 307254, loss:  0.014145905152\n",
      "training step: 307295, loss:  0.003880497534\n",
      "training step: 307336, loss:  0.003467082279\n",
      "training step: 307377, loss:  0.006285338197\n",
      "training step: 307418, loss:  0.003473303979\n",
      "training step: 307459, loss:  0.004080093466\n",
      "training step: 307500, loss:  0.022392421961\n",
      "training step: 307541, loss:  0.002975780051\n",
      "training step: 307582, loss:  0.003473303979\n",
      "training step: 307623, loss:  0.003638759954\n",
      "training step: 307664, loss:  0.003880497534\n",
      "training step: 307705, loss:  0.003002761398\n",
      "training step: 307746, loss:  0.003986798227\n",
      "training step: 307787, loss:  0.003284437582\n",
      "training step: 307828, loss:  0.003284437582\n",
      "training step: 307869, loss:  0.003880497534\n",
      "training step: 307910, loss:  0.003011468332\n",
      "training step: 307951, loss:  0.002386306180\n",
      "training step: 307992, loss:  0.006285338197\n",
      "training step: 308033, loss:  0.003120336682\n",
      "training step: 308074, loss:  0.025568028912\n",
      "training step: 308115, loss:  0.002930165734\n",
      "training step: 308156, loss:  0.003808671376\n",
      "training step: 308197, loss:  0.003692979924\n",
      "training step: 308238, loss:  0.003986798227\n",
      "training step: 308279, loss:  0.003900909098\n",
      "training step: 308320, loss:  0.003467082279\n",
      "training step: 308361, loss:  0.004087368958\n",
      "training step: 308402, loss:  0.003543942003\n",
      "training step: 308443, loss:  0.003900909098\n",
      "training step: 308484, loss:  0.003543942003\n",
      "training step: 308525, loss:  0.003880497534\n",
      "training step: 308566, loss:  0.002396283671\n",
      "training step: 308607, loss:  0.003543942003\n",
      "training step: 308648, loss:  0.005715856329\n",
      "training step: 308689, loss:  0.003986798227\n",
      "training step: 308730, loss:  0.004080093466\n",
      "training step: 308771, loss:  0.005715856329\n",
      "training step: 308812, loss:  0.003880497534\n",
      "training step: 308853, loss:  0.002396283671\n",
      "training step: 308894, loss:  0.002975780051\n",
      "training step: 308935, loss:  0.005003615282\n",
      "training step: 308976, loss:  0.002930165734\n",
      "training step: 309017, loss:  0.024754593149\n",
      "training step: 309058, loss:  0.009004401974\n",
      "training step: 309099, loss:  0.002386306180\n",
      "training step: 309140, loss:  0.003473303979\n",
      "training step: 309181, loss:  0.003543942003\n",
      "training step: 309222, loss:  0.007752379868\n",
      "training step: 309263, loss:  0.011776376516\n",
      "training step: 309304, loss:  0.003002761398\n",
      "training step: 309345, loss:  0.003284437582\n",
      "training step: 309386, loss:  0.002396283671\n",
      "training step: 309427, loss:  0.003473303979\n",
      "training step: 309468, loss:  0.003284437582\n",
      "training step: 309509, loss:  0.003692979924\n",
      "training step: 309550, loss:  0.023757282645\n",
      "training step: 309591, loss:  0.022392421961\n",
      "training step: 309632, loss:  0.003776642960\n",
      "training step: 309673, loss:  0.003467082279\n",
      "training step: 309714, loss:  0.004087368958\n",
      "training step: 309755, loss:  0.005715856329\n",
      "training step: 309796, loss:  0.002396283671\n",
      "training step: 309837, loss:  0.018698843196\n",
      "training step: 309878, loss:  0.004080093466\n",
      "training step: 309919, loss:  0.011776376516\n",
      "training step: 309960, loss:  0.003002761398\n",
      "training step: 310001, loss:  0.003120336682\n",
      "training step: 310042, loss:  0.003135976614\n",
      "training step: 310083, loss:  0.004087368958\n",
      "training step: 310124, loss:  0.003808671376\n",
      "training step: 310165, loss:  0.020691098645\n",
      "training step: 310206, loss:  0.003543942003\n",
      "training step: 310247, loss:  0.020691098645\n",
      "training step: 310288, loss:  0.007877553813\n",
      "training step: 310329, loss:  0.014145905152\n",
      "training step: 310370, loss:  0.018698843196\n",
      "training step: 310411, loss:  0.003120336682\n",
      "training step: 310452, loss:  0.025363046676\n",
      "training step: 310493, loss:  0.024754593149\n",
      "training step: 310534, loss:  0.007341358811\n",
      "training step: 310575, loss:  0.003692979924\n",
      "training step: 310616, loss:  0.003467082279\n",
      "training step: 310657, loss:  0.007341358811\n",
      "training step: 310698, loss:  0.018698843196\n",
      "training step: 310739, loss:  0.003473303979\n",
      "training step: 310780, loss:  0.002930165734\n",
      "training step: 310821, loss:  0.003120336682\n",
      "training step: 310862, loss:  0.003002761398\n",
      "training step: 310903, loss:  0.016485802829\n",
      "training step: 310944, loss:  0.003880497534\n",
      "training step: 310985, loss:  0.007341358811\n",
      "training step: 311026, loss:  0.003986798227\n",
      "training step: 311067, loss:  0.005485921632\n",
      "training step: 311108, loss:  0.011776376516\n",
      "training step: 311149, loss:  0.005715856329\n",
      "training step: 311190, loss:  0.006285338197\n",
      "training step: 311231, loss:  0.004087368958\n",
      "training step: 311272, loss:  0.020691098645\n",
      "training step: 311313, loss:  0.002975780051\n",
      "training step: 311354, loss:  0.003135976614\n",
      "training step: 311395, loss:  0.003011468332\n",
      "training step: 311436, loss:  0.003638759954\n",
      "training step: 311477, loss:  0.006285338197\n",
      "training step: 311518, loss:  0.003692979924\n",
      "training step: 311559, loss:  0.003002761398\n",
      "training step: 311600, loss:  0.003011468332\n",
      "training step: 311641, loss:  0.005485921632\n",
      "training step: 311682, loss:  0.020691098645\n",
      "training step: 311723, loss:  0.003866109066\n",
      "training step: 311764, loss:  0.003900909098\n",
      "training step: 311805, loss:  0.003776642960\n",
      "training step: 311846, loss:  0.006285338197\n",
      "training step: 311887, loss:  0.007341358811\n",
      "training step: 311928, loss:  0.003900909098\n",
      "training step: 311969, loss:  0.003543942003\n",
      "training step: 312010, loss:  0.007877553813\n",
      "training step: 312051, loss:  0.003900909098\n",
      "training step: 312092, loss:  0.025568028912\n",
      "training step: 312133, loss:  0.007877553813\n",
      "training step: 312174, loss:  0.009004401974\n",
      "training step: 312215, loss:  0.002975780051\n",
      "training step: 312256, loss:  0.007341358811\n",
      "training step: 312297, loss:  0.007877553813\n",
      "training step: 312338, loss:  0.002930165734\n",
      "training step: 312379, loss:  0.002386306180\n",
      "training step: 312420, loss:  0.003467082279\n",
      "training step: 312461, loss:  0.009004401974\n",
      "training step: 312502, loss:  0.005715856329\n",
      "training step: 312543, loss:  0.006285338197\n",
      "training step: 312584, loss:  0.003543942003\n",
      "training step: 312625, loss:  0.003543942003\n",
      "training step: 312666, loss:  0.007877553813\n",
      "training step: 312707, loss:  0.002396283671\n",
      "training step: 312748, loss:  0.002930165734\n",
      "training step: 312789, loss:  0.018698843196\n",
      "training step: 312830, loss:  0.007877553813\n",
      "training step: 312871, loss:  0.003467082279\n",
      "training step: 312912, loss:  0.005485921632\n",
      "training step: 312953, loss:  0.003900909098\n",
      "training step: 312994, loss:  0.003135976614\n",
      "training step: 313035, loss:  0.003638759954\n",
      "training step: 313076, loss:  0.003284437582\n",
      "training step: 313117, loss:  0.007341358811\n",
      "training step: 313158, loss:  0.003467082279\n",
      "training step: 313199, loss:  0.011776376516\n",
      "training step: 313240, loss:  0.009004401974\n",
      "training step: 313281, loss:  0.005003615282\n",
      "training step: 313322, loss:  0.007877553813\n",
      "training step: 313363, loss:  0.003986798227\n",
      "training step: 313404, loss:  0.003986798227\n",
      "training step: 313445, loss:  0.003473303979\n",
      "training step: 313486, loss:  0.002396283671\n",
      "training step: 313527, loss:  0.003120336682\n",
      "training step: 313568, loss:  0.007341358811\n",
      "training step: 313609, loss:  0.003120336682\n",
      "training step: 313650, loss:  0.007877553813\n",
      "training step: 313691, loss:  0.003135976614\n",
      "training step: 313732, loss:  0.004087368958\n",
      "training step: 313773, loss:  0.024754593149\n",
      "training step: 313814, loss:  0.014145905152\n",
      "training step: 313855, loss:  0.003473303979\n",
      "training step: 313896, loss:  0.018698843196\n",
      "training step: 313937, loss:  0.003120336682\n",
      "training step: 313978, loss:  0.023757282645\n",
      "training step: 314019, loss:  0.003776642960\n",
      "training step: 314060, loss:  0.005485921632\n",
      "training step: 314101, loss:  0.002386306180\n",
      "training step: 314142, loss:  0.007877553813\n",
      "training step: 314183, loss:  0.016485802829\n",
      "training step: 314224, loss:  0.002396283671\n",
      "training step: 314265, loss:  0.003692979924\n",
      "training step: 314306, loss:  0.003120336682\n",
      "training step: 314347, loss:  0.009004401974\n",
      "training step: 314388, loss:  0.014145905152\n",
      "training step: 314429, loss:  0.003880497534\n",
      "training step: 314470, loss:  0.006285338197\n",
      "training step: 314511, loss:  0.003900909098\n",
      "training step: 314552, loss:  0.002386306180\n",
      "training step: 314593, loss:  0.023757282645\n",
      "training step: 314634, loss:  0.009004401974\n",
      "training step: 314675, loss:  0.003473303979\n",
      "training step: 314716, loss:  0.011776376516\n",
      "training step: 314757, loss:  0.003120336682\n",
      "training step: 314798, loss:  0.003900909098\n",
      "training step: 314839, loss:  0.009473215789\n",
      "training step: 314880, loss:  0.003284437582\n",
      "training step: 314921, loss:  0.003880497534\n",
      "training step: 314962, loss:  0.003284437582\n",
      "training step: 315003, loss:  0.007341358811\n",
      "training step: 315044, loss:  0.009004401974\n",
      "training step: 315085, loss:  0.025568028912\n",
      "training step: 315126, loss:  0.020691098645\n",
      "training step: 315167, loss:  0.003638759954\n",
      "training step: 315208, loss:  0.003692979924\n",
      "training step: 315249, loss:  0.003011468332\n",
      "training step: 315290, loss:  0.003776642960\n",
      "training step: 315331, loss:  0.007877553813\n",
      "training step: 315372, loss:  0.003284437582\n",
      "training step: 315413, loss:  0.005003615282\n",
      "training step: 315454, loss:  0.002396283671\n",
      "training step: 315495, loss:  0.003880497534\n",
      "training step: 315536, loss:  0.023757282645\n",
      "training step: 315577, loss:  0.018698843196\n",
      "training step: 315618, loss:  0.011776376516\n",
      "training step: 315659, loss:  0.022392421961\n",
      "training step: 315700, loss:  0.004080093466\n",
      "training step: 315741, loss:  0.005715856329\n",
      "training step: 315782, loss:  0.004087368958\n",
      "training step: 315823, loss:  0.002396283671\n",
      "training step: 315864, loss:  0.003776642960\n",
      "training step: 315905, loss:  0.003692979924\n",
      "training step: 315946, loss:  0.005485921632\n",
      "training step: 315987, loss:  0.003467082279\n",
      "training step: 316028, loss:  0.006285338197\n",
      "training step: 316069, loss:  0.003473303979\n",
      "training step: 316110, loss:  0.020691098645\n",
      "training step: 316151, loss:  0.002930165734\n",
      "training step: 316192, loss:  0.003543942003\n",
      "training step: 316233, loss:  0.018698843196\n",
      "training step: 316274, loss:  0.003776642960\n",
      "training step: 316315, loss:  0.003473303979\n",
      "training step: 316356, loss:  0.003120336682\n",
      "training step: 316397, loss:  0.007752379868\n",
      "training step: 316438, loss:  0.005715856329\n",
      "training step: 316479, loss:  0.002396283671\n",
      "training step: 316520, loss:  0.003900909098\n",
      "training step: 316561, loss:  0.005003615282\n",
      "training step: 316602, loss:  0.003638759954\n",
      "training step: 316643, loss:  0.009004401974\n",
      "training step: 316684, loss:  0.009473215789\n",
      "training step: 316725, loss:  0.011776376516\n",
      "training step: 316766, loss:  0.018698843196\n",
      "training step: 316807, loss:  0.002975780051\n",
      "training step: 316848, loss:  0.007752379868\n",
      "training step: 316889, loss:  0.009473215789\n",
      "training step: 316930, loss:  0.002975780051\n",
      "training step: 316971, loss:  0.016485802829\n",
      "training step: 317012, loss:  0.025568028912\n",
      "training step: 317053, loss:  0.003880497534\n",
      "training step: 317094, loss:  0.007752379868\n",
      "training step: 317135, loss:  0.003880497534\n",
      "training step: 317176, loss:  0.023757282645\n",
      "training step: 317217, loss:  0.009004401974\n",
      "training step: 317258, loss:  0.007752379868\n",
      "training step: 317299, loss:  0.002975780051\n",
      "training step: 317340, loss:  0.016485802829\n",
      "training step: 317381, loss:  0.005485921632\n",
      "training step: 317422, loss:  0.005003615282\n",
      "training step: 317463, loss:  0.024754593149\n",
      "training step: 317504, loss:  0.023757282645\n",
      "training step: 317545, loss:  0.003002761398\n",
      "training step: 317586, loss:  0.003011468332\n",
      "training step: 317627, loss:  0.003866109066\n",
      "training step: 317668, loss:  0.007877553813\n",
      "training step: 317709, loss:  0.003986798227\n",
      "training step: 317750, loss:  0.002930165734\n",
      "training step: 317791, loss:  0.004087368958\n",
      "training step: 317832, loss:  0.022392421961\n",
      "training step: 317873, loss:  0.014145905152\n",
      "training step: 317914, loss:  0.003473303979\n",
      "training step: 317955, loss:  0.016485802829\n",
      "training step: 317996, loss:  0.005003615282\n",
      "training step: 318037, loss:  0.007877553813\n",
      "training step: 318078, loss:  0.022392421961\n",
      "training step: 318119, loss:  0.003467082279\n",
      "training step: 318160, loss:  0.003692979924\n",
      "training step: 318201, loss:  0.005485921632\n",
      "training step: 318242, loss:  0.016485802829\n",
      "training step: 318283, loss:  0.011776376516\n",
      "training step: 318324, loss:  0.003473303979\n",
      "training step: 318365, loss:  0.024754593149\n",
      "training step: 318406, loss:  0.009004401974\n",
      "training step: 318447, loss:  0.025363046676\n",
      "training step: 318488, loss:  0.003135976614\n",
      "training step: 318529, loss:  0.003866109066\n",
      "training step: 318570, loss:  0.007341358811\n",
      "training step: 318611, loss:  0.005003615282\n",
      "training step: 318652, loss:  0.018698843196\n",
      "training step: 318693, loss:  0.018698843196\n",
      "training step: 318734, loss:  0.003638759954\n",
      "training step: 318775, loss:  0.003880497534\n",
      "training step: 318816, loss:  0.003473303979\n",
      "training step: 318857, loss:  0.003120336682\n",
      "training step: 318898, loss:  0.007341358811\n",
      "training step: 318939, loss:  0.003776642960\n",
      "training step: 318980, loss:  0.003135976614\n",
      "training step: 319021, loss:  0.003986798227\n",
      "training step: 319062, loss:  0.003135976614\n",
      "training step: 319103, loss:  0.016485802829\n",
      "training step: 319144, loss:  0.018698843196\n",
      "training step: 319185, loss:  0.016485802829\n",
      "training step: 319226, loss:  0.014145905152\n",
      "training step: 319267, loss:  0.007752379868\n",
      "training step: 319308, loss:  0.020691098645\n",
      "training step: 319349, loss:  0.003866109066\n",
      "training step: 319390, loss:  0.002386306180\n",
      "training step: 319431, loss:  0.014145905152\n",
      "training step: 319472, loss:  0.007877553813\n",
      "training step: 319513, loss:  0.003986798227\n",
      "training step: 319554, loss:  0.003011468332\n",
      "training step: 319595, loss:  0.005485921632\n",
      "training step: 319636, loss:  0.003900909098\n",
      "training step: 319677, loss:  0.022392421961\n",
      "training step: 319718, loss:  0.009004401974\n",
      "training step: 319759, loss:  0.004087368958\n",
      "training step: 319800, loss:  0.003135976614\n",
      "training step: 319841, loss:  0.003866109066\n",
      "training step: 319882, loss:  0.005715856329\n",
      "training step: 319923, loss:  0.025568028912\n",
      "training step: 319964, loss:  0.003011468332\n",
      "training step: 320005, loss:  0.002386306180\n",
      "training step: 320046, loss:  0.003135976614\n",
      "training step: 320087, loss:  0.003776642960\n",
      "training step: 320128, loss:  0.003986798227\n",
      "training step: 320169, loss:  0.003808671376\n",
      "training step: 320210, loss:  0.003808671376\n",
      "training step: 320251, loss:  0.003776642960\n",
      "training step: 320292, loss:  0.025568028912\n",
      "training step: 320333, loss:  0.003543942003\n",
      "training step: 320374, loss:  0.020691098645\n",
      "training step: 320415, loss:  0.003808671376\n",
      "training step: 320456, loss:  0.004080093466\n",
      "training step: 320497, loss:  0.004080093466\n",
      "training step: 320538, loss:  0.003638759954\n",
      "training step: 320579, loss:  0.022392421961\n",
      "training step: 320620, loss:  0.003002761398\n",
      "training step: 320661, loss:  0.003880497534\n",
      "training step: 320702, loss:  0.003011468332\n",
      "training step: 320743, loss:  0.005485921632\n",
      "training step: 320784, loss:  0.003011468332\n",
      "training step: 320825, loss:  0.006285338197\n",
      "training step: 320866, loss:  0.002396283671\n",
      "training step: 320907, loss:  0.005715856329\n",
      "training step: 320948, loss:  0.004080093466\n",
      "training step: 320989, loss:  0.006285338197\n",
      "training step: 321030, loss:  0.003776642960\n",
      "training step: 321071, loss:  0.020691098645\n",
      "training step: 321112, loss:  0.003002761398\n",
      "training step: 321153, loss:  0.003986798227\n",
      "training step: 321194, loss:  0.003986798227\n",
      "training step: 321235, loss:  0.002386306180\n",
      "training step: 321276, loss:  0.022392421961\n",
      "training step: 321317, loss:  0.003011468332\n",
      "training step: 321358, loss:  0.003900909098\n",
      "training step: 321399, loss:  0.002930165734\n",
      "training step: 321440, loss:  0.025568028912\n",
      "training step: 321481, loss:  0.025568028912\n",
      "training step: 321522, loss:  0.011776376516\n",
      "training step: 321563, loss:  0.003866109066\n",
      "training step: 321604, loss:  0.002396283671\n",
      "training step: 321645, loss:  0.005003615282\n",
      "training step: 321686, loss:  0.020691098645\n",
      "training step: 321727, loss:  0.022392421961\n",
      "training step: 321768, loss:  0.007341358811\n",
      "training step: 321809, loss:  0.014145905152\n",
      "training step: 321850, loss:  0.003135976614\n",
      "training step: 321891, loss:  0.004087368958\n",
      "training step: 321932, loss:  0.003467082279\n",
      "training step: 321973, loss:  0.011776376516\n",
      "training step: 322014, loss:  0.003776642960\n",
      "training step: 322055, loss:  0.003986798227\n",
      "training step: 322096, loss:  0.003986798227\n",
      "training step: 322137, loss:  0.004080093466\n",
      "training step: 322178, loss:  0.003880497534\n",
      "training step: 322219, loss:  0.003776642960\n",
      "training step: 322260, loss:  0.009004401974\n",
      "training step: 322301, loss:  0.003543942003\n",
      "training step: 322342, loss:  0.018698843196\n",
      "training step: 322383, loss:  0.025363046676\n",
      "training step: 322424, loss:  0.003002761398\n",
      "training step: 322465, loss:  0.003900909098\n",
      "training step: 322506, loss:  0.007341358811\n",
      "training step: 322547, loss:  0.003880497534\n",
      "training step: 322588, loss:  0.018698843196\n",
      "training step: 322629, loss:  0.003986798227\n",
      "training step: 322670, loss:  0.005715856329\n",
      "training step: 322711, loss:  0.007877553813\n",
      "training step: 322752, loss:  0.009473215789\n",
      "training step: 322793, loss:  0.007341358811\n",
      "training step: 322834, loss:  0.006285338197\n",
      "training step: 322875, loss:  0.003543942003\n",
      "training step: 322916, loss:  0.006285338197\n",
      "training step: 322957, loss:  0.023757282645\n",
      "training step: 322998, loss:  0.007752379868\n",
      "training step: 323039, loss:  0.003135976614\n",
      "training step: 323080, loss:  0.002930165734\n",
      "training step: 323121, loss:  0.003692979924\n",
      "training step: 323162, loss:  0.003880497534\n",
      "training step: 323203, loss:  0.007341358811\n",
      "training step: 323244, loss:  0.003986798227\n",
      "training step: 323285, loss:  0.003011468332\n",
      "training step: 323326, loss:  0.003135976614\n",
      "training step: 323367, loss:  0.003120336682\n",
      "training step: 323408, loss:  0.003135976614\n",
      "training step: 323449, loss:  0.016485802829\n",
      "training step: 323490, loss:  0.025363046676\n",
      "training step: 323531, loss:  0.023757282645\n",
      "training step: 323572, loss:  0.005485921632\n",
      "training step: 323613, loss:  0.005715856329\n",
      "training step: 323654, loss:  0.025568028912\n",
      "training step: 323695, loss:  0.006285338197\n",
      "training step: 323736, loss:  0.005715856329\n",
      "training step: 323777, loss:  0.003135976614\n",
      "training step: 323818, loss:  0.025568028912\n",
      "training step: 323859, loss:  0.003543942003\n",
      "training step: 323900, loss:  0.003986798227\n",
      "training step: 323941, loss:  0.004087368958\n",
      "training step: 323982, loss:  0.003473303979\n",
      "training step: 324023, loss:  0.002930165734\n",
      "training step: 324064, loss:  0.009004401974\n",
      "training step: 324105, loss:  0.007877553813\n",
      "training step: 324146, loss:  0.003011468332\n",
      "training step: 324187, loss:  0.025363046676\n",
      "training step: 324228, loss:  0.018698843196\n",
      "training step: 324269, loss:  0.022392421961\n",
      "training step: 324310, loss:  0.009473215789\n",
      "training step: 324351, loss:  0.002930165734\n",
      "training step: 324392, loss:  0.007752379868\n",
      "training step: 324433, loss:  0.011776376516\n",
      "training step: 324474, loss:  0.004080093466\n",
      "training step: 324515, loss:  0.022392421961\n",
      "training step: 324556, loss:  0.003467082279\n",
      "training step: 324597, loss:  0.003135976614\n",
      "training step: 324638, loss:  0.003776642960\n",
      "training step: 324679, loss:  0.009473215789\n",
      "training step: 324720, loss:  0.003638759954\n",
      "training step: 324761, loss:  0.003986798227\n",
      "training step: 324802, loss:  0.006285338197\n",
      "training step: 324843, loss:  0.003866109066\n",
      "training step: 324884, loss:  0.002386306180\n",
      "training step: 324925, loss:  0.003543942003\n",
      "training step: 324966, loss:  0.002930165734\n",
      "training step: 325007, loss:  0.004080093466\n",
      "training step: 325048, loss:  0.003002761398\n",
      "training step: 325089, loss:  0.003776642960\n",
      "training step: 325130, loss:  0.007341358811\n",
      "training step: 325171, loss:  0.025568028912\n",
      "training step: 325212, loss:  0.025568028912\n",
      "training step: 325253, loss:  0.016485802829\n",
      "training step: 325294, loss:  0.002975780051\n",
      "training step: 325335, loss:  0.024754593149\n",
      "training step: 325376, loss:  0.003900909098\n",
      "training step: 325417, loss:  0.003473303979\n",
      "training step: 325458, loss:  0.007341358811\n",
      "training step: 325499, loss:  0.003880497534\n",
      "training step: 325540, loss:  0.011776376516\n",
      "training step: 325581, loss:  0.003808671376\n",
      "training step: 325622, loss:  0.014145905152\n",
      "training step: 325663, loss:  0.003002761398\n",
      "training step: 325704, loss:  0.003776642960\n",
      "training step: 325745, loss:  0.007752379868\n",
      "training step: 325786, loss:  0.005485921632\n",
      "training step: 325827, loss:  0.005003615282\n",
      "training step: 325868, loss:  0.018698843196\n",
      "training step: 325909, loss:  0.003284437582\n",
      "training step: 325950, loss:  0.003866109066\n",
      "training step: 325991, loss:  0.018698843196\n",
      "training step: 326032, loss:  0.007877553813\n",
      "training step: 326073, loss:  0.003284437582\n",
      "training step: 326114, loss:  0.005715856329\n",
      "training step: 326155, loss:  0.025363046676\n",
      "training step: 326196, loss:  0.018698843196\n",
      "training step: 326237, loss:  0.003692979924\n",
      "training step: 326278, loss:  0.011776376516\n",
      "training step: 326319, loss:  0.009473215789\n",
      "training step: 326360, loss:  0.003002761398\n",
      "training step: 326401, loss:  0.011776376516\n",
      "training step: 326442, loss:  0.003638759954\n",
      "training step: 326483, loss:  0.020691098645\n",
      "training step: 326524, loss:  0.018698843196\n",
      "training step: 326565, loss:  0.003900909098\n",
      "training step: 326606, loss:  0.003543942003\n",
      "training step: 326647, loss:  0.022392421961\n",
      "training step: 326688, loss:  0.007752379868\n",
      "training step: 326729, loss:  0.003900909098\n",
      "training step: 326770, loss:  0.020691098645\n",
      "training step: 326811, loss:  0.007877553813\n",
      "training step: 326852, loss:  0.003543942003\n",
      "training step: 326893, loss:  0.002386306180\n",
      "training step: 326934, loss:  0.002975780051\n",
      "training step: 326975, loss:  0.023757282645\n",
      "training step: 327016, loss:  0.005003615282\n",
      "training step: 327057, loss:  0.003692979924\n",
      "training step: 327098, loss:  0.023757282645\n",
      "training step: 327139, loss:  0.005715856329\n",
      "training step: 327180, loss:  0.024754593149\n",
      "training step: 327221, loss:  0.007877553813\n",
      "training step: 327262, loss:  0.020691098645\n",
      "training step: 327303, loss:  0.003900909098\n",
      "training step: 327344, loss:  0.003467082279\n",
      "training step: 327385, loss:  0.018698843196\n",
      "training step: 327426, loss:  0.005715856329\n",
      "training step: 327467, loss:  0.003692979924\n",
      "training step: 327508, loss:  0.002386306180\n",
      "training step: 327549, loss:  0.003011468332\n",
      "training step: 327590, loss:  0.003135976614\n",
      "training step: 327631, loss:  0.011776376516\n",
      "training step: 327672, loss:  0.005715856329\n",
      "training step: 327713, loss:  0.016485802829\n",
      "training step: 327754, loss:  0.003776642960\n",
      "training step: 327795, loss:  0.023757282645\n",
      "training step: 327836, loss:  0.003866109066\n",
      "training step: 327877, loss:  0.002930165734\n",
      "training step: 327918, loss:  0.009004401974\n",
      "training step: 327959, loss:  0.020691098645\n",
      "training step: 328000, loss:  0.009004401974\n",
      "training step: 328041, loss:  0.020691098645\n",
      "training step: 328082, loss:  0.011776376516\n",
      "training step: 328123, loss:  0.003638759954\n",
      "training step: 328164, loss:  0.009473215789\n",
      "training step: 328205, loss:  0.003986798227\n",
      "training step: 328246, loss:  0.014145905152\n",
      "training step: 328287, loss:  0.005715856329\n",
      "training step: 328328, loss:  0.024754593149\n",
      "training step: 328369, loss:  0.002975780051\n",
      "training step: 328410, loss:  0.006285338197\n",
      "training step: 328451, loss:  0.003880497534\n",
      "training step: 328492, loss:  0.003986798227\n",
      "training step: 328533, loss:  0.003900909098\n",
      "training step: 328574, loss:  0.014145905152\n",
      "training step: 328615, loss:  0.003135976614\n",
      "training step: 328656, loss:  0.003986798227\n",
      "training step: 328697, loss:  0.022392421961\n",
      "training step: 328738, loss:  0.003986798227\n",
      "training step: 328779, loss:  0.004080093466\n",
      "training step: 328820, loss:  0.018698843196\n",
      "training step: 328861, loss:  0.003120336682\n",
      "training step: 328902, loss:  0.003284437582\n",
      "training step: 328943, loss:  0.003120336682\n",
      "training step: 328984, loss:  0.016485802829\n",
      "training step: 329025, loss:  0.003808671376\n",
      "training step: 329066, loss:  0.022392421961\n",
      "training step: 329107, loss:  0.011776376516\n",
      "training step: 329148, loss:  0.002386306180\n",
      "training step: 329189, loss:  0.023757282645\n",
      "training step: 329230, loss:  0.003135976614\n",
      "training step: 329271, loss:  0.022392421961\n",
      "training step: 329312, loss:  0.003543942003\n",
      "training step: 329353, loss:  0.003808671376\n",
      "training step: 329394, loss:  0.003866109066\n",
      "training step: 329435, loss:  0.007752379868\n",
      "training step: 329476, loss:  0.002975780051\n",
      "training step: 329517, loss:  0.020691098645\n",
      "training step: 329558, loss:  0.004087368958\n",
      "training step: 329599, loss:  0.004080093466\n",
      "training step: 329640, loss:  0.006285338197\n",
      "training step: 329681, loss:  0.003900909098\n",
      "training step: 329722, loss:  0.004080093466\n",
      "training step: 329763, loss:  0.005715856329\n",
      "training step: 329804, loss:  0.007752379868\n",
      "training step: 329845, loss:  0.002386306180\n",
      "training step: 329886, loss:  0.016485802829\n",
      "training step: 329927, loss:  0.009004401974\n",
      "training step: 329968, loss:  0.022392421961\n",
      "training step: 330009, loss:  0.003808671376\n",
      "training step: 330050, loss:  0.003638759954\n",
      "training step: 330091, loss:  0.005003615282\n",
      "training step: 330132, loss:  0.005003615282\n",
      "training step: 330173, loss:  0.016485802829\n",
      "training step: 330214, loss:  0.005003615282\n",
      "training step: 330255, loss:  0.003692979924\n",
      "training step: 330296, loss:  0.009004401974\n",
      "training step: 330337, loss:  0.004080093466\n",
      "training step: 330378, loss:  0.003880497534\n",
      "training step: 330419, loss:  0.003120336682\n",
      "training step: 330460, loss:  0.002396283671\n",
      "training step: 330501, loss:  0.009004401974\n",
      "training step: 330542, loss:  0.005003615282\n",
      "training step: 330583, loss:  0.005485921632\n",
      "training step: 330624, loss:  0.003284437582\n",
      "training step: 330665, loss:  0.007752379868\n",
      "training step: 330706, loss:  0.003776642960\n",
      "training step: 330747, loss:  0.023757282645\n",
      "training step: 330788, loss:  0.025363046676\n",
      "training step: 330829, loss:  0.006285338197\n",
      "training step: 330870, loss:  0.024754593149\n",
      "training step: 330911, loss:  0.004087368958\n",
      "training step: 330952, loss:  0.003002761398\n",
      "training step: 330993, loss:  0.004080093466\n",
      "training step: 331034, loss:  0.005003615282\n",
      "training step: 331075, loss:  0.014145905152\n",
      "training step: 331116, loss:  0.009473215789\n",
      "training step: 331157, loss:  0.003776642960\n",
      "training step: 331198, loss:  0.025363046676\n",
      "training step: 331239, loss:  0.004080093466\n",
      "training step: 331280, loss:  0.003011468332\n",
      "training step: 331321, loss:  0.005485921632\n",
      "training step: 331362, loss:  0.009473215789\n",
      "training step: 331403, loss:  0.002396283671\n",
      "training step: 331444, loss:  0.014145905152\n",
      "training step: 331485, loss:  0.003776642960\n",
      "training step: 331526, loss:  0.005485921632\n",
      "training step: 331567, loss:  0.003900909098\n",
      "training step: 331608, loss:  0.003808671376\n",
      "training step: 331649, loss:  0.003467082279\n",
      "training step: 331690, loss:  0.022392421961\n",
      "training step: 331731, loss:  0.003543942003\n",
      "training step: 331772, loss:  0.003986798227\n",
      "training step: 331813, loss:  0.005715856329\n",
      "training step: 331854, loss:  0.003543942003\n",
      "training step: 331895, loss:  0.007752379868\n",
      "training step: 331936, loss:  0.003986798227\n",
      "training step: 331977, loss:  0.003002761398\n",
      "training step: 332018, loss:  0.003284437582\n",
      "training step: 332059, loss:  0.003866109066\n",
      "training step: 332100, loss:  0.003692979924\n",
      "training step: 332141, loss:  0.007877553813\n",
      "training step: 332182, loss:  0.003002761398\n",
      "training step: 332223, loss:  0.022392421961\n",
      "training step: 332264, loss:  0.004087368958\n",
      "training step: 332305, loss:  0.003900909098\n",
      "training step: 332346, loss:  0.018698843196\n",
      "training step: 332387, loss:  0.007341358811\n",
      "training step: 332428, loss:  0.003808671376\n",
      "training step: 332469, loss:  0.004087368958\n",
      "training step: 332510, loss:  0.004087368958\n",
      "training step: 332551, loss:  0.014145905152\n",
      "training step: 332592, loss:  0.007752379868\n",
      "training step: 332633, loss:  0.018698843196\n",
      "training step: 332674, loss:  0.025363046676\n",
      "training step: 332715, loss:  0.025568028912\n",
      "training step: 332756, loss:  0.003986798227\n",
      "training step: 332797, loss:  0.004080093466\n",
      "training step: 332838, loss:  0.003467082279\n",
      "training step: 332879, loss:  0.002396283671\n",
      "training step: 332920, loss:  0.003866109066\n",
      "training step: 332961, loss:  0.016485802829\n",
      "training step: 333002, loss:  0.005003615282\n",
      "training step: 333043, loss:  0.003776642960\n",
      "training step: 333084, loss:  0.003880497534\n",
      "training step: 333125, loss:  0.003880497534\n",
      "training step: 333166, loss:  0.003638759954\n",
      "training step: 333207, loss:  0.025363046676\n",
      "training step: 333248, loss:  0.003284437582\n",
      "training step: 333289, loss:  0.003692979924\n",
      "training step: 333330, loss:  0.003473303979\n",
      "training step: 333371, loss:  0.005485921632\n",
      "training step: 333412, loss:  0.025568028912\n",
      "training step: 333453, loss:  0.003473303979\n",
      "training step: 333494, loss:  0.003638759954\n",
      "training step: 333535, loss:  0.003880497534\n",
      "training step: 333576, loss:  0.018698843196\n",
      "training step: 333617, loss:  0.003120336682\n",
      "training step: 333658, loss:  0.007752379868\n",
      "training step: 333699, loss:  0.009473215789\n",
      "training step: 333740, loss:  0.003011468332\n",
      "training step: 333781, loss:  0.003120336682\n",
      "training step: 333822, loss:  0.014145905152\n",
      "training step: 333863, loss:  0.003467082279\n",
      "training step: 333904, loss:  0.002930165734\n",
      "training step: 333945, loss:  0.003808671376\n",
      "training step: 333986, loss:  0.014145905152\n",
      "training step: 334027, loss:  0.005715856329\n",
      "training step: 334068, loss:  0.009004401974\n",
      "training step: 334109, loss:  0.009473215789\n",
      "training step: 334150, loss:  0.003638759954\n",
      "training step: 334191, loss:  0.014145905152\n",
      "training step: 334232, loss:  0.007752379868\n",
      "training step: 334273, loss:  0.003900909098\n",
      "training step: 334314, loss:  0.003880497534\n",
      "training step: 334355, loss:  0.005485921632\n",
      "training step: 334396, loss:  0.023757282645\n",
      "training step: 334437, loss:  0.002386306180\n",
      "training step: 334478, loss:  0.003900909098\n",
      "training step: 334519, loss:  0.002386306180\n",
      "training step: 334560, loss:  0.003011468332\n",
      "training step: 334601, loss:  0.007341358811\n",
      "training step: 334642, loss:  0.003900909098\n",
      "training step: 334683, loss:  0.003002761398\n",
      "training step: 334724, loss:  0.009004401974\n",
      "training step: 334765, loss:  0.003473303979\n",
      "training step: 334806, loss:  0.003866109066\n",
      "training step: 334847, loss:  0.003900909098\n",
      "training step: 334888, loss:  0.022392421961\n",
      "training step: 334929, loss:  0.011776376516\n",
      "training step: 334970, loss:  0.002386306180\n",
      "training step: 335011, loss:  0.024754593149\n",
      "training step: 335052, loss:  0.007877553813\n",
      "training step: 335093, loss:  0.007752379868\n",
      "training step: 335134, loss:  0.003284437582\n",
      "training step: 335175, loss:  0.006285338197\n",
      "training step: 335216, loss:  0.002386306180\n",
      "training step: 335257, loss:  0.005003615282\n",
      "training step: 335298, loss:  0.005715856329\n",
      "training step: 335339, loss:  0.004080093466\n",
      "training step: 335380, loss:  0.003467082279\n",
      "training step: 335421, loss:  0.011776376516\n",
      "training step: 335462, loss:  0.014145905152\n",
      "training step: 335503, loss:  0.003011468332\n",
      "training step: 335544, loss:  0.014145905152\n",
      "training step: 335585, loss:  0.003011468332\n",
      "training step: 335626, loss:  0.023757282645\n",
      "training step: 335667, loss:  0.003808671376\n",
      "training step: 335708, loss:  0.003002761398\n",
      "training step: 335749, loss:  0.022392421961\n",
      "training step: 335790, loss:  0.003011468332\n",
      "training step: 335831, loss:  0.005003615282\n",
      "training step: 335872, loss:  0.002975780051\n",
      "training step: 335913, loss:  0.005715856329\n",
      "training step: 335954, loss:  0.003467082279\n",
      "training step: 335995, loss:  0.003002761398\n",
      "training step: 336036, loss:  0.003120336682\n",
      "training step: 336077, loss:  0.005003615282\n",
      "training step: 336118, loss:  0.002386306180\n",
      "training step: 336159, loss:  0.002975780051\n",
      "training step: 336200, loss:  0.007877553813\n",
      "training step: 336241, loss:  0.003866109066\n",
      "training step: 336282, loss:  0.009473215789\n",
      "training step: 336323, loss:  0.002975780051\n",
      "training step: 336364, loss:  0.003776642960\n",
      "training step: 336405, loss:  0.009473215789\n",
      "training step: 336446, loss:  0.007877553813\n",
      "training step: 336487, loss:  0.003866109066\n",
      "training step: 336528, loss:  0.003120336682\n",
      "training step: 336569, loss:  0.024754593149\n",
      "training step: 336610, loss:  0.018698843196\n",
      "training step: 336651, loss:  0.024754593149\n",
      "training step: 336692, loss:  0.003986798227\n",
      "training step: 336733, loss:  0.007752379868\n",
      "training step: 336774, loss:  0.003880497534\n",
      "training step: 336815, loss:  0.003120336682\n",
      "training step: 336856, loss:  0.003692979924\n",
      "training step: 336897, loss:  0.014145905152\n",
      "training step: 336938, loss:  0.003135976614\n",
      "training step: 336979, loss:  0.024754593149\n",
      "training step: 337020, loss:  0.003880497534\n",
      "training step: 337061, loss:  0.002396283671\n",
      "training step: 337102, loss:  0.002386306180\n",
      "training step: 337143, loss:  0.007877553813\n",
      "training step: 337184, loss:  0.003808671376\n",
      "training step: 337225, loss:  0.002975780051\n",
      "training step: 337266, loss:  0.011776376516\n",
      "training step: 337307, loss:  0.025363046676\n",
      "training step: 337348, loss:  0.002396283671\n",
      "training step: 337389, loss:  0.007877553813\n",
      "training step: 337430, loss:  0.003473303979\n",
      "training step: 337471, loss:  0.002386306180\n",
      "training step: 337512, loss:  0.002975780051\n",
      "training step: 337553, loss:  0.007877553813\n",
      "training step: 337594, loss:  0.006285338197\n",
      "training step: 337635, loss:  0.020691098645\n",
      "training step: 337676, loss:  0.018698843196\n",
      "training step: 337717, loss:  0.002930165734\n",
      "training step: 337758, loss:  0.007877553813\n",
      "training step: 337799, loss:  0.003543942003\n",
      "training step: 337840, loss:  0.023757282645\n",
      "training step: 337881, loss:  0.003120336682\n",
      "training step: 337922, loss:  0.007877553813\n",
      "training step: 337963, loss:  0.003880497534\n",
      "training step: 338004, loss:  0.023757282645\n",
      "training step: 338045, loss:  0.022392421961\n",
      "training step: 338086, loss:  0.003986798227\n",
      "training step: 338127, loss:  0.025363046676\n",
      "training step: 338168, loss:  0.006285338197\n",
      "training step: 338209, loss:  0.002975780051\n",
      "training step: 338250, loss:  0.003866109066\n",
      "training step: 338291, loss:  0.002386306180\n",
      "training step: 338332, loss:  0.003467082279\n",
      "training step: 338373, loss:  0.007752379868\n",
      "training step: 338414, loss:  0.016485802829\n",
      "training step: 338455, loss:  0.014145905152\n",
      "training step: 338496, loss:  0.003473303979\n",
      "training step: 338537, loss:  0.003473303979\n",
      "training step: 338578, loss:  0.002386306180\n",
      "training step: 338619, loss:  0.007877553813\n",
      "training step: 338660, loss:  0.007341358811\n",
      "training step: 338701, loss:  0.007341358811\n",
      "training step: 338742, loss:  0.005003615282\n",
      "training step: 338783, loss:  0.003900909098\n",
      "training step: 338824, loss:  0.007877553813\n",
      "training step: 338865, loss:  0.002386306180\n",
      "training step: 338906, loss:  0.006285338197\n",
      "training step: 338947, loss:  0.005485921632\n",
      "training step: 338988, loss:  0.005485921632\n",
      "training step: 339029, loss:  0.003543942003\n",
      "training step: 339070, loss:  0.005485921632\n",
      "training step: 339111, loss:  0.020691098645\n",
      "training step: 339152, loss:  0.005485921632\n",
      "training step: 339193, loss:  0.009004401974\n",
      "training step: 339234, loss:  0.006285338197\n",
      "training step: 339275, loss:  0.002386306180\n",
      "training step: 339316, loss:  0.002975780051\n",
      "training step: 339357, loss:  0.005485921632\n",
      "training step: 339398, loss:  0.004087368958\n",
      "training step: 339439, loss:  0.004087368958\n",
      "training step: 339480, loss:  0.024754593149\n",
      "training step: 339521, loss:  0.022392421961\n",
      "training step: 339562, loss:  0.007752379868\n",
      "training step: 339603, loss:  0.003866109066\n",
      "training step: 339644, loss:  0.003776642960\n",
      "training step: 339685, loss:  0.007752379868\n",
      "training step: 339726, loss:  0.025568028912\n",
      "training step: 339767, loss:  0.003135976614\n",
      "training step: 339808, loss:  0.003120336682\n",
      "training step: 339849, loss:  0.003473303979\n",
      "training step: 339890, loss:  0.020691098645\n",
      "training step: 339931, loss:  0.006285338197\n",
      "training step: 339972, loss:  0.007341358811\n",
      "training step: 340013, loss:  0.003467082279\n",
      "training step: 340054, loss:  0.003002761398\n",
      "training step: 340095, loss:  0.023757282645\n",
      "training step: 340136, loss:  0.014145905152\n",
      "training step: 340177, loss:  0.002975780051\n",
      "training step: 340218, loss:  0.004080093466\n",
      "training step: 340259, loss:  0.003135976614\n",
      "training step: 340300, loss:  0.003120336682\n",
      "training step: 340341, loss:  0.002396283671\n",
      "training step: 340382, loss:  0.005485921632\n",
      "training step: 340423, loss:  0.007877553813\n",
      "training step: 340464, loss:  0.016485802829\n",
      "training step: 340505, loss:  0.014145905152\n",
      "training step: 340546, loss:  0.009004401974\n",
      "training step: 340587, loss:  0.025363046676\n",
      "training step: 340628, loss:  0.005003615282\n",
      "training step: 340669, loss:  0.022392421961\n",
      "training step: 340710, loss:  0.003638759954\n",
      "training step: 340751, loss:  0.003543942003\n",
      "training step: 340792, loss:  0.005003615282\n",
      "training step: 340833, loss:  0.004080093466\n",
      "training step: 340874, loss:  0.014145905152\n",
      "training step: 340915, loss:  0.003808671376\n",
      "training step: 340956, loss:  0.003473303979\n",
      "training step: 340997, loss:  0.003473303979\n",
      "training step: 341038, loss:  0.009473215789\n",
      "training step: 341079, loss:  0.003808671376\n",
      "training step: 341120, loss:  0.003284437582\n",
      "training step: 341161, loss:  0.014145905152\n",
      "training step: 341202, loss:  0.025363046676\n",
      "training step: 341243, loss:  0.014145905152\n",
      "training step: 341284, loss:  0.025363046676\n",
      "training step: 341325, loss:  0.003120336682\n",
      "training step: 341366, loss:  0.003467082279\n",
      "training step: 341407, loss:  0.002386306180\n",
      "training step: 341448, loss:  0.014145905152\n",
      "training step: 341489, loss:  0.002386306180\n",
      "training step: 341530, loss:  0.022392421961\n",
      "training step: 341571, loss:  0.022392421961\n",
      "training step: 341612, loss:  0.003135976614\n",
      "training step: 341653, loss:  0.003011468332\n",
      "training step: 341694, loss:  0.003284437582\n",
      "training step: 341735, loss:  0.003692979924\n",
      "training step: 341776, loss:  0.003692979924\n",
      "training step: 341817, loss:  0.003543942003\n",
      "training step: 341858, loss:  0.003284437582\n",
      "training step: 341899, loss:  0.004087368958\n",
      "training step: 341940, loss:  0.003473303979\n",
      "training step: 341981, loss:  0.002930165734\n",
      "training step: 342022, loss:  0.003880497534\n",
      "training step: 342063, loss:  0.024754593149\n",
      "training step: 342104, loss:  0.014145905152\n",
      "training step: 342145, loss:  0.011776376516\n",
      "training step: 342186, loss:  0.003467082279\n",
      "training step: 342227, loss:  0.020691098645\n",
      "training step: 342268, loss:  0.002930165734\n",
      "training step: 342309, loss:  0.007341358811\n",
      "training step: 342350, loss:  0.003473303979\n",
      "training step: 342391, loss:  0.002396283671\n",
      "training step: 342432, loss:  0.009004401974\n",
      "training step: 342473, loss:  0.025363046676\n",
      "training step: 342514, loss:  0.002386306180\n",
      "training step: 342555, loss:  0.003002761398\n",
      "training step: 342596, loss:  0.003880497534\n",
      "training step: 342637, loss:  0.025568028912\n",
      "training step: 342678, loss:  0.003011468332\n",
      "training step: 342719, loss:  0.016485802829\n",
      "training step: 342760, loss:  0.003866109066\n",
      "training step: 342801, loss:  0.007877553813\n",
      "training step: 342842, loss:  0.007752379868\n",
      "training step: 342883, loss:  0.007752379868\n",
      "training step: 342924, loss:  0.020691098645\n",
      "training step: 342965, loss:  0.003543942003\n",
      "training step: 343006, loss:  0.003776642960\n",
      "training step: 343047, loss:  0.004080093466\n",
      "training step: 343088, loss:  0.003986798227\n",
      "training step: 343129, loss:  0.007877553813\n",
      "training step: 343170, loss:  0.024754593149\n",
      "training step: 343211, loss:  0.007752379868\n",
      "training step: 343252, loss:  0.022392421961\n",
      "training step: 343293, loss:  0.005485921632\n",
      "training step: 343334, loss:  0.003135976614\n",
      "training step: 343375, loss:  0.007341358811\n",
      "training step: 343416, loss:  0.003135976614\n",
      "training step: 343457, loss:  0.025363046676\n",
      "training step: 343498, loss:  0.003692979924\n",
      "training step: 343539, loss:  0.022392421961\n",
      "training step: 343580, loss:  0.005485921632\n",
      "training step: 343621, loss:  0.003002761398\n",
      "training step: 343662, loss:  0.007341358811\n",
      "training step: 343703, loss:  0.003808671376\n",
      "training step: 343744, loss:  0.002386306180\n",
      "training step: 343785, loss:  0.007341358811\n",
      "training step: 343826, loss:  0.003120336682\n",
      "training step: 343867, loss:  0.003866109066\n",
      "training step: 343908, loss:  0.025363046676\n",
      "training step: 343949, loss:  0.003986798227\n",
      "training step: 343990, loss:  0.003692979924\n",
      "training step: 344031, loss:  0.023757282645\n",
      "training step: 344072, loss:  0.003002761398\n",
      "training step: 344113, loss:  0.003473303979\n",
      "training step: 344154, loss:  0.014145905152\n",
      "training step: 344195, loss:  0.003900909098\n",
      "training step: 344236, loss:  0.003880497534\n",
      "training step: 344277, loss:  0.024754593149\n",
      "training step: 344318, loss:  0.003543942003\n",
      "training step: 344359, loss:  0.003986798227\n",
      "training step: 344400, loss:  0.009473215789\n",
      "training step: 344441, loss:  0.002975780051\n",
      "training step: 344482, loss:  0.003808671376\n",
      "training step: 344523, loss:  0.007877553813\n",
      "training step: 344564, loss:  0.025363046676\n",
      "training step: 344605, loss:  0.003011468332\n",
      "training step: 344646, loss:  0.024754593149\n",
      "training step: 344687, loss:  0.003776642960\n",
      "training step: 344728, loss:  0.003284437582\n",
      "training step: 344769, loss:  0.003473303979\n",
      "training step: 344810, loss:  0.003135976614\n",
      "training step: 344851, loss:  0.005003615282\n",
      "training step: 344892, loss:  0.005715856329\n",
      "training step: 344933, loss:  0.005715856329\n",
      "training step: 344974, loss:  0.003808671376\n",
      "training step: 345015, loss:  0.024754593149\n",
      "training step: 345056, loss:  0.002930165734\n",
      "training step: 345097, loss:  0.005485921632\n",
      "training step: 345138, loss:  0.022392421961\n",
      "training step: 345179, loss:  0.003692979924\n",
      "training step: 345220, loss:  0.025363046676\n",
      "training step: 345261, loss:  0.014145905152\n",
      "training step: 345302, loss:  0.003776642960\n",
      "training step: 345343, loss:  0.018698843196\n",
      "training step: 345384, loss:  0.003900909098\n",
      "training step: 345425, loss:  0.005485921632\n",
      "training step: 345466, loss:  0.023757282645\n",
      "training step: 345507, loss:  0.025363046676\n",
      "training step: 345548, loss:  0.025363046676\n",
      "training step: 345589, loss:  0.002386306180\n",
      "training step: 345630, loss:  0.003776642960\n",
      "training step: 345671, loss:  0.025568028912\n",
      "training step: 345712, loss:  0.003120336682\n",
      "training step: 345753, loss:  0.007341358811\n",
      "training step: 345794, loss:  0.023757282645\n",
      "training step: 345835, loss:  0.006285338197\n",
      "training step: 345876, loss:  0.002930165734\n",
      "training step: 345917, loss:  0.002396283671\n",
      "training step: 345958, loss:  0.005715856329\n",
      "training step: 345999, loss:  0.003002761398\n",
      "training step: 346040, loss:  0.009473215789\n",
      "training step: 346081, loss:  0.007341358811\n",
      "training step: 346122, loss:  0.004080093466\n",
      "training step: 346163, loss:  0.006285338197\n",
      "training step: 346204, loss:  0.004087368958\n",
      "training step: 346245, loss:  0.003002761398\n",
      "training step: 346286, loss:  0.003880497534\n",
      "training step: 346327, loss:  0.003011468332\n",
      "training step: 346368, loss:  0.003692979924\n",
      "training step: 346409, loss:  0.007877553813\n",
      "training step: 346450, loss:  0.003120336682\n",
      "training step: 346491, loss:  0.023757282645\n",
      "training step: 346532, loss:  0.003467082279\n",
      "training step: 346573, loss:  0.009004401974\n",
      "training step: 346614, loss:  0.007752379868\n",
      "training step: 346655, loss:  0.002975780051\n",
      "training step: 346696, loss:  0.009004401974\n",
      "training step: 346737, loss:  0.022392421961\n",
      "training step: 346778, loss:  0.020691098645\n",
      "training step: 346819, loss:  0.003776642960\n",
      "training step: 346860, loss:  0.025568028912\n",
      "training step: 346901, loss:  0.009004401974\n",
      "training step: 346942, loss:  0.002975780051\n",
      "training step: 346983, loss:  0.002386306180\n",
      "training step: 347024, loss:  0.003284437582\n",
      "training step: 347065, loss:  0.024754593149\n",
      "training step: 347106, loss:  0.005485921632\n",
      "training step: 347147, loss:  0.007752379868\n",
      "training step: 347188, loss:  0.003866109066\n",
      "training step: 347229, loss:  0.024754593149\n",
      "training step: 347270, loss:  0.020691098645\n",
      "training step: 347311, loss:  0.023757282645\n",
      "training step: 347352, loss:  0.004080093466\n",
      "training step: 347393, loss:  0.024754593149\n",
      "training step: 347434, loss:  0.007341358811\n",
      "training step: 347475, loss:  0.003467082279\n",
      "training step: 347516, loss:  0.023757282645\n",
      "training step: 347557, loss:  0.025568028912\n",
      "training step: 347598, loss:  0.003986798227\n",
      "training step: 347639, loss:  0.023757282645\n",
      "training step: 347680, loss:  0.002386306180\n",
      "training step: 347721, loss:  0.003866109066\n",
      "training step: 347762, loss:  0.007752379868\n",
      "training step: 347803, loss:  0.002975780051\n",
      "training step: 347844, loss:  0.003135976614\n",
      "training step: 347885, loss:  0.003002761398\n",
      "training step: 347926, loss:  0.002386306180\n",
      "training step: 347967, loss:  0.003284437582\n",
      "training step: 348008, loss:  0.011776376516\n",
      "training step: 348049, loss:  0.003776642960\n",
      "training step: 348090, loss:  0.007752379868\n",
      "training step: 348131, loss:  0.002386306180\n",
      "training step: 348172, loss:  0.014145905152\n",
      "training step: 348213, loss:  0.003776642960\n",
      "training step: 348254, loss:  0.002975780051\n",
      "training step: 348295, loss:  0.002975780051\n",
      "training step: 348336, loss:  0.004080093466\n",
      "training step: 348377, loss:  0.016485802829\n",
      "training step: 348418, loss:  0.003866109066\n",
      "training step: 348459, loss:  0.003284437582\n",
      "training step: 348500, loss:  0.005485921632\n",
      "training step: 348541, loss:  0.007877553813\n",
      "training step: 348582, loss:  0.007752379868\n",
      "training step: 348623, loss:  0.003473303979\n",
      "training step: 348664, loss:  0.003467082279\n",
      "training step: 348705, loss:  0.006285338197\n",
      "training step: 348746, loss:  0.009004401974\n",
      "training step: 348787, loss:  0.004087368958\n",
      "training step: 348828, loss:  0.009004401974\n",
      "training step: 348869, loss:  0.003120336682\n",
      "training step: 348910, loss:  0.003467082279\n",
      "training step: 348951, loss:  0.023757282645\n",
      "training step: 348992, loss:  0.005003615282\n",
      "training step: 349033, loss:  0.009473215789\n",
      "training step: 349074, loss:  0.003692979924\n",
      "training step: 349115, loss:  0.003473303979\n",
      "training step: 349156, loss:  0.011776376516\n",
      "training step: 349197, loss:  0.003467082279\n",
      "training step: 349238, loss:  0.007877553813\n",
      "training step: 349279, loss:  0.002386306180\n",
      "training step: 349320, loss:  0.005715856329\n",
      "training step: 349361, loss:  0.003638759954\n",
      "training step: 349402, loss:  0.007752379868\n",
      "training step: 349443, loss:  0.002930165734\n",
      "training step: 349484, loss:  0.003002761398\n",
      "training step: 349525, loss:  0.007752379868\n",
      "training step: 349566, loss:  0.003776642960\n",
      "training step: 349607, loss:  0.022392421961\n",
      "training step: 349648, loss:  0.002396283671\n",
      "training step: 349689, loss:  0.003808671376\n",
      "training step: 349730, loss:  0.024754593149\n",
      "training step: 349771, loss:  0.003880497534\n",
      "training step: 349812, loss:  0.025568028912\n",
      "training step: 349853, loss:  0.002396283671\n",
      "training step: 349894, loss:  0.020691098645\n",
      "training step: 349935, loss:  0.004080093466\n",
      "training step: 349976, loss:  0.005485921632\n",
      "training step: 350017, loss:  0.005003615282\n",
      "training step: 350058, loss:  0.002975780051\n",
      "training step: 350099, loss:  0.009473215789\n",
      "training step: 350140, loss:  0.005485921632\n",
      "training step: 350181, loss:  0.003543942003\n",
      "training step: 350222, loss:  0.002930165734\n",
      "training step: 350263, loss:  0.003692979924\n",
      "training step: 350304, loss:  0.003776642960\n",
      "training step: 350345, loss:  0.009473215789\n",
      "training step: 350386, loss:  0.002975780051\n",
      "training step: 350427, loss:  0.020691098645\n",
      "training step: 350468, loss:  0.003284437582\n",
      "training step: 350509, loss:  0.007341358811\n",
      "training step: 350550, loss:  0.003900909098\n",
      "training step: 350591, loss:  0.005003615282\n",
      "training step: 350632, loss:  0.025568028912\n",
      "training step: 350673, loss:  0.003808671376\n",
      "training step: 350714, loss:  0.003284437582\n",
      "training step: 350755, loss:  0.002930165734\n",
      "training step: 350796, loss:  0.003467082279\n",
      "training step: 350837, loss:  0.003776642960\n",
      "training step: 350878, loss:  0.003866109066\n",
      "training step: 350919, loss:  0.003900909098\n",
      "training step: 350960, loss:  0.023757282645\n",
      "training step: 351001, loss:  0.003808671376\n",
      "training step: 351042, loss:  0.003692979924\n",
      "training step: 351083, loss:  0.003002761398\n",
      "training step: 351124, loss:  0.025363046676\n",
      "training step: 351165, loss:  0.018698843196\n",
      "training step: 351206, loss:  0.003900909098\n",
      "training step: 351247, loss:  0.011776376516\n",
      "training step: 351288, loss:  0.006285338197\n",
      "training step: 351329, loss:  0.025363046676\n",
      "training step: 351370, loss:  0.025568028912\n",
      "training step: 351411, loss:  0.003986798227\n",
      "training step: 351452, loss:  0.002930165734\n",
      "training step: 351493, loss:  0.003692979924\n",
      "training step: 351534, loss:  0.025363046676\n",
      "training step: 351575, loss:  0.025568028912\n",
      "training step: 351616, loss:  0.002975780051\n",
      "training step: 351657, loss:  0.003986798227\n",
      "training step: 351698, loss:  0.011776376516\n",
      "training step: 351739, loss:  0.025363046676\n",
      "training step: 351780, loss:  0.009473215789\n",
      "training step: 351821, loss:  0.007877553813\n",
      "training step: 351862, loss:  0.025568028912\n",
      "training step: 351903, loss:  0.003900909098\n",
      "training step: 351944, loss:  0.003638759954\n",
      "training step: 351985, loss:  0.004080093466\n",
      "training step: 352026, loss:  0.025568028912\n",
      "training step: 352067, loss:  0.003986798227\n",
      "training step: 352108, loss:  0.003002761398\n",
      "training step: 352149, loss:  0.016485802829\n",
      "training step: 352190, loss:  0.007877553813\n",
      "training step: 352231, loss:  0.004080093466\n",
      "training step: 352272, loss:  0.018698843196\n",
      "training step: 352313, loss:  0.022392421961\n",
      "training step: 352354, loss:  0.002930165734\n",
      "training step: 352395, loss:  0.003467082279\n",
      "training step: 352436, loss:  0.024754593149\n",
      "training step: 352477, loss:  0.003638759954\n",
      "training step: 352518, loss:  0.006285338197\n",
      "training step: 352559, loss:  0.003776642960\n",
      "training step: 352600, loss:  0.003866109066\n",
      "training step: 352641, loss:  0.003692979924\n",
      "training step: 352682, loss:  0.004087368958\n",
      "training step: 352723, loss:  0.011776376516\n",
      "training step: 352764, loss:  0.007877553813\n",
      "training step: 352805, loss:  0.003880497534\n",
      "training step: 352846, loss:  0.003011468332\n",
      "training step: 352887, loss:  0.007752379868\n",
      "training step: 352928, loss:  0.003135976614\n",
      "training step: 352969, loss:  0.003638759954\n",
      "training step: 353010, loss:  0.003986798227\n",
      "training step: 353051, loss:  0.014145905152\n",
      "training step: 353092, loss:  0.006285338197\n",
      "training step: 353133, loss:  0.006285338197\n",
      "training step: 353174, loss:  0.003776642960\n",
      "training step: 353215, loss:  0.016485802829\n",
      "training step: 353256, loss:  0.002930165734\n",
      "training step: 353297, loss:  0.002975780051\n",
      "training step: 353338, loss:  0.003900909098\n",
      "training step: 353379, loss:  0.003808671376\n",
      "training step: 353420, loss:  0.003135976614\n",
      "training step: 353461, loss:  0.025363046676\n",
      "training step: 353502, loss:  0.006285338197\n",
      "training step: 353543, loss:  0.002386306180\n",
      "training step: 353584, loss:  0.003543942003\n",
      "training step: 353625, loss:  0.002975780051\n",
      "training step: 353666, loss:  0.003135976614\n",
      "training step: 353707, loss:  0.006285338197\n",
      "training step: 353748, loss:  0.014145905152\n",
      "training step: 353789, loss:  0.002386306180\n",
      "training step: 353830, loss:  0.003473303979\n",
      "training step: 353871, loss:  0.020691098645\n",
      "training step: 353912, loss:  0.016485802829\n",
      "training step: 353953, loss:  0.007752379868\n",
      "training step: 353994, loss:  0.003543942003\n",
      "training step: 354035, loss:  0.003467082279\n",
      "training step: 354076, loss:  0.023757282645\n",
      "training step: 354117, loss:  0.003692979924\n",
      "training step: 354158, loss:  0.003776642960\n",
      "training step: 354199, loss:  0.006285338197\n",
      "training step: 354240, loss:  0.003135976614\n",
      "training step: 354281, loss:  0.003776642960\n",
      "training step: 354322, loss:  0.006285338197\n",
      "training step: 354363, loss:  0.004087368958\n",
      "training step: 354404, loss:  0.007341358811\n",
      "training step: 354445, loss:  0.003866109066\n",
      "training step: 354486, loss:  0.005003615282\n",
      "training step: 354527, loss:  0.007341358811\n",
      "training step: 354568, loss:  0.006285338197\n",
      "training step: 354609, loss:  0.009473215789\n",
      "training step: 354650, loss:  0.007341358811\n",
      "training step: 354691, loss:  0.003284437582\n",
      "training step: 354732, loss:  0.006285338197\n",
      "training step: 354773, loss:  0.011776376516\n",
      "training step: 354814, loss:  0.005003615282\n",
      "training step: 354855, loss:  0.005715856329\n",
      "training step: 354896, loss:  0.023757282645\n",
      "training step: 354937, loss:  0.004087368958\n",
      "training step: 354978, loss:  0.003692979924\n",
      "training step: 355019, loss:  0.003776642960\n",
      "training step: 355060, loss:  0.023757282645\n",
      "training step: 355101, loss:  0.002930165734\n",
      "training step: 355142, loss:  0.009004401974\n",
      "training step: 355183, loss:  0.016485802829\n",
      "training step: 355224, loss:  0.003135976614\n",
      "training step: 355265, loss:  0.025363046676\n",
      "training step: 355306, loss:  0.005003615282\n",
      "training step: 355347, loss:  0.004087368958\n",
      "training step: 355388, loss:  0.007752379868\n",
      "training step: 355429, loss:  0.004087368958\n",
      "training step: 355470, loss:  0.003880497534\n",
      "training step: 355511, loss:  0.002975780051\n",
      "training step: 355552, loss:  0.006285338197\n",
      "training step: 355593, loss:  0.022392421961\n",
      "training step: 355634, loss:  0.003002761398\n",
      "training step: 355675, loss:  0.003467082279\n",
      "training step: 355716, loss:  0.004080093466\n",
      "training step: 355757, loss:  0.003543942003\n",
      "training step: 355798, loss:  0.005715856329\n",
      "training step: 355839, loss:  0.002930165734\n",
      "training step: 355880, loss:  0.003284437582\n",
      "training step: 355921, loss:  0.009473215789\n",
      "training step: 355962, loss:  0.022392421961\n",
      "training step: 356003, loss:  0.020691098645\n",
      "training step: 356044, loss:  0.003002761398\n",
      "training step: 356085, loss:  0.003473303979\n",
      "training step: 356126, loss:  0.025363046676\n",
      "training step: 356167, loss:  0.003776642960\n",
      "training step: 356208, loss:  0.004080093466\n",
      "training step: 356249, loss:  0.025363046676\n",
      "training step: 356290, loss:  0.007877553813\n",
      "training step: 356331, loss:  0.003638759954\n",
      "training step: 356372, loss:  0.003880497534\n",
      "training step: 356413, loss:  0.023757282645\n",
      "training step: 356454, loss:  0.002930165734\n",
      "training step: 356495, loss:  0.023757282645\n",
      "training step: 356536, loss:  0.022392421961\n",
      "training step: 356577, loss:  0.007752379868\n",
      "training step: 356618, loss:  0.005485921632\n",
      "training step: 356659, loss:  0.003473303979\n",
      "training step: 356700, loss:  0.023757282645\n",
      "training step: 356741, loss:  0.002975780051\n",
      "training step: 356782, loss:  0.003900909098\n",
      "training step: 356823, loss:  0.003120336682\n",
      "training step: 356864, loss:  0.002386306180\n",
      "training step: 356905, loss:  0.005715856329\n",
      "training step: 356946, loss:  0.002396283671\n",
      "training step: 356987, loss:  0.023757282645\n",
      "training step: 357028, loss:  0.020691098645\n",
      "training step: 357069, loss:  0.018698843196\n",
      "training step: 357110, loss:  0.006285338197\n",
      "training step: 357151, loss:  0.006285338197\n",
      "training step: 357192, loss:  0.009004401974\n",
      "training step: 357233, loss:  0.007752379868\n",
      "training step: 357274, loss:  0.024754593149\n",
      "training step: 357315, loss:  0.003900909098\n",
      "training step: 357356, loss:  0.002396283671\n",
      "training step: 357397, loss:  0.018698843196\n",
      "training step: 357438, loss:  0.007341358811\n",
      "training step: 357479, loss:  0.025568028912\n",
      "training step: 357520, loss:  0.005715856329\n",
      "training step: 357561, loss:  0.003284437582\n",
      "training step: 357602, loss:  0.014145905152\n",
      "training step: 357643, loss:  0.005715856329\n",
      "training step: 357684, loss:  0.003866109066\n",
      "training step: 357725, loss:  0.006285338197\n",
      "training step: 357766, loss:  0.003880497534\n",
      "training step: 357807, loss:  0.003135976614\n",
      "training step: 357848, loss:  0.007752379868\n",
      "training step: 357889, loss:  0.002930165734\n",
      "training step: 357930, loss:  0.003135976614\n",
      "training step: 357971, loss:  0.003135976614\n",
      "training step: 358012, loss:  0.003120336682\n",
      "training step: 358053, loss:  0.003473303979\n",
      "training step: 358094, loss:  0.009004401974\n",
      "training step: 358135, loss:  0.003120336682\n",
      "training step: 358176, loss:  0.007752379868\n",
      "training step: 358217, loss:  0.003120336682\n",
      "training step: 358258, loss:  0.009004401974\n",
      "training step: 358299, loss:  0.003880497534\n",
      "training step: 358340, loss:  0.003543942003\n",
      "training step: 358381, loss:  0.003880497534\n",
      "training step: 358422, loss:  0.003011468332\n",
      "training step: 358463, loss:  0.003284437582\n",
      "training step: 358504, loss:  0.003776642960\n",
      "training step: 358545, loss:  0.003880497534\n",
      "training step: 358586, loss:  0.003120336682\n",
      "training step: 358627, loss:  0.025568028912\n",
      "training step: 358668, loss:  0.024754593149\n",
      "training step: 358709, loss:  0.009473215789\n",
      "training step: 358750, loss:  0.003866109066\n",
      "training step: 358791, loss:  0.002930165734\n",
      "training step: 358832, loss:  0.003011468332\n",
      "training step: 358873, loss:  0.005485921632\n",
      "training step: 358914, loss:  0.020691098645\n",
      "training step: 358955, loss:  0.005485921632\n",
      "training step: 358996, loss:  0.002396283671\n",
      "training step: 359037, loss:  0.004087368958\n",
      "training step: 359078, loss:  0.002975780051\n",
      "training step: 359119, loss:  0.007752379868\n",
      "training step: 359160, loss:  0.003692979924\n",
      "training step: 359201, loss:  0.003866109066\n",
      "training step: 359242, loss:  0.007877553813\n",
      "training step: 359283, loss:  0.005485921632\n",
      "training step: 359324, loss:  0.002386306180\n",
      "training step: 359365, loss:  0.003011468332\n",
      "training step: 359406, loss:  0.002975780051\n",
      "training step: 359447, loss:  0.007752379868\n",
      "training step: 359488, loss:  0.003011468332\n",
      "training step: 359529, loss:  0.004087368958\n",
      "training step: 359570, loss:  0.005003615282\n",
      "training step: 359611, loss:  0.016485802829\n",
      "training step: 359652, loss:  0.004087368958\n",
      "training step: 359693, loss:  0.020691098645\n",
      "training step: 359734, loss:  0.025568028912\n",
      "training step: 359775, loss:  0.005715856329\n",
      "training step: 359816, loss:  0.003808671376\n",
      "training step: 359857, loss:  0.003467082279\n",
      "training step: 359898, loss:  0.003866109066\n",
      "training step: 359939, loss:  0.004080093466\n",
      "training step: 359980, loss:  0.016485802829\n",
      "training step: 360021, loss:  0.011776376516\n",
      "training step: 360062, loss:  0.003776642960\n",
      "training step: 360103, loss:  0.003002761398\n",
      "training step: 360144, loss:  0.002396283671\n",
      "training step: 360185, loss:  0.003120336682\n",
      "training step: 360226, loss:  0.016485802829\n",
      "training step: 360267, loss:  0.003866109066\n",
      "training step: 360308, loss:  0.009473215789\n",
      "training step: 360349, loss:  0.005003615282\n",
      "training step: 360390, loss:  0.007752379868\n",
      "training step: 360431, loss:  0.007341358811\n",
      "training step: 360472, loss:  0.003986798227\n",
      "training step: 360513, loss:  0.006285338197\n",
      "training step: 360554, loss:  0.003120336682\n",
      "training step: 360595, loss:  0.003900909098\n",
      "training step: 360636, loss:  0.002975780051\n",
      "training step: 360677, loss:  0.003284437582\n",
      "training step: 360718, loss:  0.004087368958\n",
      "training step: 360759, loss:  0.003002761398\n",
      "training step: 360800, loss:  0.005715856329\n",
      "training step: 360841, loss:  0.003866109066\n",
      "training step: 360882, loss:  0.004080093466\n",
      "training step: 360923, loss:  0.020691098645\n",
      "training step: 360964, loss:  0.005003615282\n",
      "training step: 361005, loss:  0.003002761398\n",
      "training step: 361046, loss:  0.002396283671\n",
      "training step: 361087, loss:  0.003120336682\n",
      "training step: 361128, loss:  0.023757282645\n",
      "training step: 361169, loss:  0.005715856329\n",
      "training step: 361210, loss:  0.022392421961\n",
      "training step: 361251, loss:  0.025568028912\n",
      "training step: 361292, loss:  0.018698843196\n",
      "training step: 361333, loss:  0.007752379868\n",
      "training step: 361374, loss:  0.007877553813\n",
      "training step: 361415, loss:  0.005485921632\n",
      "training step: 361456, loss:  0.004080093466\n",
      "training step: 361497, loss:  0.002396283671\n",
      "training step: 361538, loss:  0.003866109066\n",
      "training step: 361579, loss:  0.003638759954\n",
      "training step: 361620, loss:  0.018698843196\n",
      "training step: 361661, loss:  0.003284437582\n",
      "training step: 361702, loss:  0.002930165734\n",
      "training step: 361743, loss:  0.003135976614\n",
      "training step: 361784, loss:  0.002930165734\n",
      "training step: 361825, loss:  0.002975780051\n",
      "training step: 361866, loss:  0.004080093466\n",
      "training step: 361907, loss:  0.003473303979\n",
      "training step: 361948, loss:  0.003808671376\n",
      "training step: 361989, loss:  0.004080093466\n",
      "training step: 362030, loss:  0.003638759954\n",
      "training step: 362071, loss:  0.025363046676\n",
      "training step: 362112, loss:  0.014145905152\n",
      "training step: 362153, loss:  0.003011468332\n",
      "training step: 362194, loss:  0.004087368958\n",
      "training step: 362235, loss:  0.025568028912\n",
      "training step: 362276, loss:  0.003900909098\n",
      "training step: 362317, loss:  0.003776642960\n",
      "training step: 362358, loss:  0.022392421961\n",
      "training step: 362399, loss:  0.014145905152\n",
      "training step: 362440, loss:  0.004080093466\n",
      "training step: 362481, loss:  0.007752379868\n",
      "training step: 362522, loss:  0.004080093466\n",
      "training step: 362563, loss:  0.020691098645\n",
      "training step: 362604, loss:  0.003692979924\n",
      "training step: 362645, loss:  0.002930165734\n",
      "training step: 362686, loss:  0.003808671376\n",
      "training step: 362727, loss:  0.002396283671\n",
      "training step: 362768, loss:  0.004087368958\n",
      "training step: 362809, loss:  0.025363046676\n",
      "training step: 362850, loss:  0.003011468332\n",
      "training step: 362891, loss:  0.003900909098\n",
      "training step: 362932, loss:  0.007341358811\n",
      "training step: 362973, loss:  0.011776376516\n",
      "training step: 363014, loss:  0.005715856329\n",
      "training step: 363055, loss:  0.003776642960\n",
      "training step: 363096, loss:  0.002930165734\n",
      "training step: 363137, loss:  0.003135976614\n",
      "training step: 363178, loss:  0.007341358811\n",
      "training step: 363219, loss:  0.003120336682\n",
      "training step: 363260, loss:  0.014145905152\n",
      "training step: 363301, loss:  0.002396283671\n",
      "training step: 363342, loss:  0.003284437582\n",
      "training step: 363383, loss:  0.025363046676\n",
      "training step: 363424, loss:  0.003986798227\n",
      "training step: 363465, loss:  0.003284437582\n",
      "training step: 363506, loss:  0.003135976614\n",
      "training step: 363547, loss:  0.003776642960\n",
      "training step: 363588, loss:  0.003986798227\n",
      "training step: 363629, loss:  0.011776376516\n",
      "training step: 363670, loss:  0.003638759954\n",
      "training step: 363711, loss:  0.007341358811\n",
      "training step: 363752, loss:  0.003120336682\n",
      "training step: 363793, loss:  0.005485921632\n",
      "training step: 363834, loss:  0.003543942003\n",
      "training step: 363875, loss:  0.002975780051\n",
      "training step: 363916, loss:  0.007752379868\n",
      "training step: 363957, loss:  0.003808671376\n",
      "training step: 363998, loss:  0.016485802829\n",
      "training step: 364039, loss:  0.003692979924\n",
      "training step: 364080, loss:  0.005003615282\n",
      "training step: 364121, loss:  0.003638759954\n",
      "training step: 364162, loss:  0.006285338197\n",
      "training step: 364203, loss:  0.004080093466\n",
      "training step: 364244, loss:  0.020691098645\n",
      "training step: 364285, loss:  0.003284437582\n",
      "training step: 364326, loss:  0.020691098645\n",
      "training step: 364367, loss:  0.007752379868\n",
      "training step: 364408, loss:  0.024754593149\n",
      "training step: 364449, loss:  0.007877553813\n",
      "training step: 364490, loss:  0.007877553813\n",
      "training step: 364531, loss:  0.003120336682\n",
      "training step: 364572, loss:  0.009473215789\n",
      "training step: 364613, loss:  0.003808671376\n",
      "training step: 364654, loss:  0.024754593149\n",
      "training step: 364695, loss:  0.007752379868\n",
      "training step: 364736, loss:  0.014145905152\n",
      "training step: 364777, loss:  0.024754593149\n",
      "training step: 364818, loss:  0.025363046676\n",
      "training step: 364859, loss:  0.003284437582\n",
      "training step: 364900, loss:  0.002386306180\n",
      "training step: 364941, loss:  0.003011468332\n",
      "training step: 364982, loss:  0.025568028912\n",
      "training step: 365023, loss:  0.003808671376\n",
      "training step: 365064, loss:  0.003692979924\n",
      "training step: 365105, loss:  0.005003615282\n",
      "training step: 365146, loss:  0.011776376516\n",
      "training step: 365187, loss:  0.007752379868\n",
      "training step: 365228, loss:  0.024754593149\n",
      "training step: 365269, loss:  0.003284437582\n",
      "training step: 365310, loss:  0.014145905152\n",
      "training step: 365351, loss:  0.003120336682\n",
      "training step: 365392, loss:  0.003638759954\n",
      "training step: 365433, loss:  0.003011468332\n",
      "training step: 365474, loss:  0.005485921632\n",
      "training step: 365515, loss:  0.003284437582\n",
      "training step: 365556, loss:  0.002396283671\n",
      "training step: 365597, loss:  0.003900909098\n",
      "training step: 365638, loss:  0.018698843196\n",
      "training step: 365679, loss:  0.003473303979\n",
      "training step: 365720, loss:  0.020691098645\n",
      "training step: 365761, loss:  0.024754593149\n",
      "training step: 365802, loss:  0.020691098645\n",
      "training step: 365843, loss:  0.006285338197\n",
      "training step: 365884, loss:  0.005715856329\n",
      "training step: 365925, loss:  0.003692979924\n",
      "training step: 365966, loss:  0.002975780051\n",
      "training step: 366007, loss:  0.003880497534\n",
      "training step: 366048, loss:  0.003543942003\n",
      "training step: 366089, loss:  0.003692979924\n",
      "training step: 366130, loss:  0.022392421961\n",
      "training step: 366171, loss:  0.020691098645\n",
      "training step: 366212, loss:  0.003866109066\n",
      "training step: 366253, loss:  0.002386306180\n",
      "training step: 366294, loss:  0.003692979924\n",
      "training step: 366335, loss:  0.014145905152\n",
      "training step: 366376, loss:  0.003776642960\n",
      "training step: 366417, loss:  0.003866109066\n",
      "training step: 366458, loss:  0.005715856329\n",
      "training step: 366499, loss:  0.002930165734\n",
      "training step: 366540, loss:  0.002930165734\n",
      "training step: 366581, loss:  0.003473303979\n",
      "training step: 366622, loss:  0.003776642960\n",
      "training step: 366663, loss:  0.007877553813\n",
      "training step: 366704, loss:  0.003135976614\n",
      "training step: 366745, loss:  0.020691098645\n",
      "training step: 366786, loss:  0.003473303979\n",
      "training step: 366827, loss:  0.009004401974\n",
      "training step: 366868, loss:  0.016485802829\n",
      "training step: 366909, loss:  0.016485802829\n",
      "training step: 366950, loss:  0.003002761398\n",
      "training step: 366991, loss:  0.025363046676\n",
      "training step: 367032, loss:  0.003120336682\n",
      "training step: 367073, loss:  0.003986798227\n",
      "training step: 367114, loss:  0.003808671376\n",
      "training step: 367155, loss:  0.009004401974\n",
      "training step: 367196, loss:  0.003692979924\n",
      "training step: 367237, loss:  0.024754593149\n",
      "training step: 367278, loss:  0.025363046676\n",
      "training step: 367319, loss:  0.003900909098\n",
      "training step: 367360, loss:  0.003776642960\n",
      "training step: 367401, loss:  0.003284437582\n",
      "training step: 367442, loss:  0.003135976614\n",
      "training step: 367483, loss:  0.003120336682\n",
      "training step: 367524, loss:  0.018698843196\n",
      "training step: 367565, loss:  0.002386306180\n",
      "training step: 367606, loss:  0.003808671376\n",
      "training step: 367647, loss:  0.007341358811\n",
      "training step: 367688, loss:  0.007341358811\n",
      "training step: 367729, loss:  0.003135976614\n",
      "training step: 367770, loss:  0.009004401974\n",
      "training step: 367811, loss:  0.009473215789\n",
      "training step: 367852, loss:  0.011776376516\n",
      "training step: 367893, loss:  0.002930165734\n",
      "training step: 367934, loss:  0.018698843196\n",
      "training step: 367975, loss:  0.003120336682\n",
      "training step: 368016, loss:  0.003473303979\n",
      "training step: 368057, loss:  0.003866109066\n",
      "training step: 368098, loss:  0.003900909098\n",
      "training step: 368139, loss:  0.003011468332\n",
      "training step: 368180, loss:  0.018698843196\n",
      "training step: 368221, loss:  0.022392421961\n",
      "training step: 368262, loss:  0.003473303979\n",
      "training step: 368303, loss:  0.002930165734\n",
      "training step: 368344, loss:  0.016485802829\n",
      "training step: 368385, loss:  0.002386306180\n",
      "training step: 368426, loss:  0.003473303979\n",
      "training step: 368467, loss:  0.003543942003\n",
      "training step: 368508, loss:  0.003776642960\n",
      "training step: 368549, loss:  0.005485921632\n",
      "training step: 368590, loss:  0.003120336682\n",
      "training step: 368631, loss:  0.002396283671\n",
      "training step: 368672, loss:  0.009004401974\n",
      "training step: 368713, loss:  0.011776376516\n",
      "training step: 368754, loss:  0.005485921632\n",
      "training step: 368795, loss:  0.003808671376\n",
      "training step: 368836, loss:  0.002975780051\n",
      "training step: 368877, loss:  0.022392421961\n",
      "training step: 368918, loss:  0.025363046676\n",
      "training step: 368959, loss:  0.018698843196\n",
      "training step: 369000, loss:  0.025363046676\n",
      "training step: 369041, loss:  0.009473215789\n",
      "training step: 369082, loss:  0.003692979924\n",
      "training step: 369123, loss:  0.003473303979\n",
      "training step: 369164, loss:  0.007341358811\n",
      "training step: 369205, loss:  0.007341358811\n",
      "training step: 369246, loss:  0.014145905152\n",
      "training step: 369287, loss:  0.003543942003\n",
      "training step: 369328, loss:  0.005003615282\n",
      "training step: 369369, loss:  0.002975780051\n",
      "training step: 369410, loss:  0.005485921632\n",
      "training step: 369451, loss:  0.004080093466\n",
      "training step: 369492, loss:  0.002975780051\n",
      "training step: 369533, loss:  0.018698843196\n",
      "training step: 369574, loss:  0.002386306180\n",
      "training step: 369615, loss:  0.018698843196\n",
      "training step: 369656, loss:  0.007341358811\n",
      "training step: 369697, loss:  0.020691098645\n",
      "training step: 369738, loss:  0.018698843196\n",
      "training step: 369779, loss:  0.002396283671\n",
      "training step: 369820, loss:  0.004080093466\n",
      "training step: 369861, loss:  0.003543942003\n",
      "training step: 369902, loss:  0.009473215789\n",
      "training step: 369943, loss:  0.005003615282\n",
      "training step: 369984, loss:  0.006285338197\n",
      "training step: 370025, loss:  0.003986798227\n",
      "training step: 370066, loss:  0.011776376516\n",
      "training step: 370107, loss:  0.002396283671\n",
      "training step: 370148, loss:  0.005003615282\n",
      "training step: 370189, loss:  0.006285338197\n",
      "training step: 370230, loss:  0.003866109066\n",
      "training step: 370271, loss:  0.003866109066\n",
      "training step: 370312, loss:  0.005715856329\n",
      "training step: 370353, loss:  0.003880497534\n",
      "training step: 370394, loss:  0.003866109066\n",
      "training step: 370435, loss:  0.009473215789\n",
      "training step: 370476, loss:  0.003692979924\n",
      "training step: 370517, loss:  0.025363046676\n",
      "training step: 370558, loss:  0.003011468332\n",
      "training step: 370599, loss:  0.007877553813\n",
      "training step: 370640, loss:  0.003692979924\n",
      "training step: 370681, loss:  0.025363046676\n",
      "training step: 370722, loss:  0.002396283671\n",
      "training step: 370763, loss:  0.024754593149\n",
      "training step: 370804, loss:  0.003900909098\n",
      "training step: 370845, loss:  0.003120336682\n",
      "training step: 370886, loss:  0.025568028912\n",
      "training step: 370927, loss:  0.007752379868\n",
      "training step: 370968, loss:  0.023757282645\n",
      "training step: 371009, loss:  0.003692979924\n",
      "training step: 371050, loss:  0.003986798227\n",
      "training step: 371091, loss:  0.024754593149\n",
      "training step: 371132, loss:  0.003011468332\n",
      "training step: 371173, loss:  0.004080093466\n",
      "training step: 371214, loss:  0.024754593149\n",
      "training step: 371255, loss:  0.011776376516\n",
      "training step: 371296, loss:  0.003120336682\n",
      "training step: 371337, loss:  0.003120336682\n",
      "training step: 371378, loss:  0.003808671376\n",
      "training step: 371419, loss:  0.002975780051\n",
      "training step: 371460, loss:  0.005003615282\n",
      "training step: 371501, loss:  0.002975780051\n",
      "training step: 371542, loss:  0.002930165734\n",
      "training step: 371583, loss:  0.002930165734\n",
      "training step: 371624, loss:  0.003986798227\n",
      "training step: 371665, loss:  0.003638759954\n",
      "training step: 371706, loss:  0.003473303979\n",
      "training step: 371747, loss:  0.005485921632\n",
      "training step: 371788, loss:  0.003011468332\n",
      "training step: 371829, loss:  0.003543942003\n",
      "training step: 371870, loss:  0.005485921632\n",
      "training step: 371911, loss:  0.002396283671\n",
      "training step: 371952, loss:  0.003692979924\n",
      "training step: 371993, loss:  0.002396283671\n",
      "training step: 372034, loss:  0.018698843196\n",
      "training step: 372075, loss:  0.003473303979\n",
      "training step: 372116, loss:  0.024754593149\n",
      "training step: 372157, loss:  0.016485802829\n",
      "training step: 372198, loss:  0.003284437582\n",
      "training step: 372239, loss:  0.023757282645\n",
      "training step: 372280, loss:  0.003808671376\n",
      "training step: 372321, loss:  0.007341358811\n",
      "training step: 372362, loss:  0.002396283671\n",
      "training step: 372403, loss:  0.009004401974\n",
      "training step: 372444, loss:  0.024754593149\n",
      "training step: 372485, loss:  0.025363046676\n",
      "training step: 372526, loss:  0.004087368958\n",
      "training step: 372567, loss:  0.003776642960\n",
      "training step: 372608, loss:  0.003638759954\n",
      "training step: 372649, loss:  0.016485802829\n",
      "training step: 372690, loss:  0.003776642960\n",
      "training step: 372731, loss:  0.003900909098\n",
      "training step: 372772, loss:  0.003986798227\n",
      "training step: 372813, loss:  0.003692979924\n",
      "training step: 372854, loss:  0.024754593149\n",
      "training step: 372895, loss:  0.003473303979\n",
      "training step: 372936, loss:  0.003900909098\n",
      "training step: 372977, loss:  0.007341358811\n",
      "training step: 373018, loss:  0.007752379868\n",
      "training step: 373059, loss:  0.016485802829\n",
      "training step: 373100, loss:  0.003900909098\n",
      "training step: 373141, loss:  0.020691098645\n",
      "training step: 373182, loss:  0.025568028912\n",
      "training step: 373223, loss:  0.011776376516\n",
      "training step: 373264, loss:  0.009004401974\n",
      "training step: 373305, loss:  0.025568028912\n",
      "training step: 373346, loss:  0.006285338197\n",
      "training step: 373387, loss:  0.003692979924\n",
      "training step: 373428, loss:  0.009473215789\n",
      "training step: 373469, loss:  0.003638759954\n",
      "training step: 373510, loss:  0.007752379868\n",
      "training step: 373551, loss:  0.011776376516\n",
      "training step: 373592, loss:  0.002396283671\n",
      "training step: 373633, loss:  0.005715856329\n",
      "training step: 373674, loss:  0.025568028912\n",
      "training step: 373715, loss:  0.002396283671\n",
      "training step: 373756, loss:  0.003638759954\n",
      "training step: 373797, loss:  0.002386306180\n",
      "training step: 373838, loss:  0.023757282645\n",
      "training step: 373879, loss:  0.009473215789\n",
      "training step: 373920, loss:  0.020691098645\n",
      "training step: 373961, loss:  0.009004401974\n",
      "training step: 374002, loss:  0.002930165734\n",
      "training step: 374043, loss:  0.024754593149\n",
      "training step: 374084, loss:  0.023757282645\n",
      "training step: 374125, loss:  0.025363046676\n",
      "training step: 374166, loss:  0.003473303979\n",
      "training step: 374207, loss:  0.003776642960\n",
      "training step: 374248, loss:  0.007341358811\n",
      "training step: 374289, loss:  0.003808671376\n",
      "training step: 374330, loss:  0.002930165734\n",
      "training step: 374371, loss:  0.003120336682\n",
      "training step: 374412, loss:  0.006285338197\n",
      "training step: 374453, loss:  0.002930165734\n",
      "training step: 374494, loss:  0.006285338197\n",
      "training step: 374535, loss:  0.024754593149\n",
      "training step: 374576, loss:  0.005003615282\n",
      "training step: 374617, loss:  0.003011468332\n",
      "training step: 374658, loss:  0.004087368958\n",
      "training step: 374699, loss:  0.025568028912\n",
      "training step: 374740, loss:  0.009473215789\n",
      "training step: 374781, loss:  0.014145905152\n",
      "training step: 374822, loss:  0.002386306180\n",
      "training step: 374863, loss:  0.007752379868\n",
      "training step: 374904, loss:  0.004080093466\n",
      "training step: 374945, loss:  0.007752379868\n",
      "training step: 374986, loss:  0.014145905152\n",
      "training step: 375027, loss:  0.014145905152\n",
      "training step: 375068, loss:  0.007877553813\n",
      "training step: 375109, loss:  0.003808671376\n",
      "training step: 375150, loss:  0.005715856329\n",
      "training step: 375191, loss:  0.003543942003\n",
      "training step: 375232, loss:  0.003638759954\n",
      "training step: 375273, loss:  0.006285338197\n",
      "training step: 375314, loss:  0.018698843196\n",
      "training step: 375355, loss:  0.003120336682\n",
      "training step: 375396, loss:  0.004080093466\n",
      "training step: 375437, loss:  0.002930165734\n",
      "training step: 375478, loss:  0.003880497534\n",
      "training step: 375519, loss:  0.025363046676\n",
      "training step: 375560, loss:  0.018698843196\n",
      "training step: 375601, loss:  0.003808671376\n",
      "training step: 375642, loss:  0.003120336682\n",
      "training step: 375683, loss:  0.003473303979\n",
      "training step: 375724, loss:  0.003986798227\n",
      "training step: 375765, loss:  0.003900909098\n",
      "training step: 375806, loss:  0.003120336682\n",
      "training step: 375847, loss:  0.025568028912\n",
      "training step: 375888, loss:  0.005715856329\n",
      "training step: 375929, loss:  0.005003615282\n",
      "training step: 375970, loss:  0.009473215789\n",
      "training step: 376011, loss:  0.007752379868\n",
      "training step: 376052, loss:  0.003120336682\n",
      "training step: 376093, loss:  0.007752379868\n",
      "training step: 376134, loss:  0.003900909098\n",
      "training step: 376175, loss:  0.024754593149\n",
      "training step: 376216, loss:  0.003776642960\n",
      "training step: 376257, loss:  0.014145905152\n",
      "training step: 376298, loss:  0.009473215789\n",
      "training step: 376339, loss:  0.002930165734\n",
      "training step: 376380, loss:  0.002975780051\n",
      "training step: 376421, loss:  0.007877553813\n",
      "training step: 376462, loss:  0.018698843196\n",
      "training step: 376503, loss:  0.005003615282\n",
      "training step: 376544, loss:  0.007877553813\n",
      "training step: 376585, loss:  0.007752379868\n",
      "training step: 376626, loss:  0.006285338197\n",
      "training step: 376667, loss:  0.003866109066\n",
      "training step: 376708, loss:  0.004087368958\n",
      "training step: 376749, loss:  0.004080093466\n",
      "training step: 376790, loss:  0.003473303979\n",
      "training step: 376831, loss:  0.004080093466\n",
      "training step: 376872, loss:  0.014145905152\n",
      "training step: 376913, loss:  0.018698843196\n",
      "training step: 376954, loss:  0.006285338197\n",
      "training step: 376995, loss:  0.003135976614\n",
      "training step: 377036, loss:  0.004087368958\n",
      "training step: 377077, loss:  0.007877553813\n",
      "training step: 377118, loss:  0.004087368958\n",
      "training step: 377159, loss:  0.002386306180\n",
      "training step: 377200, loss:  0.007752379868\n",
      "training step: 377241, loss:  0.025363046676\n",
      "training step: 377282, loss:  0.003011468332\n",
      "training step: 377323, loss:  0.002975780051\n",
      "training step: 377364, loss:  0.002930165734\n",
      "training step: 377405, loss:  0.018698843196\n",
      "training step: 377446, loss:  0.007341358811\n",
      "training step: 377487, loss:  0.025363046676\n",
      "training step: 377528, loss:  0.002930165734\n",
      "training step: 377569, loss:  0.016485802829\n",
      "training step: 377610, loss:  0.011776376516\n",
      "training step: 377651, loss:  0.022392421961\n",
      "training step: 377692, loss:  0.014145905152\n",
      "training step: 377733, loss:  0.003135976614\n",
      "training step: 377774, loss:  0.025363046676\n",
      "training step: 377815, loss:  0.003986798227\n",
      "training step: 377856, loss:  0.003011468332\n",
      "training step: 377897, loss:  0.003120336682\n",
      "training step: 377938, loss:  0.003900909098\n",
      "training step: 377979, loss:  0.023757282645\n",
      "training step: 378020, loss:  0.003002761398\n",
      "training step: 378061, loss:  0.016485802829\n",
      "training step: 378102, loss:  0.022392421961\n",
      "training step: 378143, loss:  0.003543942003\n",
      "training step: 378184, loss:  0.003284437582\n",
      "training step: 378225, loss:  0.003692979924\n",
      "training step: 378266, loss:  0.003135976614\n",
      "training step: 378307, loss:  0.016485802829\n",
      "training step: 378348, loss:  0.002930165734\n",
      "training step: 378389, loss:  0.003880497534\n",
      "training step: 378430, loss:  0.003866109066\n",
      "training step: 378471, loss:  0.006285338197\n",
      "training step: 378512, loss:  0.009004401974\n",
      "training step: 378553, loss:  0.003692979924\n",
      "training step: 378594, loss:  0.003986798227\n",
      "training step: 378635, loss:  0.003692979924\n",
      "training step: 378676, loss:  0.023757282645\n",
      "training step: 378717, loss:  0.022392421961\n",
      "training step: 378758, loss:  0.003473303979\n",
      "training step: 378799, loss:  0.007341358811\n",
      "training step: 378840, loss:  0.007752379868\n",
      "training step: 378881, loss:  0.003692979924\n",
      "training step: 378922, loss:  0.003808671376\n",
      "training step: 378963, loss:  0.003467082279\n",
      "training step: 379004, loss:  0.003467082279\n",
      "training step: 379045, loss:  0.020691098645\n",
      "training step: 379086, loss:  0.002396283671\n",
      "training step: 379127, loss:  0.003467082279\n",
      "training step: 379168, loss:  0.004087368958\n",
      "training step: 379209, loss:  0.007341358811\n",
      "training step: 379250, loss:  0.003011468332\n",
      "training step: 379291, loss:  0.003284437582\n",
      "training step: 379332, loss:  0.003467082279\n",
      "training step: 379373, loss:  0.022392421961\n",
      "training step: 379414, loss:  0.007877553813\n",
      "training step: 379455, loss:  0.011776376516\n",
      "training step: 379496, loss:  0.003808671376\n",
      "training step: 379537, loss:  0.003776642960\n",
      "training step: 379578, loss:  0.002386306180\n",
      "training step: 379619, loss:  0.011776376516\n",
      "training step: 379660, loss:  0.003011468332\n",
      "training step: 379701, loss:  0.003808671376\n",
      "training step: 379742, loss:  0.005715856329\n",
      "training step: 379783, loss:  0.011776376516\n",
      "training step: 379824, loss:  0.025363046676\n",
      "training step: 379865, loss:  0.024754593149\n",
      "training step: 379906, loss:  0.003011468332\n",
      "training step: 379947, loss:  0.003135976614\n",
      "training step: 379988, loss:  0.007341358811\n",
      "training step: 380029, loss:  0.018698843196\n",
      "training step: 380070, loss:  0.003284437582\n",
      "training step: 380111, loss:  0.025568028912\n",
      "training step: 380152, loss:  0.003473303979\n",
      "training step: 380193, loss:  0.003808671376\n",
      "training step: 380234, loss:  0.009004401974\n",
      "training step: 380275, loss:  0.018698843196\n",
      "training step: 380316, loss:  0.002386306180\n",
      "training step: 380357, loss:  0.003135976614\n",
      "training step: 380398, loss:  0.006285338197\n",
      "training step: 380439, loss:  0.003002761398\n",
      "training step: 380480, loss:  0.003543942003\n",
      "training step: 380521, loss:  0.002386306180\n",
      "training step: 380562, loss:  0.003986798227\n",
      "training step: 380603, loss:  0.005715856329\n",
      "training step: 380644, loss:  0.007877553813\n",
      "training step: 380685, loss:  0.003986798227\n",
      "training step: 380726, loss:  0.003002761398\n",
      "training step: 380767, loss:  0.005715856329\n",
      "training step: 380808, loss:  0.025363046676\n",
      "training step: 380849, loss:  0.005715856329\n",
      "training step: 380890, loss:  0.002396283671\n",
      "training step: 380931, loss:  0.018698843196\n",
      "training step: 380972, loss:  0.022392421961\n",
      "training step: 381013, loss:  0.007877553813\n",
      "training step: 381054, loss:  0.007752379868\n",
      "training step: 381095, loss:  0.006285338197\n",
      "training step: 381136, loss:  0.020691098645\n",
      "training step: 381177, loss:  0.003284437582\n",
      "training step: 381218, loss:  0.003808671376\n",
      "training step: 381259, loss:  0.003284437582\n",
      "training step: 381300, loss:  0.003692979924\n",
      "training step: 381341, loss:  0.009004401974\n",
      "training step: 381382, loss:  0.003866109066\n",
      "training step: 381423, loss:  0.003120336682\n",
      "training step: 381464, loss:  0.003880497534\n",
      "training step: 381505, loss:  0.009473215789\n",
      "training step: 381546, loss:  0.003467082279\n",
      "training step: 381587, loss:  0.006285338197\n",
      "training step: 381628, loss:  0.002930165734\n",
      "training step: 381669, loss:  0.003986798227\n",
      "training step: 381710, loss:  0.003986798227\n",
      "training step: 381751, loss:  0.002386306180\n",
      "training step: 381792, loss:  0.002975780051\n",
      "training step: 381833, loss:  0.003135976614\n",
      "training step: 381874, loss:  0.005003615282\n",
      "training step: 381915, loss:  0.003120336682\n",
      "training step: 381956, loss:  0.003002761398\n",
      "training step: 381997, loss:  0.005003615282\n",
      "training step: 382038, loss:  0.002396283671\n",
      "training step: 382079, loss:  0.002396283671\n",
      "training step: 382120, loss:  0.005715856329\n",
      "training step: 382161, loss:  0.003986798227\n",
      "training step: 382202, loss:  0.020691098645\n",
      "training step: 382243, loss:  0.005003615282\n",
      "training step: 382284, loss:  0.006285338197\n",
      "training step: 382325, loss:  0.007877553813\n",
      "training step: 382366, loss:  0.003880497534\n",
      "training step: 382407, loss:  0.003002761398\n",
      "training step: 382448, loss:  0.018698843196\n",
      "training step: 382489, loss:  0.003776642960\n",
      "training step: 382530, loss:  0.002386306180\n",
      "training step: 382571, loss:  0.004087368958\n",
      "training step: 382612, loss:  0.003776642960\n",
      "training step: 382653, loss:  0.005003615282\n",
      "training step: 382694, loss:  0.023757282645\n",
      "training step: 382735, loss:  0.003011468332\n",
      "training step: 382776, loss:  0.003866109066\n",
      "training step: 382817, loss:  0.003120336682\n",
      "training step: 382858, loss:  0.003900909098\n",
      "training step: 382899, loss:  0.005485921632\n",
      "training step: 382940, loss:  0.022392421961\n",
      "training step: 382981, loss:  0.004087368958\n",
      "training step: 383022, loss:  0.003880497534\n",
      "training step: 383063, loss:  0.014145905152\n",
      "training step: 383104, loss:  0.003776642960\n",
      "training step: 383145, loss:  0.004087368958\n",
      "training step: 383186, loss:  0.018698843196\n",
      "training step: 383227, loss:  0.007752379868\n",
      "training step: 383268, loss:  0.003880497534\n",
      "training step: 383309, loss:  0.003011468332\n",
      "training step: 383350, loss:  0.005485921632\n",
      "training step: 383391, loss:  0.003467082279\n",
      "training step: 383432, loss:  0.002975780051\n",
      "training step: 383473, loss:  0.004087368958\n",
      "training step: 383514, loss:  0.002386306180\n",
      "training step: 383555, loss:  0.002975780051\n",
      "training step: 383596, loss:  0.003135976614\n",
      "training step: 383637, loss:  0.024754593149\n",
      "training step: 383678, loss:  0.002930165734\n",
      "training step: 383719, loss:  0.004080093466\n",
      "training step: 383760, loss:  0.003467082279\n",
      "training step: 383801, loss:  0.007877553813\n",
      "training step: 383842, loss:  0.002930165734\n",
      "training step: 383883, loss:  0.007341358811\n",
      "training step: 383924, loss:  0.005003615282\n",
      "training step: 383965, loss:  0.003638759954\n",
      "training step: 384006, loss:  0.004080093466\n",
      "training step: 384047, loss:  0.016485802829\n",
      "training step: 384088, loss:  0.003692979924\n",
      "training step: 384129, loss:  0.005715856329\n",
      "training step: 384170, loss:  0.011776376516\n",
      "training step: 384211, loss:  0.007341358811\n",
      "training step: 384252, loss:  0.002396283671\n",
      "training step: 384293, loss:  0.007341358811\n",
      "training step: 384334, loss:  0.003120336682\n",
      "training step: 384375, loss:  0.003986798227\n",
      "training step: 384416, loss:  0.002386306180\n",
      "training step: 384457, loss:  0.007752379868\n",
      "training step: 384498, loss:  0.004080093466\n",
      "training step: 384539, loss:  0.023757282645\n",
      "training step: 384580, loss:  0.011776376516\n",
      "training step: 384621, loss:  0.002386306180\n",
      "training step: 384662, loss:  0.007341358811\n",
      "training step: 384703, loss:  0.024754593149\n",
      "training step: 384744, loss:  0.020691098645\n",
      "training step: 384785, loss:  0.002386306180\n",
      "training step: 384826, loss:  0.003473303979\n",
      "training step: 384867, loss:  0.003808671376\n",
      "training step: 384908, loss:  0.018698843196\n",
      "training step: 384949, loss:  0.003638759954\n",
      "training step: 384990, loss:  0.003880497534\n",
      "training step: 385031, loss:  0.002975780051\n",
      "training step: 385072, loss:  0.024754593149\n",
      "training step: 385113, loss:  0.002975780051\n",
      "training step: 385154, loss:  0.009004401974\n",
      "training step: 385195, loss:  0.003880497534\n",
      "training step: 385236, loss:  0.011776376516\n",
      "training step: 385277, loss:  0.003808671376\n",
      "training step: 385318, loss:  0.003002761398\n",
      "training step: 385359, loss:  0.011776376516\n",
      "training step: 385400, loss:  0.009473215789\n",
      "training step: 385441, loss:  0.003880497534\n",
      "training step: 385482, loss:  0.004087368958\n",
      "training step: 385523, loss:  0.007341358811\n",
      "training step: 385564, loss:  0.016485802829\n",
      "training step: 385605, loss:  0.006285338197\n",
      "training step: 385646, loss:  0.011776376516\n",
      "training step: 385687, loss:  0.020691098645\n",
      "training step: 385728, loss:  0.003880497534\n",
      "training step: 385769, loss:  0.007752379868\n",
      "training step: 385810, loss:  0.016485802829\n",
      "training step: 385851, loss:  0.002975780051\n",
      "training step: 385892, loss:  0.023757282645\n",
      "training step: 385933, loss:  0.009473215789\n",
      "training step: 385974, loss:  0.007877553813\n",
      "training step: 386015, loss:  0.004080093466\n",
      "training step: 386056, loss:  0.002386306180\n",
      "training step: 386097, loss:  0.018698843196\n",
      "training step: 386138, loss:  0.005715856329\n",
      "training step: 386179, loss:  0.025363046676\n",
      "training step: 386220, loss:  0.003543942003\n",
      "training step: 386261, loss:  0.005715856329\n",
      "training step: 386302, loss:  0.004087368958\n",
      "training step: 386343, loss:  0.002975780051\n",
      "training step: 386384, loss:  0.002396283671\n",
      "training step: 386425, loss:  0.003135976614\n",
      "training step: 386466, loss:  0.003135976614\n",
      "training step: 386507, loss:  0.003692979924\n",
      "training step: 386548, loss:  0.020691098645\n",
      "training step: 386589, loss:  0.025568028912\n",
      "training step: 386630, loss:  0.003866109066\n",
      "training step: 386671, loss:  0.007877553813\n",
      "training step: 386712, loss:  0.003986798227\n",
      "training step: 386753, loss:  0.003284437582\n",
      "training step: 386794, loss:  0.007752379868\n",
      "training step: 386835, loss:  0.024754593149\n",
      "training step: 386876, loss:  0.003776642960\n",
      "training step: 386917, loss:  0.003638759954\n",
      "training step: 386958, loss:  0.007341358811\n",
      "training step: 386999, loss:  0.003135976614\n",
      "training step: 387040, loss:  0.024754593149\n",
      "training step: 387081, loss:  0.004087368958\n",
      "training step: 387122, loss:  0.009004401974\n",
      "training step: 387163, loss:  0.007877553813\n",
      "training step: 387204, loss:  0.003543942003\n",
      "training step: 387245, loss:  0.003986798227\n",
      "training step: 387286, loss:  0.005715856329\n",
      "training step: 387327, loss:  0.006285338197\n",
      "training step: 387368, loss:  0.005485921632\n",
      "training step: 387409, loss:  0.003473303979\n",
      "training step: 387450, loss:  0.003467082279\n",
      "training step: 387491, loss:  0.003120336682\n",
      "training step: 387532, loss:  0.009004401974\n",
      "training step: 387573, loss:  0.003135976614\n",
      "training step: 387614, loss:  0.002930165734\n",
      "training step: 387655, loss:  0.003808671376\n",
      "training step: 387696, loss:  0.003808671376\n",
      "training step: 387737, loss:  0.003543942003\n",
      "training step: 387778, loss:  0.003880497534\n",
      "training step: 387819, loss:  0.003120336682\n",
      "training step: 387860, loss:  0.003808671376\n",
      "training step: 387901, loss:  0.003692979924\n",
      "training step: 387942, loss:  0.003638759954\n",
      "training step: 387983, loss:  0.004087368958\n",
      "training step: 388024, loss:  0.003135976614\n",
      "training step: 388065, loss:  0.024754593149\n",
      "training step: 388106, loss:  0.006285338197\n",
      "training step: 388147, loss:  0.009004401974\n",
      "training step: 388188, loss:  0.007877553813\n",
      "training step: 388229, loss:  0.003880497534\n",
      "training step: 388270, loss:  0.005715856329\n",
      "training step: 388311, loss:  0.003467082279\n",
      "training step: 388352, loss:  0.009473215789\n",
      "training step: 388393, loss:  0.009004401974\n",
      "training step: 388434, loss:  0.002930165734\n",
      "training step: 388475, loss:  0.002975780051\n",
      "training step: 388516, loss:  0.002396283671\n",
      "training step: 388557, loss:  0.003986798227\n",
      "training step: 388598, loss:  0.003638759954\n",
      "training step: 388639, loss:  0.003986798227\n",
      "training step: 388680, loss:  0.003692979924\n",
      "training step: 388721, loss:  0.020691098645\n",
      "training step: 388762, loss:  0.003120336682\n",
      "training step: 388803, loss:  0.003473303979\n",
      "training step: 388844, loss:  0.011776376516\n",
      "training step: 388885, loss:  0.005485921632\n",
      "training step: 388926, loss:  0.003986798227\n",
      "training step: 388967, loss:  0.022392421961\n",
      "training step: 389008, loss:  0.003284437582\n",
      "training step: 389049, loss:  0.002386306180\n",
      "training step: 389090, loss:  0.025363046676\n",
      "training step: 389131, loss:  0.014145905152\n",
      "training step: 389172, loss:  0.002386306180\n",
      "training step: 389213, loss:  0.003692979924\n",
      "training step: 389254, loss:  0.007752379868\n",
      "training step: 389295, loss:  0.002396283671\n",
      "training step: 389336, loss:  0.003986798227\n",
      "training step: 389377, loss:  0.003467082279\n",
      "training step: 389418, loss:  0.009473215789\n",
      "training step: 389459, loss:  0.024754593149\n",
      "training step: 389500, loss:  0.005485921632\n",
      "training step: 389541, loss:  0.003986798227\n",
      "training step: 389582, loss:  0.007752379868\n",
      "training step: 389623, loss:  0.002396283671\n",
      "training step: 389664, loss:  0.003808671376\n",
      "training step: 389705, loss:  0.023757282645\n",
      "training step: 389746, loss:  0.020691098645\n",
      "training step: 389787, loss:  0.007877553813\n",
      "training step: 389828, loss:  0.025363046676\n",
      "training step: 389869, loss:  0.003692979924\n",
      "training step: 389910, loss:  0.003473303979\n",
      "training step: 389951, loss:  0.018698843196\n",
      "training step: 389992, loss:  0.003002761398\n",
      "training step: 390033, loss:  0.003135976614\n",
      "training step: 390074, loss:  0.023757282645\n",
      "training step: 390115, loss:  0.004087368958\n",
      "training step: 390156, loss:  0.005003615282\n",
      "training step: 390197, loss:  0.009004401974\n",
      "training step: 390238, loss:  0.005003615282\n",
      "training step: 390279, loss:  0.003638759954\n",
      "training step: 390320, loss:  0.003002761398\n",
      "training step: 390361, loss:  0.024754593149\n",
      "training step: 390402, loss:  0.003002761398\n",
      "training step: 390443, loss:  0.007877553813\n",
      "training step: 390484, loss:  0.002386306180\n",
      "training step: 390525, loss:  0.003986798227\n",
      "training step: 390566, loss:  0.003135976614\n",
      "training step: 390607, loss:  0.003120336682\n",
      "training step: 390648, loss:  0.011776376516\n",
      "training step: 390689, loss:  0.003900909098\n",
      "training step: 390730, loss:  0.025363046676\n",
      "training step: 390771, loss:  0.025568028912\n",
      "training step: 390812, loss:  0.003900909098\n",
      "training step: 390853, loss:  0.011776376516\n",
      "training step: 390894, loss:  0.003900909098\n",
      "training step: 390935, loss:  0.003473303979\n",
      "training step: 390976, loss:  0.003638759954\n",
      "training step: 391017, loss:  0.002396283671\n",
      "training step: 391058, loss:  0.003002761398\n",
      "training step: 391099, loss:  0.003011468332\n",
      "training step: 391140, loss:  0.002396283671\n",
      "training step: 391181, loss:  0.003120336682\n",
      "training step: 391222, loss:  0.003692979924\n",
      "training step: 391263, loss:  0.007877553813\n",
      "training step: 391304, loss:  0.003866109066\n",
      "training step: 391345, loss:  0.024754593149\n",
      "training step: 391386, loss:  0.003543942003\n",
      "training step: 391427, loss:  0.003473303979\n",
      "training step: 391468, loss:  0.003002761398\n",
      "training step: 391509, loss:  0.025568028912\n",
      "training step: 391550, loss:  0.018698843196\n",
      "training step: 391591, loss:  0.014145905152\n",
      "training step: 391632, loss:  0.002386306180\n",
      "training step: 391673, loss:  0.025363046676\n",
      "training step: 391714, loss:  0.003002761398\n",
      "training step: 391755, loss:  0.003467082279\n",
      "training step: 391796, loss:  0.003284437582\n",
      "training step: 391837, loss:  0.007752379868\n",
      "training step: 391878, loss:  0.007341358811\n",
      "training step: 391919, loss:  0.003692979924\n",
      "training step: 391960, loss:  0.007877553813\n",
      "training step: 392001, loss:  0.003900909098\n",
      "training step: 392042, loss:  0.004087368958\n",
      "training step: 392083, loss:  0.003986798227\n",
      "training step: 392124, loss:  0.014145905152\n",
      "training step: 392165, loss:  0.003866109066\n",
      "training step: 392206, loss:  0.003473303979\n",
      "training step: 392247, loss:  0.003986798227\n",
      "training step: 392288, loss:  0.025363046676\n",
      "training step: 392329, loss:  0.003120336682\n",
      "training step: 392370, loss:  0.005485921632\n",
      "training step: 392411, loss:  0.025363046676\n",
      "training step: 392452, loss:  0.004087368958\n",
      "training step: 392493, loss:  0.003473303979\n",
      "training step: 392534, loss:  0.003866109066\n",
      "training step: 392575, loss:  0.003808671376\n",
      "training step: 392616, loss:  0.022392421961\n",
      "training step: 392657, loss:  0.014145905152\n",
      "training step: 392698, loss:  0.003866109066\n",
      "training step: 392739, loss:  0.014145905152\n",
      "training step: 392780, loss:  0.005003615282\n",
      "training step: 392821, loss:  0.023757282645\n",
      "training step: 392862, loss:  0.016485802829\n",
      "training step: 392903, loss:  0.003900909098\n",
      "training step: 392944, loss:  0.003900909098\n",
      "training step: 392985, loss:  0.003473303979\n",
      "training step: 393026, loss:  0.003543942003\n",
      "training step: 393067, loss:  0.003692979924\n",
      "training step: 393108, loss:  0.009004401974\n",
      "training step: 393149, loss:  0.022392421961\n",
      "training step: 393190, loss:  0.003120336682\n",
      "training step: 393231, loss:  0.003135976614\n",
      "training step: 393272, loss:  0.003808671376\n",
      "training step: 393313, loss:  0.024754593149\n",
      "training step: 393354, loss:  0.003776642960\n",
      "training step: 393395, loss:  0.004087368958\n",
      "training step: 393436, loss:  0.003284437582\n",
      "training step: 393477, loss:  0.003692979924\n",
      "training step: 393518, loss:  0.003692979924\n",
      "training step: 393559, loss:  0.007877553813\n",
      "training step: 393600, loss:  0.020691098645\n",
      "training step: 393641, loss:  0.003543942003\n",
      "training step: 393682, loss:  0.003900909098\n",
      "training step: 393723, loss:  0.002975780051\n",
      "training step: 393764, loss:  0.005003615282\n",
      "training step: 393805, loss:  0.002930165734\n",
      "training step: 393846, loss:  0.016485802829\n",
      "training step: 393887, loss:  0.018698843196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "setting = {'activation':'tanh', 'nn_shape':(256,256,256), 'batch_size':41, 'training_steps':500000,\\\n",
    "'learning_rate': 0.0005, 'decay_rate':0.99, 'decay_per_steps':41, 'save_step':41, 'drop_rate':0, 'save_path':'./save2/model',\\\n",
    "'seed':None, 'debug_traj': True}\n",
    "NN = nn.NN(setting_dict=setting)\n",
    "NN.train(train_homo_pairs,train_c_homo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b143ab74-322f-4b26-9ee5-7f7d433b2074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6774685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'pred')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4efc8b0fd134ba2ae2bbfe61860e463",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "print(np.mean((NN.model(train_homo_pairs, training=False)-c_homo)/c_homo))\n",
    "x = np.linspace(0, 4, 41)\n",
    "y = np.linspace(0, 4, 41)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = NN.model(train_homo_pairs, training=False).numpy().reshape((41,41))\n",
    "Z1 = c_homo.reshape((41,41))\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "ax.contourf(x,y, Z)\n",
    "ax.set_title('pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbeac264-1b61-4a8a-bb30-b368f5551f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " concatenate (Concatenate)   multiple                  0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  multiple                 1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  multiple                 1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  multiple                 1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  65792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 200,705\n",
      "Trainable params: 199,169\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03c3106-5eca-4313-b014-b33b23c558f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.404766 11.926116  6.404766 11.926116  6.404766 11.926116  6.404766\n",
      " 11.926116]\n"
     ]
    }
   ],
   "source": [
    "print(homo[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ace6c2-030a-4f0d-aef2-2440da10b435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
