{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65900bf3-c182-48a6-b6a0-efd08e340f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-28 21:57:20.597786: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-28 21:57:20.597915: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='export TF_INTRA_OP_PARALLELISM_THREADS=12', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mo_descriptor as md\n",
    "import nn_frame as nn\n",
    "import numpy as np\n",
    "import subprocess\n",
    "subprocess.run('export TF_INTRA_OP_PARALLELISM_THREADS=12', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3819da32-6769-405e-8d3f-1fa9c336c9b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "prepare data_set\n",
    "1. make mo_pair descriptor\n",
    "'''\n",
    "# x_shift = np.arange(0, 4.1, 0.1)\n",
    "# y_shift = np.arange(0, 4.1, 0.1)\n",
    "# z_shift = np.zeros(x_shift.shape)\n",
    "# # the original mo, e.g. homo\n",
    "# homo = md.MO_descriptor('data/homo-s0.cube').make()\n",
    "# lumo = md.MO_descriptor('data/lumo-s0.cube').make()\n",
    "\n",
    "# # for the original pair of one mo and itself\n",
    "# homo_pair = md.MO_pair_descriptor(homo, homo).make()\n",
    "# lumo_pair = md.MO_pair_descriptor(lumo, lumo).make()\n",
    "\n",
    "# homo_pairs = np.zeros((len(x_shift)*len(y_shift),) + homo_pair.shape)\n",
    "# lumo_pairs = np.zeros((len(x_shift)*len(y_shift),) + lumo_pair.shape)\n",
    "\n",
    "# homo_ = np.zeros(homo.shape)\n",
    "# lumo_ = np.zeros(lumo.shape)\n",
    "\n",
    "# for ii, i in enumerate(x_shift):\n",
    "#     for jj, j in enumerate(y_shift):\n",
    "#         idx = ii * len(y_shift) + jj\n",
    "#         homo_[:,0] = np.add(homo[:,0],0)\n",
    "#         homo_[:,1] = np.add(homo[:,1],i)\n",
    "#         homo_[:,2] = np.add(homo[:,2],j)\n",
    "#         homo_[:,3] = np.add(homo[:,3],3.5)\n",
    "        \n",
    "#         homo_pair_ = md.MO_pair_descriptor(homo, homo_).make()\n",
    "#         homo_pairs[idx] = homo_pair_\n",
    "        \n",
    "#         lumo_[:,0] = np.add(lumo[:,0],0)\n",
    "#         lumo_[:,1] = np.add(lumo[:,1],i)\n",
    "#         lumo_[:,2] = np.add(lumo[:,2],j)\n",
    "#         lumo_[:,3] = np.add(lumo[:,3],3.5)\n",
    "        \n",
    "#         lumo_pair_ = md.MO_pair_descriptor(lumo, lumo_).make()\n",
    "#         lumo_pairs[idx] = lumo_pair_\n",
    "        \n",
    "# def dir_mat(mat):\n",
    "#     mat_shape = mat.shape\n",
    "#     mat_ = mat.flatten()\n",
    "#     for ii, i in enumerate(mat_):\n",
    "#         if i > 1e-6:\n",
    "#             mat_[ii] = 1\n",
    "#         elif (i < 1e-6) and (i > -1e-6):\n",
    "#             mat_[ii] = -1\n",
    "#         elif i < -1e-6:\n",
    "#             mat_[ii] = -1\n",
    "#     return mat_.reshape(mat_shape)\n",
    "\n",
    "# direct = dir_mat(homo_pair)\n",
    "\n",
    "# # for the shifted pair\n",
    "# homo_pairs = np.zeros((len(x_shift)*len(y_shift),) + homo_pair.shape)\n",
    "# lumo_pairs = np.zeros((len(x_shift)*len(y_shift),) + lumo_pair.shape)\n",
    "# for ii, i in enumerate(x_shift):\n",
    "#     for jj, j in enumerate(y_shift):\n",
    "#         idx = ii * len(y_shift) + jj\n",
    "#         homo_pairs[idx][0] = homo_pair[0]\n",
    "#         homo_pairs[idx][1] = np.add(homo_pair[1],i*direct[1])\n",
    "#         homo_pairs[idx][2] = np.add(homo_pair[2],j*direct[2])\n",
    "#         homo_pairs[idx][3] = homo_pair[3]\n",
    "#         lumo_pairs[idx][0] = lumo_pair[0]\n",
    "#         lumo_pairs[idx][1] = np.add(lumo_pair[1],i)\n",
    "#         lumo_pairs[idx][2] = np.add(lumo_pair[2],j)\n",
    "#         lumo_pairs[idx][3] = lumo_pair[3]\n",
    "# np.save('homo_homo_pair.npy', homo_pairs)\n",
    "# np.save('lumo_lumo_pair.npy', lumo_pairs)\n",
    "homo_pairs = np.load('homo_homo_pair.npy')\n",
    "lumo_pairs = np.load('lumo_lumo_pair.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91a92a2-f076-46c8-835f-958569393703",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. read coupling\n",
    "'''\n",
    "raw_data = np.loadtxt('../ML_Coupling/results.csv', delimiter=',',comments='#')\n",
    "# c_homo = np.add(raw_data[:,2], raw_data[:,3]) * 1/2\n",
    "# c_lumo = np.add(raw_data[:,4], raw_data[:,5]) * 1/2\n",
    "c_homo = abs(raw_data[:,3])\n",
    "c_lumo = abs(raw_data[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda7ef17-1e21-42a4-9daf-27feba4fc51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x1 = homo_pairs[:,0,:,:]\n",
    "# x2 = homo_pairs[:,1,:,:]\n",
    "# x3 = homo_pairs[:,2,:,:]\n",
    "# x4 = homo_pairs[:,3,:,:]\n",
    "# x = np.einsum('aij,aij,aij,aij->aij', x1, x2, x3, x4)\n",
    "\n",
    "train_homo_pairs = homo_pairs\n",
    "train_lumo_pairs = lumo_pairs\n",
    "\n",
    "train_c_homo = -np.log(c_homo)\n",
    "train_c_lumo = -np.log(c_lumo)\n",
    "\n",
    "test_homo_pairs = homo_pairs[1200:]\n",
    "test_lumo_pairs = lumo_pairs[1200:]\n",
    "\n",
    "test_c_homo = c_homo[1200:].reshape((len(c_homo[1200:]),1))\n",
    "test_c_lumo = c_lumo[1200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70cd9854-4918-4889-be28-ef85dd8c9379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "train_homo = copy.deepcopy(train_homo_pairs)\n",
    "train_chomo = copy.deepcopy(train_c_homo)\n",
    "print(len(train_chomo))\n",
    "index = np.random.choice(1681,size=481, replace=False)\n",
    "train_homo_ = np.delete(train_homo,index,0)\n",
    "train_chomo_ = np.delete(train_chomo,index,0)\n",
    "print(len(train_chomo_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d30ba-e044-469f-b2eb-25f05497f346",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-28 21:57:22.311661: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-10-28 21:57:22.311785: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Deng-PC): /proc/driver/nvidia/version does not exist\n",
      "2022-10-28 21:57:22.312357: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  41.7953339\n",
      "training step:     0\n",
      "loss:  36.6198235\n",
      "training step:   100\n",
      "loss:  36.2871704\n",
      "training step:   200\n",
      "loss:  36.0032349\n",
      "training step:   300\n",
      "loss:  35.7298813\n",
      "training step:   400\n",
      "loss:  35.4590683\n",
      "training step:   500\n",
      "loss:  35.1882591\n",
      "training step:   600\n",
      "loss:  34.9155846\n",
      "training step:   700\n",
      "loss:  34.6392\n",
      "training step:   800\n",
      "loss:  34.3566742\n",
      "training step:   900\n",
      "loss:  34.06847\n",
      "training step:  1000\n",
      "loss:  33.7735786\n",
      "training step:  1100\n",
      "loss:  33.4712257\n",
      "training step:  1200\n",
      "loss:  33.1608925\n",
      "training step:  1300\n",
      "loss:  32.8424492\n",
      "training step:  1400\n",
      "loss:  32.5160561\n",
      "training step:  1500\n",
      "loss:  32.1817207\n",
      "training step:  1600\n",
      "loss:  31.8395252\n",
      "training step:  1700\n",
      "loss:  31.489851\n",
      "training step:  1800\n",
      "loss:  31.1330051\n",
      "training step:  1900\n",
      "loss:  30.7692375\n",
      "training step:  2000\n",
      "loss:  30.3990421\n",
      "training step:  2100\n",
      "loss:  30.0228901\n",
      "training step:  2200\n",
      "loss:  29.6411858\n",
      "training step:  2300\n",
      "loss:  29.2543106\n",
      "training step:  2400\n",
      "loss:  28.862709\n",
      "training step:  2500\n",
      "loss:  28.4666595\n",
      "training step:  2600\n",
      "loss:  28.0664978\n",
      "training step:  2700\n",
      "loss:  27.6626434\n",
      "training step:  2800\n",
      "loss:  27.2551785\n",
      "training step:  2900\n",
      "loss:  26.8446827\n",
      "training step:  3000\n",
      "loss:  26.4314785\n",
      "training step:  3100\n",
      "loss:  26.0156307\n",
      "training step:  3200\n",
      "loss:  25.5974388\n",
      "training step:  3300\n",
      "loss:  25.1773586\n",
      "training step:  3400\n",
      "loss:  24.7556114\n",
      "training step:  3500\n",
      "loss:  24.3322468\n",
      "training step:  3600\n",
      "loss:  23.9077053\n",
      "training step:  3700\n",
      "loss:  23.4822063\n",
      "training step:  3800\n",
      "loss:  23.0557194\n",
      "training step:  3900\n",
      "loss:  22.6287346\n",
      "training step:  4000\n",
      "loss:  22.2013092\n",
      "training step:  4100\n",
      "loss:  21.7737217\n",
      "training step:  4200\n",
      "loss:  21.3461323\n",
      "training step:  4300\n",
      "loss:  20.9187374\n",
      "training step:  4400\n",
      "loss:  20.4917145\n",
      "training step:  4500\n",
      "loss:  20.0652466\n",
      "training step:  4600\n",
      "loss:  19.6395226\n",
      "training step:  4700\n",
      "loss:  19.2147694\n",
      "training step:  4800\n",
      "loss:  18.791153\n",
      "training step:  4900\n",
      "loss:  18.3687935\n",
      "training step:  5000\n",
      "loss:  17.9478817\n",
      "training step:  5100\n",
      "loss:  17.5286617\n",
      "training step:  5200\n",
      "loss:  17.1112747\n",
      "training step:  5300\n",
      "loss:  16.6958885\n",
      "training step:  5400\n",
      "loss:  16.2825985\n",
      "training step:  5500\n",
      "loss:  15.8716898\n",
      "training step:  5600\n",
      "loss:  15.4632711\n",
      "training step:  5700\n",
      "loss:  15.0575199\n",
      "training step:  5800\n",
      "loss:  14.6546288\n",
      "training step:  5900\n",
      "loss:  14.2568636\n",
      "training step:  6000\n",
      "loss:  13.8578548\n",
      "training step:  6100\n",
      "loss:  13.464345\n",
      "training step:  6200\n",
      "loss:  13.074317\n",
      "training step:  6300\n",
      "loss:  12.6890545\n",
      "training step:  6400\n",
      "loss:  12.3060493\n",
      "training step:  6500\n",
      "loss:  11.926816\n",
      "training step:  6600\n",
      "loss:  11.5518847\n",
      "training step:  6700\n",
      "loss:  11.1814632\n",
      "training step:  6800\n",
      "loss:  10.8163977\n",
      "training step:  6900\n",
      "loss:  10.4538088\n",
      "training step:  7000\n",
      "loss:  10.0968981\n",
      "training step:  7100\n",
      "loss:  9.74483681\n",
      "training step:  7200\n",
      "loss:  9.39763165\n",
      "training step:  7300\n",
      "loss:  9.05555153\n",
      "training step:  7400\n",
      "loss:  8.71863937\n",
      "training step:  7500\n",
      "loss:  8.38707829\n",
      "training step:  7600\n",
      "loss:  8.06099415\n",
      "training step:  7700\n",
      "loss:  7.74048\n",
      "training step:  7800\n",
      "loss:  7.42570782\n",
      "training step:  7900\n",
      "loss:  7.11672783\n",
      "training step:  8000\n",
      "loss:  6.8137269\n",
      "training step:  8100\n",
      "loss:  6.51678562\n",
      "training step:  8200\n",
      "loss:  6.22602654\n",
      "training step:  8300\n",
      "loss:  5.94159412\n",
      "training step:  8400\n",
      "loss:  5.66346\n",
      "training step:  8500\n",
      "loss:  5.39185762\n",
      "training step:  8600\n",
      "loss:  5.12685251\n",
      "training step:  8700\n",
      "loss:  4.8700366\n",
      "training step:  8800\n",
      "loss:  4.61681509\n",
      "training step:  8900\n",
      "loss:  4.37199116\n",
      "training step:  9000\n",
      "loss:  4.13408136\n",
      "training step:  9100\n",
      "loss:  3.90314364\n",
      "training step:  9200\n",
      "loss:  3.67928314\n",
      "training step:  9300\n",
      "loss:  3.46249175\n",
      "training step:  9400\n",
      "loss:  3.25271153\n",
      "training step:  9500\n",
      "loss:  3.0502677\n",
      "training step:  9600\n",
      "loss:  2.85484123\n",
      "training step:  9700\n",
      "loss:  2.66673946\n",
      "training step:  9800\n",
      "loss:  2.48588586\n",
      "training step:  9900\n",
      "loss:  2.31227255\n",
      "training step: 10000\n",
      "loss:  2.14633489\n",
      "training step: 10100\n",
      "loss:  1.98680079\n",
      "training step: 10200\n",
      "loss:  1.83529\n",
      "training step: 10300\n",
      "loss:  1.69020212\n",
      "training step: 10400\n",
      "loss:  1.5526377\n",
      "training step: 10500\n",
      "loss:  1.4221493\n",
      "training step: 10600\n",
      "loss:  1.29866683\n",
      "training step: 10700\n",
      "loss:  1.18215883\n",
      "training step: 10800\n",
      "loss:  1.07246161\n",
      "training step: 10900\n",
      "loss:  0.969531238\n",
      "training step: 11000\n",
      "loss:  0.873220921\n",
      "training step: 11100\n",
      "loss:  0.783416331\n",
      "training step: 11200\n",
      "loss:  0.699954629\n",
      "training step: 11300\n",
      "loss:  0.622675478\n",
      "training step: 11400\n",
      "loss:  0.551374912\n",
      "training step: 11500\n",
      "loss:  0.486771554\n",
      "training step: 11600\n",
      "loss:  0.426465958\n",
      "training step: 11700\n",
      "loss:  0.371544\n",
      "training step: 11800\n",
      "loss:  0.322225153\n",
      "training step: 11900\n",
      "loss:  0.277838528\n",
      "training step: 12000\n",
      "loss:  0.238145068\n",
      "training step: 12100\n",
      "loss:  0.202830464\n",
      "training step: 12200\n",
      "loss:  0.171594381\n",
      "training step: 12300\n",
      "loss:  0.14420037\n",
      "training step: 12400\n",
      "loss:  0.120520018\n",
      "training step: 12500\n",
      "loss:  0.0997844413\n",
      "training step: 12600\n",
      "loss:  0.0824372768\n",
      "training step: 12700\n",
      "loss:  0.0672910437\n",
      "training step: 12800\n",
      "loss:  0.0547384396\n",
      "training step: 12900\n",
      "loss:  0.0448065475\n",
      "training step: 13000\n",
      "loss:  0.0357772559\n",
      "training step: 13100\n",
      "loss:  0.0291807745\n",
      "training step: 13200\n",
      "loss:  0.0234421473\n",
      "training step: 13300\n",
      "loss:  0.0191127807\n",
      "training step: 13400\n",
      "loss:  0.0154932467\n",
      "training step: 13500\n",
      "loss:  0.0133610312\n",
      "training step: 13600\n",
      "loss:  0.011325473\n",
      "training step: 13700\n",
      "loss:  0.00938160345\n",
      "training step: 13800\n",
      "loss:  0.00819031522\n",
      "training step: 13900\n",
      "loss:  0.00733369729\n",
      "training step: 14000\n",
      "loss:  0.00792489\n",
      "training step: 14100\n",
      "loss:  0.00636352785\n",
      "training step: 14200\n",
      "loss:  0.00602842215\n",
      "training step: 14300\n",
      "loss:  0.00573010836\n",
      "training step: 14400\n",
      "loss:  0.00554389739\n",
      "training step: 14500\n",
      "loss:  0.00550122745\n",
      "training step: 14600\n",
      "loss:  0.005570109\n",
      "training step: 14700\n",
      "loss:  0.00516632386\n",
      "training step: 14800\n",
      "loss:  0.00511909742\n",
      "training step: 14900\n",
      "loss:  0.0050301319\n",
      "training step: 15000\n",
      "loss:  0.00504213478\n",
      "training step: 15100\n",
      "loss:  0.00512954453\n",
      "training step: 15200\n",
      "loss:  0.0050942637\n",
      "training step: 15300\n",
      "loss:  0.0049983738\n",
      "training step: 15400\n",
      "loss:  0.00556966243\n",
      "training step: 15500\n",
      "loss:  0.00452466728\n",
      "training step: 15600\n",
      "loss:  0.0048500062\n",
      "training step: 15700\n",
      "loss:  0.00440309476\n",
      "training step: 15800\n",
      "loss:  0.00433052238\n",
      "training step: 15900\n",
      "loss:  0.00420801714\n",
      "training step: 16000\n",
      "loss:  0.00648759492\n",
      "training step: 16100\n",
      "loss:  0.00407495024\n",
      "training step: 16200\n",
      "loss:  0.00487196678\n",
      "training step: 16300\n",
      "loss:  0.00396935921\n",
      "training step: 16400\n",
      "loss:  0.00392708\n",
      "training step: 16500\n",
      "loss:  0.00400230382\n",
      "training step: 16600\n",
      "loss:  0.00379800238\n",
      "training step: 16700\n",
      "loss:  0.00382220908\n",
      "training step: 16800\n",
      "loss:  0.00373972184\n",
      "training step: 16900\n",
      "loss:  0.0036380759\n",
      "training step: 17000\n",
      "loss:  0.00355308456\n",
      "training step: 17100\n",
      "loss:  0.00352979335\n",
      "training step: 17200\n",
      "loss:  0.00349815\n",
      "training step: 17300\n",
      "loss:  0.00351619325\n",
      "training step: 17400\n",
      "loss:  0.00451050512\n",
      "training step: 17500\n",
      "loss:  0.00358336698\n",
      "training step: 17600\n",
      "loss:  0.00336034014\n",
      "training step: 17700\n",
      "loss:  0.00319037726\n",
      "training step: 17800\n",
      "loss:  0.00312065589\n",
      "training step: 17900\n",
      "loss:  0.00315112621\n",
      "training step: 18000\n",
      "loss:  0.00306578586\n",
      "training step: 18100\n",
      "loss:  0.0030243895\n",
      "training step: 18200\n",
      "loss:  0.00329629611\n",
      "training step: 18300\n",
      "loss:  0.00293538859\n",
      "training step: 18400\n",
      "loss:  0.00291187712\n",
      "training step: 18500\n",
      "loss:  0.00282003498\n",
      "training step: 18600\n",
      "loss:  0.00276833936\n",
      "training step: 18700\n",
      "loss:  0.00273557706\n",
      "training step: 18800\n",
      "loss:  0.00269261911\n",
      "training step: 18900\n",
      "loss:  0.00265299552\n",
      "training step: 19000\n",
      "loss:  0.00264248718\n",
      "training step: 19100\n",
      "loss:  0.00363963121\n",
      "training step: 19200\n",
      "loss:  0.00411324203\n",
      "training step: 19300\n",
      "loss:  0.00249447767\n",
      "training step: 19400\n",
      "loss:  0.00263229012\n",
      "training step: 19500\n",
      "loss:  0.00244351267\n",
      "training step: 19600\n",
      "loss:  0.00239653816\n",
      "training step: 19700\n",
      "loss:  0.0028102689\n",
      "training step: 19800\n",
      "loss:  0.00258521573\n",
      "training step: 19900\n",
      "loss:  0.00247420208\n",
      "training step: 20000\n",
      "loss:  0.00231027394\n",
      "training step: 20100\n",
      "loss:  0.00224263617\n",
      "training step: 20200\n",
      "loss:  0.00219597109\n",
      "training step: 20300\n",
      "loss:  0.00221417588\n",
      "training step: 20400\n",
      "loss:  0.00215574354\n",
      "training step: 20500\n",
      "loss:  0.00211246777\n",
      "training step: 20600\n",
      "loss:  0.00253712805\n",
      "training step: 20700\n",
      "loss:  0.00208621286\n",
      "training step: 20800\n",
      "loss:  0.00206378382\n",
      "training step: 20900\n",
      "loss:  0.00202047965\n",
      "training step: 21000\n",
      "loss:  0.00216991454\n",
      "training step: 21100\n",
      "loss:  0.00207068445\n",
      "training step: 21200\n",
      "loss:  0.00197530398\n",
      "training step: 21300\n",
      "loss:  0.00189934613\n",
      "training step: 21400\n",
      "loss:  0.00190707727\n",
      "training step: 21500\n",
      "loss:  0.00208821893\n",
      "training step: 21600\n",
      "loss:  0.00182312308\n",
      "training step: 21700\n",
      "loss:  0.00179865642\n",
      "training step: 21800\n",
      "loss:  0.00177741051\n",
      "training step: 21900\n",
      "loss:  0.00175563688\n",
      "training step: 22000\n",
      "loss:  0.00182508351\n",
      "training step: 22100\n",
      "loss:  0.00205069571\n",
      "training step: 22200\n",
      "loss:  0.00174358289\n",
      "training step: 22300\n",
      "loss:  0.0017563065\n",
      "training step: 22400\n",
      "loss:  0.00165489421\n",
      "training step: 22500\n",
      "loss:  0.00164598599\n",
      "training step: 22600\n",
      "loss:  0.00161271018\n",
      "training step: 22700\n",
      "loss:  0.0015960536\n",
      "training step: 22800\n",
      "loss:  0.0015746986\n",
      "training step: 22900\n",
      "loss:  0.00162092643\n",
      "training step: 23000\n",
      "loss:  0.00187955343\n",
      "training step: 23100\n",
      "loss:  0.00159147452\n",
      "training step: 23200\n",
      "loss:  0.00150613964\n",
      "training step: 23300\n",
      "loss:  0.00149937032\n",
      "training step: 23400\n",
      "loss:  0.00195642863\n",
      "training step: 23500\n",
      "loss:  0.00152142323\n",
      "training step: 23600\n",
      "loss:  0.00144326582\n",
      "training step: 23700\n",
      "loss:  0.00154584355\n",
      "training step: 23800\n",
      "loss:  0.00141372567\n",
      "training step: 23900\n",
      "loss:  0.00139895035\n",
      "training step: 24000\n",
      "loss:  0.00152881327\n",
      "training step: 24100\n",
      "loss:  0.00139329804\n",
      "training step: 24200\n",
      "loss:  0.0014563906\n",
      "training step: 24300\n",
      "loss:  0.00136001466\n",
      "training step: 24400\n",
      "loss:  0.00132047257\n",
      "training step: 24500\n",
      "loss:  0.00131183572\n",
      "training step: 24600\n",
      "loss:  0.00128942565\n",
      "training step: 24700\n",
      "loss:  0.00127621135\n",
      "training step: 24800\n",
      "loss:  0.0022817438\n",
      "training step: 24900\n",
      "loss:  0.0012494002\n",
      "training step: 25000\n",
      "loss:  0.00124487025\n",
      "training step: 25100\n",
      "loss:  0.00123984693\n",
      "training step: 25200\n",
      "loss:  0.00143851351\n",
      "training step: 25300\n",
      "loss:  0.00126783014\n",
      "training step: 25400\n",
      "loss:  0.00120637415\n",
      "training step: 25500\n",
      "loss:  0.00119302829\n",
      "training step: 25600\n",
      "loss:  0.00123813178\n",
      "training step: 25700\n",
      "loss:  0.00122474437\n",
      "training step: 25800\n",
      "loss:  0.00114885555\n",
      "training step: 25900\n",
      "loss:  0.00115122681\n",
      "training step: 26000\n",
      "loss:  0.00117545552\n",
      "training step: 26100\n",
      "loss:  0.00113308476\n",
      "training step: 26200\n",
      "loss:  0.00111244735\n",
      "training step: 26300\n",
      "loss:  0.00108910108\n",
      "training step: 26400\n",
      "loss:  0.00107882358\n",
      "training step: 26500\n",
      "loss:  0.00107187848\n",
      "training step: 26600\n",
      "loss:  0.00109343405\n",
      "training step: 26700\n",
      "loss:  0.00153268303\n",
      "training step: 26800\n",
      "loss:  0.0010491733\n",
      "training step: 26900\n",
      "loss:  0.00104558095\n",
      "training step: 27000\n",
      "loss:  0.00103138422\n",
      "training step: 27100\n",
      "loss:  0.00108676252\n",
      "training step: 27200\n",
      "loss:  0.00113531528\n",
      "training step: 27300\n",
      "loss:  0.000991768669\n",
      "training step: 27400\n",
      "loss:  0.00100412115\n",
      "training step: 27500\n",
      "loss:  0.00111059903\n",
      "training step: 27600\n",
      "loss:  0.00132624584\n",
      "training step: 27700\n",
      "loss:  0.000969166751\n",
      "training step: 27800\n",
      "loss:  0.000971768284\n",
      "training step: 27900\n",
      "loss:  0.000936807075\n",
      "training step: 28000\n",
      "loss:  0.000945793407\n",
      "training step: 28100\n",
      "loss:  0.00100843911\n",
      "training step: 28200\n",
      "loss:  0.00103923772\n",
      "training step: 28300\n",
      "loss:  0.00090261508\n",
      "training step: 28400\n",
      "loss:  0.000893376884\n",
      "training step: 28500\n",
      "loss:  0.000889334304\n",
      "training step: 28600\n",
      "loss:  0.000951760623\n",
      "training step: 28700\n",
      "loss:  0.000935891643\n",
      "training step: 28800\n",
      "loss:  0.00087832\n",
      "training step: 28900\n",
      "loss:  0.000862903602\n",
      "training step: 29000\n",
      "loss:  0.000853203121\n",
      "training step: 29100\n",
      "loss:  0.000840165827\n",
      "training step: 29200\n",
      "loss:  0.00087314815\n",
      "training step: 29300\n",
      "loss:  0.00125920016\n",
      "training step: 29400\n",
      "loss:  0.000825157098\n",
      "training step: 29500\n",
      "loss:  0.00189646741\n",
      "training step: 29600\n",
      "loss:  0.000808856741\n",
      "training step: 29700\n",
      "loss:  0.000934069045\n",
      "training step: 29800\n",
      "loss:  0.000790511433\n",
      "training step: 29900\n",
      "loss:  0.000782617251\n",
      "training step: 30000\n",
      "loss:  0.00242319074\n",
      "training step: 30100\n",
      "loss:  0.000923464773\n",
      "training step: 30200\n",
      "loss:  0.00115750253\n",
      "training step: 30300\n",
      "loss:  0.000803123636\n",
      "training step: 30400\n",
      "loss:  0.000756181253\n",
      "training step: 30500\n",
      "loss:  0.000744527963\n",
      "training step: 30600\n",
      "loss:  0.000768109749\n",
      "training step: 30700\n",
      "loss:  0.000859095249\n",
      "training step: 30800\n",
      "loss:  0.000729522493\n",
      "training step: 30900\n",
      "loss:  0.000728569401\n",
      "training step: 31000\n",
      "loss:  0.000755512621\n",
      "training step: 31100\n",
      "loss:  0.000755255518\n",
      "training step: 31200\n",
      "loss:  0.00147988496\n",
      "training step: 31300\n",
      "loss:  0.000731322274\n",
      "training step: 31400\n",
      "loss:  0.000690318\n",
      "training step: 31500\n",
      "loss:  0.000686020881\n",
      "training step: 31600\n",
      "loss:  0.000679369\n",
      "training step: 31700\n",
      "loss:  0.000720176\n",
      "training step: 31800\n",
      "loss:  0.000692148926\n",
      "training step: 31900\n",
      "loss:  0.000666810083\n",
      "training step: 32000\n",
      "loss:  0.00101136451\n",
      "training step: 32100\n",
      "loss:  0.000661104918\n",
      "training step: 32200\n",
      "loss:  0.000732366461\n",
      "training step: 32300\n",
      "loss:  0.000660488906\n",
      "training step: 32400\n",
      "loss:  0.000693063717\n",
      "training step: 32500\n",
      "loss:  0.000632759\n",
      "training step: 32600\n",
      "loss:  0.000620775914\n",
      "training step: 32700\n",
      "loss:  0.000621951127\n",
      "training step: 32800\n",
      "loss:  0.000616443\n",
      "training step: 32900\n",
      "loss:  0.000605337496\n",
      "training step: 33000\n",
      "loss:  0.000602254877\n",
      "training step: 33100\n",
      "loss:  0.000670858426\n",
      "training step: 33200\n",
      "loss:  0.000591535645\n",
      "training step: 33300\n",
      "loss:  0.00115452812\n",
      "training step: 33400\n",
      "loss:  0.00075843354\n",
      "training step: 33500\n",
      "loss:  0.000576971041\n",
      "training step: 33600\n",
      "loss:  0.000868525298\n",
      "training step: 33700\n",
      "loss:  0.000624216569\n",
      "training step: 33800\n",
      "loss:  0.000600564468\n",
      "training step: 33900\n",
      "loss:  0.00056284\n",
      "training step: 34000\n",
      "loss:  0.00055671006\n",
      "training step: 34100\n",
      "loss:  0.0010697752\n",
      "training step: 34200\n",
      "loss:  0.000595121412\n",
      "training step: 34300\n",
      "loss:  0.000548440439\n",
      "training step: 34400\n",
      "loss:  0.000535401516\n",
      "training step: 34500\n",
      "loss:  0.00126941816\n",
      "training step: 34600\n",
      "loss:  0.000529142038\n",
      "training step: 34700\n",
      "loss:  0.000564728572\n",
      "training step: 34800\n",
      "loss:  0.000696355593\n",
      "training step: 34900\n",
      "loss:  0.000518505403\n",
      "training step: 35000\n",
      "loss:  0.00074213522\n",
      "training step: 35100\n",
      "loss:  0.000525012787\n",
      "training step: 35200\n",
      "loss:  0.00052705541\n",
      "training step: 35300\n",
      "loss:  0.000498256763\n",
      "training step: 35400\n",
      "loss:  0.000497835455\n",
      "training step: 35500\n",
      "loss:  0.000985027407\n",
      "training step: 35600\n",
      "loss:  0.000488266349\n",
      "training step: 35700\n",
      "loss:  0.000509260281\n",
      "training step: 35800\n",
      "loss:  0.000695795403\n",
      "training step: 35900\n",
      "loss:  0.00052370748\n",
      "training step: 36000\n",
      "loss:  0.000474637549\n",
      "training step: 36100\n",
      "loss:  0.000468617072\n",
      "training step: 36200\n",
      "loss:  0.000557544176\n",
      "training step: 36300\n",
      "loss:  0.000534443127\n",
      "training step: 36400\n",
      "loss:  0.000491604384\n",
      "training step: 36500\n",
      "loss:  0.000449826\n",
      "training step: 36600\n",
      "loss:  0.000452134409\n",
      "training step: 36700\n",
      "loss:  0.000813573773\n",
      "training step: 36800\n",
      "loss:  0.000454083667\n",
      "training step: 36900\n",
      "loss:  0.000439241034\n",
      "training step: 37000\n",
      "loss:  0.000516778149\n",
      "training step: 37100\n",
      "loss:  0.000656828284\n",
      "training step: 37200\n",
      "loss:  0.000425520644\n",
      "training step: 37300\n",
      "loss:  0.00059918582\n",
      "training step: 37400\n",
      "loss:  0.00118090969\n",
      "training step: 37500\n",
      "loss:  0.000833739352\n",
      "training step: 37600\n",
      "loss:  0.000419152435\n",
      "training step: 37700\n",
      "loss:  0.000423757621\n",
      "training step: 37800\n",
      "loss:  0.000416658469\n",
      "training step: 37900\n",
      "loss:  0.00040341678\n",
      "training step: 38000\n",
      "loss:  0.000398126402\n",
      "training step: 38100\n",
      "loss:  0.000424844038\n",
      "training step: 38200\n",
      "loss:  0.00045360715\n",
      "training step: 38300\n",
      "loss:  0.000452036562\n",
      "training step: 38400\n",
      "loss:  0.000477830559\n",
      "training step: 38500\n",
      "loss:  0.000396200863\n",
      "training step: 38600\n",
      "loss:  0.000385632971\n",
      "training step: 38700\n",
      "loss:  0.000424759724\n",
      "training step: 38800\n",
      "loss:  0.000639742822\n",
      "training step: 38900\n",
      "loss:  0.000372744311\n",
      "training step: 39000\n",
      "loss:  0.000368281151\n",
      "training step: 39100\n",
      "loss:  0.000364740728\n",
      "training step: 39200\n",
      "loss:  0.000410899956\n",
      "training step: 39300\n",
      "loss:  0.000675027\n",
      "training step: 39400\n",
      "loss:  0.000361425569\n",
      "training step: 39500\n",
      "loss:  0.000356643111\n",
      "training step: 39600\n",
      "loss:  0.000964757\n",
      "training step: 39700\n",
      "loss:  0.000369519199\n",
      "training step: 39800\n",
      "loss:  0.000393691065\n",
      "training step: 39900\n",
      "loss:  0.000359572208\n",
      "training step: 40000\n",
      "loss:  0.000342334795\n",
      "training step: 40100\n",
      "loss:  0.000419873657\n",
      "training step: 40200\n",
      "loss:  0.000388417131\n",
      "training step: 40300\n",
      "loss:  0.000832857797\n",
      "training step: 40400\n",
      "loss:  0.000339973456\n",
      "training step: 40500\n",
      "loss:  0.00033190081\n",
      "training step: 40600\n",
      "loss:  0.000337027101\n",
      "training step: 40700\n",
      "loss:  0.000425567268\n",
      "training step: 40800\n",
      "loss:  0.000424705533\n",
      "training step: 40900\n",
      "loss:  0.000356599194\n",
      "training step: 41000\n",
      "loss:  0.000471357605\n",
      "training step: 41100\n",
      "loss:  0.000324764289\n",
      "training step: 41200\n",
      "loss:  0.000430580811\n",
      "training step: 41300\n",
      "loss:  0.000477039663\n",
      "training step: 41400\n",
      "loss:  0.000485325349\n",
      "training step: 41500\n",
      "loss:  0.000310885807\n",
      "training step: 41600\n",
      "loss:  0.000413664879\n",
      "training step: 41700\n",
      "loss:  0.000373569375\n",
      "training step: 41800\n",
      "loss:  0.000796993147\n",
      "training step: 41900\n",
      "loss:  0.000291210163\n",
      "training step: 42000\n",
      "loss:  0.000290873373\n",
      "training step: 42100\n",
      "loss:  0.000385841209\n",
      "training step: 42200\n",
      "loss:  0.000290385535\n",
      "training step: 42300\n",
      "loss:  0.000291989389\n",
      "training step: 42400\n",
      "loss:  0.00028093165\n",
      "training step: 42500\n",
      "loss:  0.000290306984\n",
      "training step: 42600\n",
      "loss:  0.000401195284\n",
      "training step: 42700\n",
      "loss:  0.000307396636\n",
      "training step: 42800\n",
      "loss:  0.000484695978\n",
      "training step: 42900\n",
      "loss:  0.000366232533\n",
      "training step: 43000\n",
      "loss:  0.000268626551\n",
      "training step: 43100\n",
      "loss:  0.000265885436\n",
      "training step: 43200\n",
      "loss:  0.000266187533\n",
      "training step: 43300\n",
      "loss:  0.000275464903\n",
      "training step: 43400\n",
      "loss:  0.000386362633\n",
      "training step: 43500\n",
      "loss:  0.000322612323\n",
      "training step: 43600\n",
      "loss:  0.000258729851\n",
      "training step: 43700\n",
      "loss:  0.000254265295\n",
      "training step: 43800\n",
      "loss:  0.000368131703\n",
      "training step: 43900\n",
      "loss:  0.000272080302\n",
      "training step: 44000\n",
      "loss:  0.000253732142\n",
      "training step: 44100\n",
      "loss:  0.000251629215\n",
      "training step: 44200\n",
      "loss:  0.000246989948\n",
      "training step: 44300\n",
      "loss:  0.000623662258\n",
      "training step: 44400\n",
      "loss:  0.000244368712\n",
      "training step: 44500\n",
      "loss:  0.000455337664\n",
      "training step: 44600\n",
      "loss:  0.000242697\n",
      "training step: 44700\n",
      "loss:  0.000250548183\n",
      "training step: 44800\n",
      "loss:  0.000237846965\n",
      "training step: 44900\n",
      "loss:  0.000248930126\n",
      "training step: 45000\n",
      "loss:  0.000258422631\n",
      "training step: 45100\n",
      "loss:  0.000233101644\n",
      "training step: 45200\n",
      "loss:  0.000255689665\n",
      "training step: 45300\n",
      "loss:  0.000228119912\n",
      "training step: 45400\n",
      "loss:  0.000225627329\n",
      "training step: 45500\n",
      "loss:  0.00061081\n",
      "training step: 45600\n",
      "loss:  0.000298507541\n",
      "training step: 45700\n",
      "loss:  0.000225446172\n",
      "training step: 45800\n",
      "loss:  0.000220387083\n",
      "training step: 45900\n",
      "loss:  0.00024966069\n",
      "training step: 46000\n",
      "loss:  0.000242903276\n",
      "training step: 46100\n",
      "loss:  0.000214158441\n",
      "training step: 46200\n",
      "loss:  0.000469740626\n",
      "training step: 46300\n",
      "loss:  0.000213759398\n",
      "training step: 46400\n",
      "loss:  0.000327788061\n",
      "training step: 46500\n",
      "loss:  0.000212142812\n",
      "training step: 46600\n",
      "loss:  0.000207678429\n",
      "training step: 46700\n",
      "loss:  0.000315827259\n",
      "training step: 46800\n",
      "loss:  0.000223849362\n",
      "training step: 46900\n",
      "loss:  0.000209779435\n",
      "training step: 47000\n",
      "loss:  0.000228563396\n",
      "training step: 47100\n",
      "loss:  0.00024848117\n",
      "training step: 47200\n",
      "loss:  0.000218677291\n",
      "training step: 47300\n",
      "loss:  0.000199858856\n",
      "training step: 47400\n",
      "loss:  0.000769293518\n",
      "training step: 47500\n",
      "loss:  0.000311087904\n",
      "training step: 47600\n",
      "loss:  0.000202021605\n",
      "training step: 47700\n",
      "loss:  0.000188680293\n",
      "training step: 47800\n",
      "loss:  0.00018822623\n",
      "training step: 47900\n",
      "loss:  0.00019149117\n",
      "training step: 48000\n",
      "loss:  0.000638641417\n",
      "training step: 48100\n",
      "loss:  0.000185095574\n",
      "training step: 48200\n",
      "loss:  0.000182050149\n",
      "training step: 48300\n",
      "loss:  0.000290022203\n",
      "training step: 48400\n",
      "loss:  0.000911136274\n",
      "training step: 48500\n",
      "loss:  0.000187616723\n",
      "training step: 48600\n",
      "loss:  0.000180596355\n",
      "training step: 48700\n",
      "loss:  0.000295371603\n",
      "training step: 48800\n",
      "loss:  0.000222667833\n",
      "training step: 48900\n",
      "loss:  0.000400045945\n",
      "training step: 49000\n",
      "loss:  0.000172092157\n",
      "training step: 49100\n",
      "loss:  0.000567597745\n",
      "training step: 49200\n",
      "loss:  0.00020346533\n",
      "training step: 49300\n",
      "loss:  0.000182427815\n",
      "training step: 49400\n",
      "loss:  0.000171949141\n",
      "training step: 49500\n",
      "loss:  0.000603269844\n",
      "training step: 49600\n",
      "loss:  0.000216396729\n",
      "training step: 49700\n",
      "loss:  0.000170606683\n",
      "training step: 49800\n",
      "loss:  0.000182328396\n",
      "training step: 49900\n",
      "loss:  0.000590607\n",
      "training step: 50000\n",
      "loss:  0.000194402179\n",
      "training step: 50100\n",
      "loss:  0.000171665306\n",
      "training step: 50200\n",
      "loss:  0.000186832403\n",
      "training step: 50300\n",
      "loss:  0.000297026389\n",
      "training step: 50400\n",
      "loss:  0.000293702382\n",
      "training step: 50500\n",
      "loss:  0.000179272771\n",
      "training step: 50600\n",
      "loss:  0.000155837493\n",
      "training step: 50700\n",
      "loss:  0.000167200837\n",
      "training step: 50800\n",
      "loss:  0.000195433429\n",
      "training step: 50900\n",
      "loss:  0.000686895277\n",
      "training step: 51000\n",
      "loss:  0.000407743879\n",
      "training step: 51100\n",
      "loss:  0.000334133569\n",
      "training step: 51200\n",
      "loss:  0.000151126806\n",
      "training step: 51300\n",
      "loss:  0.000146351798\n",
      "training step: 51400\n",
      "loss:  0.00014498258\n",
      "training step: 51500\n",
      "loss:  0.000143710422\n",
      "training step: 51600\n",
      "loss:  0.000161540389\n",
      "training step: 51700\n",
      "loss:  0.000143190671\n",
      "training step: 51800\n",
      "loss:  0.00045344056\n",
      "training step: 51900\n",
      "loss:  0.000152882261\n",
      "training step: 52000\n",
      "loss:  0.000140550721\n",
      "training step: 52100\n",
      "loss:  0.000140651\n",
      "training step: 52200\n",
      "loss:  0.000157457936\n",
      "training step: 52300\n",
      "loss:  0.000143253783\n",
      "training step: 52400\n",
      "loss:  0.000206122102\n",
      "training step: 52500\n",
      "loss:  0.000139305776\n",
      "training step: 52600\n",
      "loss:  0.000240057605\n",
      "training step: 52700\n",
      "loss:  0.000240769936\n",
      "training step: 52800\n",
      "loss:  0.000137591356\n",
      "training step: 52900\n",
      "loss:  0.00013117156\n",
      "training step: 53000\n",
      "loss:  0.000136924835\n",
      "training step: 53100\n",
      "loss:  0.000131925743\n",
      "training step: 53200\n",
      "loss:  0.000136063434\n",
      "training step: 53300\n",
      "loss:  0.000391867332\n",
      "training step: 53400\n",
      "loss:  0.000129377731\n",
      "training step: 53500\n",
      "loss:  0.00012535874\n",
      "training step: 53600\n",
      "loss:  0.000123450358\n",
      "training step: 53700\n",
      "loss:  0.000124260914\n",
      "training step: 53800\n",
      "loss:  0.000144394187\n",
      "training step: 53900\n",
      "loss:  0.000149569256\n",
      "training step: 54000\n",
      "loss:  0.000120232624\n",
      "training step: 54100\n",
      "loss:  0.000201403542\n",
      "training step: 54200\n",
      "loss:  0.000337425765\n",
      "training step: 54300\n",
      "loss:  0.000485053868\n",
      "training step: 54400\n",
      "loss:  0.000135177106\n",
      "training step: 54500\n",
      "loss:  0.00012361382\n",
      "training step: 54600\n",
      "loss:  0.00011825619\n",
      "training step: 54700\n",
      "loss:  0.00012686156\n",
      "training step: 54800\n",
      "loss:  0.000118101663\n",
      "training step: 54900\n",
      "loss:  0.000114795912\n",
      "training step: 55000\n",
      "loss:  0.000146859966\n",
      "training step: 55100\n",
      "loss:  0.000235474508\n",
      "training step: 55200\n",
      "loss:  0.000122393802\n",
      "training step: 55300\n",
      "loss:  0.000143264901\n",
      "training step: 55400\n",
      "loss:  0.000468747225\n",
      "training step: 55500\n",
      "loss:  0.000199504502\n",
      "training step: 55600\n",
      "loss:  0.000471661187\n",
      "training step: 55700\n",
      "loss:  0.000117433243\n",
      "training step: 55800\n",
      "loss:  0.000111569068\n",
      "training step: 55900\n",
      "loss:  0.000110403795\n",
      "training step: 56000\n",
      "loss:  0.000104812432\n",
      "training step: 56100\n",
      "loss:  0.000123723803\n",
      "training step: 56200\n",
      "loss:  0.000183298805\n",
      "training step: 56300\n",
      "loss:  0.000117117219\n",
      "training step: 56400\n",
      "loss:  0.000177270093\n",
      "training step: 56500\n",
      "loss:  0.000114532631\n",
      "training step: 56600\n",
      "loss:  0.00031383126\n",
      "training step: 56700\n",
      "loss:  0.000104629871\n",
      "training step: 56800\n",
      "loss:  0.000829011551\n",
      "training step: 56900\n",
      "loss:  0.000161423566\n",
      "training step: 57000\n",
      "loss:  0.000105369487\n",
      "training step: 57100\n",
      "loss:  9.91332272e-05\n",
      "training step: 57200\n",
      "loss:  0.000136620656\n",
      "training step: 57300\n",
      "loss:  0.000110190733\n",
      "training step: 57400\n",
      "loss:  0.000177923503\n",
      "training step: 57500\n",
      "loss:  0.000220723727\n",
      "training step: 57600\n",
      "loss:  0.000878334453\n",
      "training step: 57700\n",
      "loss:  9.54394491e-05\n",
      "training step: 57800\n",
      "loss:  9.40573809e-05\n",
      "training step: 57900\n",
      "loss:  0.000144399877\n",
      "training step: 58000\n",
      "loss:  0.000128603249\n",
      "training step: 58100\n",
      "loss:  9.21725223e-05\n",
      "training step: 58200\n",
      "loss:  9.41826584e-05\n",
      "training step: 58300\n",
      "loss:  0.000107275075\n",
      "training step: 58400\n",
      "loss:  9.08447691e-05\n",
      "training step: 58500\n",
      "loss:  9.29741291e-05\n",
      "training step: 58600\n",
      "loss:  9.44781714e-05\n",
      "training step: 58700\n",
      "loss:  0.000230444988\n",
      "training step: 58800\n",
      "loss:  9.16974459e-05\n",
      "training step: 58900\n",
      "loss:  0.000293276564\n",
      "training step: 59000\n",
      "loss:  0.00010915118\n",
      "training step: 59100\n",
      "loss:  0.000119338809\n",
      "training step: 59200\n",
      "loss:  9.73961578e-05\n",
      "training step: 59300\n",
      "loss:  8.54073805e-05\n",
      "training step: 59400\n",
      "loss:  0.000160127878\n",
      "training step: 59500\n",
      "loss:  0.00026657313\n",
      "training step: 59600\n",
      "loss:  0.000448886363\n",
      "training step: 59700\n",
      "loss:  9.00092564e-05\n",
      "training step: 59800\n",
      "loss:  8.82915265e-05\n",
      "training step: 59900\n",
      "loss:  8.1439277e-05\n",
      "training step: 60000\n",
      "loss:  8.02090653e-05\n",
      "training step: 60100\n",
      "loss:  8.7357439e-05\n",
      "training step: 60200\n",
      "loss:  0.000148603474\n",
      "training step: 60300\n",
      "loss:  0.000161695672\n",
      "training step: 60400\n",
      "loss:  0.00013165221\n",
      "training step: 60500\n",
      "loss:  0.000104125233\n",
      "training step: 60600\n",
      "loss:  8.17333421e-05\n",
      "training step: 60700\n",
      "loss:  8.86131602e-05\n",
      "training step: 60800\n",
      "loss:  7.62518903e-05\n",
      "training step: 60900\n",
      "loss:  8.61956e-05\n",
      "training step: 61000\n",
      "loss:  0.000102613929\n",
      "training step: 61100\n",
      "loss:  9.46756918e-05\n",
      "training step: 61200\n",
      "loss:  9.69049943e-05\n",
      "training step: 61300\n",
      "loss:  0.000206650016\n",
      "training step: 61400\n",
      "loss:  7.55378569e-05\n",
      "training step: 61500\n",
      "loss:  7.46965889e-05\n",
      "training step: 61600\n",
      "loss:  7.37266091e-05\n",
      "training step: 61700\n",
      "loss:  8.14367449e-05\n",
      "training step: 61800\n",
      "loss:  9.74310897e-05\n",
      "training step: 61900\n",
      "loss:  7.17932271e-05\n",
      "training step: 62000\n",
      "loss:  0.000160875803\n",
      "training step: 62100\n",
      "loss:  7.65544464e-05\n",
      "training step: 62200\n",
      "loss:  0.000143452533\n",
      "training step: 62300\n",
      "loss:  0.000121476871\n",
      "training step: 62400\n",
      "loss:  8.20390269e-05\n",
      "training step: 62500\n",
      "loss:  0.0001419685\n",
      "training step: 62600\n",
      "loss:  0.000131198816\n",
      "training step: 62700\n",
      "loss:  7.28310333e-05\n",
      "training step: 62800\n",
      "loss:  0.000113305323\n",
      "training step: 62900\n",
      "loss:  0.000130397311\n",
      "training step: 63000\n",
      "loss:  0.000213924344\n",
      "training step: 63100\n",
      "loss:  8.7029548e-05\n",
      "training step: 63200\n",
      "loss:  6.70380105e-05\n",
      "training step: 63300\n",
      "loss:  7.05048224e-05\n",
      "training step: 63400\n",
      "loss:  6.93130423e-05\n",
      "training step: 63500\n",
      "loss:  6.90082088e-05\n",
      "training step: 63600\n",
      "loss:  0.000171846055\n",
      "training step: 63700\n",
      "loss:  7.32679691e-05\n",
      "training step: 63800\n",
      "loss:  8.94821205e-05\n",
      "training step: 63900\n",
      "loss:  0.00104579527\n",
      "training step: 64000\n",
      "loss:  6.22672451e-05\n",
      "training step: 64100\n",
      "loss:  0.000316961348\n",
      "training step: 64200\n",
      "loss:  8.57759369e-05\n",
      "training step: 64300\n",
      "loss:  7.85885422e-05\n",
      "training step: 64400\n",
      "loss:  7.69819817e-05\n",
      "training step: 64500\n",
      "loss:  0.000208265672\n",
      "training step: 64600\n",
      "loss:  8.60124055e-05\n",
      "training step: 64700\n",
      "loss:  7.15886563e-05\n",
      "training step: 64800\n",
      "loss:  0.000106427244\n",
      "training step: 64900\n",
      "loss:  6.34700773e-05\n",
      "training step: 65000\n",
      "loss:  0.000103874445\n",
      "training step: 65100\n",
      "loss:  6.47523775e-05\n",
      "training step: 65200\n",
      "loss:  5.90398085e-05\n",
      "training step: 65300\n",
      "loss:  6.23328087e-05\n",
      "training step: 65400\n",
      "loss:  7.15193601e-05\n",
      "training step: 65500\n",
      "loss:  8.64618487e-05\n",
      "training step: 65600\n",
      "loss:  5.88709736e-05\n",
      "training step: 65700\n",
      "loss:  6.04850429e-05\n",
      "training step: 65800\n",
      "loss:  6.4376407e-05\n",
      "training step: 65900\n",
      "loss:  0.000151374814\n",
      "training step: 66000\n",
      "loss:  7.11288085e-05\n",
      "training step: 66100\n",
      "loss:  5.82350804e-05\n",
      "training step: 66200\n",
      "loss:  5.65059236e-05\n",
      "training step: 66300\n",
      "loss:  6.25892208e-05\n",
      "training step: 66400\n",
      "loss:  0.000244962372\n",
      "training step: 66500\n",
      "loss:  0.000274011109\n",
      "training step: 66600\n",
      "loss:  6.74889598e-05\n",
      "training step: 66700\n",
      "loss:  6.38624697e-05\n",
      "training step: 66800\n",
      "loss:  0.000405532744\n",
      "training step: 66900\n",
      "loss:  0.000177946727\n",
      "training step: 67000\n",
      "loss:  0.000376139622\n",
      "training step: 67100\n",
      "loss:  0.000180299859\n",
      "training step: 67200\n",
      "loss:  5.40442779e-05\n",
      "training step: 67300\n",
      "loss:  5.41889713e-05\n",
      "training step: 67400\n",
      "loss:  5.22615192e-05\n",
      "training step: 67500\n",
      "loss:  5.18425404e-05\n",
      "training step: 67600\n",
      "loss:  5.78477229e-05\n",
      "training step: 67700\n",
      "loss:  0.000101548729\n",
      "training step: 67800\n",
      "loss:  9.34102354e-05\n",
      "training step: 67900\n",
      "loss:  6.91628e-05\n",
      "training step: 68000\n",
      "loss:  0.000110944609\n",
      "training step: 68100\n",
      "loss:  5.88213916e-05\n",
      "training step: 68200\n",
      "loss:  9.40383616e-05\n",
      "training step: 68300\n",
      "loss:  6.34610769e-05\n",
      "training step: 68400\n",
      "loss:  4.89933045e-05\n",
      "training step: 68500\n",
      "loss:  5.38062195e-05\n",
      "training step: 68600\n",
      "loss:  8.70882286e-05\n",
      "training step: 68700\n",
      "loss:  0.000174522866\n",
      "training step: 68800\n",
      "loss:  4.81416e-05\n",
      "training step: 68900\n",
      "loss:  7.71491104e-05\n",
      "training step: 69000\n",
      "loss:  4.84759366e-05\n",
      "training step: 69100\n",
      "loss:  6.32819647e-05\n",
      "training step: 69200\n",
      "loss:  0.000499916263\n",
      "training step: 69300\n",
      "loss:  4.94824963e-05\n",
      "training step: 69400\n",
      "loss:  7.05358107e-05\n",
      "training step: 69500\n",
      "loss:  0.000124115948\n",
      "training step: 69600\n",
      "loss:  4.69731167e-05\n",
      "training step: 69700\n",
      "loss:  4.8124326e-05\n",
      "training step: 69800\n",
      "loss:  4.52087188e-05\n",
      "training step: 69900\n",
      "loss:  0.000106185442\n",
      "training step: 70000\n",
      "loss:  0.000157858536\n",
      "training step: 70100\n",
      "loss:  5.8542e-05\n",
      "training step: 70200\n",
      "loss:  0.000117843352\n",
      "training step: 70300\n",
      "loss:  9.68468667e-05\n",
      "training step: 70400\n",
      "loss:  4.79555893e-05\n",
      "training step: 70500\n",
      "loss:  0.000438332616\n",
      "training step: 70600\n",
      "loss:  5.80926426e-05\n",
      "training step: 70700\n",
      "loss:  8.97859936e-05\n",
      "training step: 70800\n",
      "loss:  6.1818384e-05\n",
      "training step: 70900\n",
      "loss:  4.68959915e-05\n",
      "training step: 71000\n",
      "loss:  0.000114335729\n",
      "training step: 71100\n",
      "loss:  7.07023501e-05\n",
      "training step: 71200\n",
      "loss:  4.40603326e-05\n",
      "training step: 71300\n",
      "loss:  5.1669831e-05\n",
      "training step: 71400\n",
      "loss:  4.49234503e-05\n",
      "training step: 71500\n",
      "loss:  8.43344606e-05\n",
      "training step: 71600\n",
      "loss:  6.06174071e-05\n",
      "training step: 71700\n",
      "loss:  6.00456078e-05\n",
      "training step: 71800\n",
      "loss:  4.70609702e-05\n",
      "training step: 71900\n",
      "loss:  4.3939468e-05\n",
      "training step: 72000\n",
      "loss:  8.33105587e-05\n",
      "training step: 72100\n",
      "loss:  5.18760644e-05\n",
      "training step: 72200\n",
      "loss:  9.22196268e-05\n",
      "training step: 72300\n",
      "loss:  5.21900511e-05\n",
      "training step: 72400\n",
      "loss:  4.22483099e-05\n",
      "training step: 72500\n",
      "loss:  4.57532878e-05\n",
      "training step: 72600\n",
      "loss:  5.73626094e-05\n",
      "training step: 72700\n",
      "loss:  4.86657846e-05\n",
      "training step: 72800\n",
      "loss:  4.70583727e-05\n",
      "training step: 72900\n",
      "loss:  4.25898215e-05\n",
      "training step: 73000\n",
      "loss:  0.000236573396\n",
      "training step: 73100\n",
      "loss:  0.000162845012\n",
      "training step: 73200\n",
      "loss:  4.47048114e-05\n",
      "training step: 73300\n",
      "loss:  7.82493735e-05\n",
      "training step: 73400\n",
      "loss:  4.18773052e-05\n",
      "training step: 73500\n",
      "loss:  0.000210605111\n",
      "training step: 73600\n",
      "loss:  0.000251557678\n",
      "training step: 73700\n",
      "loss:  7.52309934e-05\n",
      "training step: 73800\n",
      "loss:  5.48068056e-05\n",
      "training step: 73900\n",
      "loss:  7.46117439e-05\n",
      "training step: 74000\n",
      "loss:  4.33486312e-05\n",
      "training step: 74100\n",
      "loss:  3.87595e-05\n",
      "training step: 74200\n",
      "loss:  0.000375502\n",
      "training step: 74300\n",
      "loss:  5.33105449e-05\n",
      "training step: 74400\n",
      "loss:  4.07242114e-05\n",
      "training step: 74500\n",
      "loss:  0.000392790593\n",
      "training step: 74600\n",
      "loss:  5.2659394e-05\n",
      "training step: 74700\n",
      "loss:  4.14177048e-05\n",
      "training step: 74800\n",
      "loss:  3.51174349e-05\n",
      "training step: 74900\n",
      "loss:  3.61884886e-05\n",
      "training step: 75000\n",
      "loss:  4.16062248e-05\n",
      "training step: 75100\n",
      "loss:  3.52157476e-05\n",
      "training step: 75200\n",
      "loss:  3.81646023e-05\n",
      "training step: 75300\n",
      "loss:  5.24061979e-05\n",
      "training step: 75400\n",
      "loss:  6.82586688e-05\n",
      "training step: 75500\n",
      "loss:  4.47536295e-05\n",
      "training step: 75600\n",
      "loss:  7.37863593e-05\n",
      "training step: 75700\n",
      "loss:  3.36658813e-05\n",
      "training step: 75800\n",
      "loss:  5.24224088e-05\n",
      "training step: 75900\n",
      "loss:  5.99009218e-05\n",
      "training step: 76000\n",
      "loss:  3.33090175e-05\n",
      "training step: 76100\n",
      "loss:  4.53320208e-05\n",
      "training step: 76200\n",
      "loss:  0.000125104285\n",
      "training step: 76300\n",
      "loss:  9.38455123e-05\n",
      "training step: 76400\n",
      "loss:  4.24253412e-05\n",
      "training step: 76500\n",
      "loss:  7.85634038e-05\n",
      "training step: 76600\n",
      "loss:  0.000102932543\n",
      "training step: 76700\n",
      "loss:  6.42542655e-05\n",
      "training step: 76800\n",
      "loss:  0.000201206785\n",
      "training step: 76900\n",
      "loss:  4.8886166e-05\n",
      "training step: 77000\n",
      "loss:  3.5631725e-05\n",
      "training step: 77100\n",
      "loss:  4.03116137e-05\n",
      "training step: 77200\n",
      "loss:  7.5930373e-05\n",
      "training step: 77300\n",
      "loss:  6.13517332e-05\n",
      "training step: 77400\n",
      "loss:  0.000222009374\n",
      "training step: 77500\n",
      "loss:  3.32950585e-05\n",
      "training step: 77600\n",
      "loss:  0.000102356578\n",
      "training step: 77700\n",
      "loss:  9.70952897e-05\n",
      "training step: 77800\n",
      "loss:  0.000182067248\n",
      "training step: 77900\n",
      "loss:  0.000329747534\n",
      "training step: 78000\n",
      "loss:  3.88947556e-05\n",
      "training step: 78100\n",
      "loss:  9.81941339e-05\n",
      "training step: 78200\n",
      "loss:  3.26985137e-05\n",
      "training step: 78300\n",
      "loss:  7.53220156e-05\n",
      "training step: 78400\n",
      "loss:  6.44498359e-05\n",
      "training step: 78500\n",
      "loss:  5.31263031e-05\n",
      "training step: 78600\n",
      "loss:  5.95195415e-05\n",
      "training step: 78700\n",
      "loss:  3.66573331e-05\n",
      "training step: 78800\n",
      "loss:  9.13714393e-05\n",
      "training step: 78900\n",
      "loss:  4.45248e-05\n",
      "training step: 79000\n",
      "loss:  4.07228545e-05\n",
      "training step: 79100\n",
      "loss:  0.000131726658\n",
      "training step: 79200\n",
      "loss:  5.75725426e-05\n",
      "training step: 79300\n",
      "loss:  3.11625481e-05\n",
      "training step: 79400\n",
      "loss:  2.99252733e-05\n",
      "training step: 79500\n",
      "loss:  0.000179382943\n",
      "training step: 79600\n",
      "loss:  8.82951645e-05\n",
      "training step: 79700\n",
      "loss:  3.26608933e-05\n",
      "training step: 79800\n",
      "loss:  0.000219889625\n",
      "training step: 79900\n",
      "loss:  5.10986902e-05\n",
      "training step: 80000\n",
      "loss:  8.84002075e-05\n",
      "training step: 80100\n",
      "loss:  3.04601035e-05\n",
      "training step: 80200\n",
      "loss:  2.79172018e-05\n",
      "training step: 80300\n",
      "loss:  7.06992796e-05\n",
      "training step: 80400\n",
      "loss:  3.08539784e-05\n",
      "training step: 80500\n",
      "loss:  8.38598353e-05\n",
      "training step: 80600\n",
      "loss:  0.00018059407\n",
      "training step: 80700\n",
      "loss:  3.57321842e-05\n",
      "training step: 80800\n",
      "loss:  2.72171983e-05\n",
      "training step: 80900\n",
      "loss:  9.31747563e-05\n",
      "training step: 81000\n",
      "loss:  2.73071655e-05\n",
      "training step: 81100\n",
      "loss:  3.07461451e-05\n",
      "training step: 81200\n",
      "loss:  2.72857524e-05\n",
      "training step: 81300\n",
      "loss:  3.69855188e-05\n",
      "training step: 81400\n",
      "loss:  4.89341e-05\n",
      "training step: 81500\n",
      "loss:  3.11136137e-05\n",
      "training step: 81600\n",
      "loss:  4.05184946e-05\n",
      "training step: 81700\n",
      "loss:  0.00015895447\n",
      "training step: 81800\n",
      "loss:  0.000143618847\n",
      "training step: 81900\n",
      "loss:  2.89626732e-05\n",
      "training step: 82000\n",
      "loss:  3.11413205e-05\n",
      "training step: 82100\n",
      "loss:  7.60243784e-05\n",
      "training step: 82200\n",
      "loss:  8.77142811e-05\n",
      "training step: 82300\n",
      "loss:  3.69502668e-05\n",
      "training step: 82400\n",
      "loss:  5.91932367e-05\n",
      "training step: 82500\n",
      "loss:  4.34229696e-05\n",
      "training step: 82600\n",
      "loss:  3.07455593e-05\n",
      "training step: 82700\n",
      "loss:  2.8032362e-05\n",
      "training step: 82800\n",
      "loss:  0.000157426024\n",
      "training step: 82900\n",
      "loss:  2.81021057e-05\n",
      "training step: 83000\n",
      "loss:  3.57529534e-05\n",
      "training step: 83100\n",
      "loss:  8.53067904e-05\n",
      "training step: 83200\n",
      "loss:  6.26535693e-05\n",
      "training step: 83300\n",
      "loss:  3.04123259e-05\n",
      "training step: 83400\n",
      "loss:  3.59959158e-05\n",
      "training step: 83500\n",
      "loss:  4.63741962e-05\n",
      "training step: 83600\n",
      "loss:  2.43457762e-05\n",
      "training step: 83700\n",
      "loss:  0.000196615452\n",
      "training step: 83800\n",
      "loss:  0.000108687331\n",
      "training step: 83900\n",
      "loss:  2.47666812e-05\n",
      "training step: 84000\n",
      "loss:  0.000212056388\n",
      "training step: 84100\n",
      "loss:  6.23206361e-05\n",
      "training step: 84200\n",
      "loss:  2.4433406e-05\n",
      "training step: 84300\n",
      "loss:  0.000108762208\n",
      "training step: 84400\n",
      "loss:  3.1161886e-05\n",
      "training step: 84500\n",
      "loss:  4.31825174e-05\n",
      "training step: 84600\n",
      "loss:  6.03530934e-05\n",
      "training step: 84700\n",
      "loss:  2.51567053e-05\n",
      "training step: 84800\n",
      "loss:  0.000152271256\n",
      "training step: 84900\n",
      "loss:  2.82017299e-05\n",
      "training step: 85000\n",
      "loss:  0.000208488724\n",
      "training step: 85100\n",
      "loss:  4.75389388e-05\n",
      "training step: 85200\n",
      "loss:  2.62044468e-05\n",
      "training step: 85300\n",
      "loss:  6.01044485e-05\n",
      "training step: 85400\n",
      "loss:  2.65876133e-05\n",
      "training step: 85500\n",
      "loss:  3.44204127e-05\n",
      "training step: 85600\n",
      "loss:  4.67837672e-05\n",
      "training step: 85700\n",
      "loss:  0.000200673632\n",
      "training step: 85800\n",
      "loss:  2.63845e-05\n",
      "training step: 85900\n",
      "loss:  0.000126296756\n",
      "training step: 86000\n",
      "loss:  3.33202333e-05\n",
      "training step: 86100\n",
      "loss:  2.35986336e-05\n",
      "training step: 86200\n",
      "loss:  2.34970648e-05\n",
      "training step: 86300\n",
      "loss:  4.31540248e-05\n",
      "training step: 86400\n",
      "loss:  5.63685535e-05\n",
      "training step: 86500\n",
      "loss:  2.29150974e-05\n",
      "training step: 86600\n",
      "loss:  0.00012696824\n",
      "training step: 86700\n",
      "loss:  3.24480534e-05\n",
      "training step: 86800\n",
      "loss:  4.08099513e-05\n",
      "training step: 86900\n",
      "loss:  2.53341077e-05\n",
      "training step: 87000\n",
      "loss:  0.000155327245\n",
      "training step: 87100\n",
      "loss:  5.41065638e-05\n",
      "training step: 87200\n",
      "loss:  2.68405238e-05\n",
      "training step: 87300\n",
      "loss:  0.000300459302\n",
      "training step: 87400\n",
      "loss:  0.000395772\n",
      "training step: 87500\n",
      "loss:  2.58116452e-05\n",
      "training step: 87600\n",
      "loss:  2.09397731e-05\n",
      "training step: 87700\n",
      "loss:  2.29428715e-05\n",
      "training step: 87800\n",
      "loss:  8.81126907e-05\n",
      "training step: 87900\n",
      "loss:  6.87620195e-05\n",
      "training step: 88000\n",
      "loss:  2.68771382e-05\n",
      "training step: 88100\n",
      "loss:  2.26174507e-05\n",
      "training step: 88200\n",
      "loss:  0.000180921852\n",
      "training step: 88300\n",
      "loss:  2.44862676e-05\n",
      "training step: 88400\n",
      "loss:  2.62482099e-05\n",
      "training step: 88500\n",
      "loss:  2.44190669e-05\n",
      "training step: 88600\n",
      "loss:  5.71665805e-05\n",
      "training step: 88700\n",
      "loss:  2.80966578e-05\n",
      "training step: 88800\n",
      "loss:  2.8056782e-05\n",
      "training step: 88900\n",
      "loss:  4.51970154e-05\n",
      "training step: 89000\n",
      "loss:  2.18194909e-05\n",
      "training step: 89100\n",
      "loss:  4.72806387e-05\n",
      "training step: 89200\n",
      "loss:  6.78485085e-05\n",
      "training step: 89300\n",
      "loss:  6.52899835e-05\n",
      "training step: 89400\n",
      "loss:  4.03063896e-05\n",
      "training step: 89500\n",
      "loss:  2.06509e-05\n",
      "training step: 89600\n",
      "loss:  6.76247946e-05\n",
      "training step: 89700\n",
      "loss:  2.09820519e-05\n",
      "training step: 89800\n",
      "loss:  2.53699563e-05\n",
      "training step: 89900\n",
      "loss:  6.44642205e-05\n",
      "training step: 90000\n",
      "loss:  0.000150698295\n",
      "training step: 90100\n",
      "loss:  0.000157180257\n",
      "training step: 90200\n",
      "loss:  3.50660521e-05\n",
      "training step: 90300\n",
      "loss:  7.12152905e-05\n",
      "training step: 90400\n",
      "loss:  5.19688547e-05\n",
      "training step: 90500\n",
      "loss:  8.2060018e-05\n",
      "training step: 90600\n",
      "loss:  2.00103659e-05\n",
      "training step: 90700\n",
      "loss:  9.9277524e-05\n",
      "training step: 90800\n",
      "loss:  8.54389727e-05\n",
      "training step: 90900\n",
      "loss:  4.03199156e-05\n",
      "training step: 91000\n",
      "loss:  8.70140284e-05\n",
      "training step: 91100\n",
      "loss:  6.44276588e-05\n",
      "training step: 91200\n",
      "loss:  2.98051855e-05\n",
      "training step: 91300\n",
      "loss:  1.89924885e-05\n",
      "training step: 91400\n",
      "loss:  0.00010320569\n",
      "training step: 91500\n",
      "loss:  0.000219852052\n",
      "training step: 91600\n",
      "loss:  3.36412777e-05\n",
      "training step: 91700\n",
      "loss:  1.94639742e-05\n",
      "training step: 91800\n",
      "loss:  1.9092613e-05\n",
      "training step: 91900\n",
      "loss:  0.00010289593\n",
      "training step: 92000\n",
      "loss:  2.06431523e-05\n",
      "training step: 92100\n",
      "loss:  2.1973503e-05\n",
      "training step: 92200\n",
      "loss:  1.94659042e-05\n",
      "training step: 92300\n",
      "loss:  3.68879519e-05\n",
      "training step: 92400\n",
      "loss:  2.96927683e-05\n",
      "training step: 92500\n",
      "loss:  2.38440389e-05\n",
      "training step: 92600\n",
      "loss:  2.09058144e-05\n",
      "training step: 92700\n",
      "loss:  1.75555288e-05\n",
      "training step: 92800\n",
      "loss:  1.79352464e-05\n",
      "training step: 92900\n",
      "loss:  3.27298403e-05\n",
      "training step: 93000\n",
      "loss:  2.03274139e-05\n",
      "training step: 93100\n",
      "loss:  0.000272608391\n",
      "training step: 93200\n",
      "loss:  2.70345754e-05\n",
      "training step: 93300\n",
      "loss:  1.85460667e-05\n",
      "training step: 93400\n",
      "loss:  4.15896284e-05\n",
      "training step: 93500\n",
      "loss:  0.000111732566\n",
      "training step: 93600\n",
      "loss:  1.8965562e-05\n",
      "training step: 93700\n",
      "loss:  0.000162729179\n",
      "training step: 93800\n",
      "loss:  4.03940612e-05\n",
      "training step: 93900\n",
      "loss:  3.20624313e-05\n",
      "training step: 94000\n",
      "loss:  1.71918764e-05\n",
      "training step: 94100\n",
      "loss:  1.8347906e-05\n",
      "training step: 94200\n",
      "loss:  6.14995661e-05\n",
      "training step: 94300\n",
      "loss:  1.92472471e-05\n",
      "training step: 94400\n",
      "loss:  1.89300808e-05\n",
      "training step: 94500\n",
      "loss:  2.82594374e-05\n",
      "training step: 94600\n",
      "loss:  1.96378733e-05\n",
      "training step: 94700\n",
      "loss:  1.86909328e-05\n",
      "training step: 94800\n",
      "loss:  1.75080841e-05\n",
      "training step: 94900\n",
      "loss:  3.04164732e-05\n",
      "training step: 95000\n",
      "loss:  2.14477623e-05\n",
      "training step: 95100\n",
      "loss:  3.45816698e-05\n",
      "training step: 95200\n",
      "loss:  2.97371862e-05\n",
      "training step: 95300\n",
      "loss:  1.89397451e-05\n",
      "training step: 95400\n",
      "loss:  2.50804278e-05\n",
      "training step: 95500\n",
      "loss:  3.07027112e-05\n",
      "training step: 95600\n",
      "loss:  2.36519791e-05\n",
      "training step: 95700\n",
      "loss:  3.99286582e-05\n",
      "training step: 95800\n",
      "loss:  3.11851873e-05\n",
      "training step: 95900\n",
      "loss:  2.18615714e-05\n",
      "training step: 96000\n",
      "loss:  3.10207179e-05\n",
      "training step: 96100\n",
      "loss:  2.27525688e-05\n",
      "training step: 96200\n",
      "loss:  3.69237168e-05\n",
      "training step: 96300\n",
      "loss:  2.50182202e-05\n",
      "training step: 96400\n",
      "loss:  0.000158171009\n",
      "training step: 96500\n",
      "loss:  1.69261566e-05\n",
      "training step: 96600\n",
      "loss:  1.60831e-05\n",
      "training step: 96700\n",
      "loss:  0.000267369702\n",
      "training step: 96800\n",
      "loss:  1.65913334e-05\n",
      "training step: 96900\n",
      "loss:  1.95600569e-05\n",
      "training step: 97000\n",
      "loss:  0.00026552807\n",
      "training step: 97100\n",
      "loss:  3.37842976e-05\n",
      "training step: 97200\n",
      "loss:  7.85663215e-05\n",
      "training step: 97300\n",
      "loss:  7.9902893e-05\n",
      "training step: 97400\n",
      "loss:  2.05371489e-05\n",
      "training step: 97500\n",
      "loss:  3.66850254e-05\n",
      "training step: 97600\n",
      "loss:  1.98882299e-05\n",
      "training step: 97700\n",
      "loss:  0.000157839677\n",
      "training step: 97800\n",
      "loss:  1.82622825e-05\n",
      "training step: 97900\n",
      "loss:  1.57375689e-05\n",
      "training step: 98000\n",
      "loss:  0.000107934029\n",
      "training step: 98100\n",
      "loss:  4.69859551e-05\n",
      "training step: 98200\n",
      "loss:  2.11523929e-05\n",
      "training step: 98300\n",
      "loss:  2.02807496e-05\n",
      "training step: 98400\n",
      "loss:  6.41618e-05\n",
      "training step: 98500\n",
      "loss:  2.72705183e-05\n",
      "training step: 98600\n",
      "loss:  1.61041644e-05\n",
      "training step: 98700\n",
      "loss:  2.33254177e-05\n",
      "training step: 98800\n",
      "loss:  0.000119022181\n",
      "training step: 98900\n",
      "loss:  3.09542702e-05\n",
      "training step: 99000\n",
      "loss:  2.48478573e-05\n",
      "training step: 99100\n",
      "loss:  0.000228733799\n",
      "training step: 99200\n",
      "loss:  2.72637299e-05\n",
      "training step: 99300\n",
      "loss:  5.1745079e-05\n",
      "training step: 99400\n",
      "loss:  3.13350065e-05\n",
      "training step: 99500\n",
      "loss:  1.95498669e-05\n",
      "training step: 99600\n",
      "loss:  1.95788507e-05\n",
      "training step: 99700\n",
      "loss:  8.00947091e-05\n",
      "training step: 99800\n",
      "loss:  2.51304118e-05\n",
      "training step: 99900\n",
      "loss:  3.52453062e-05\n",
      "training step: 100000\n",
      "loss:  1.66164064e-05\n",
      "training step: 100100\n",
      "loss:  3.15226971e-05\n",
      "training step: 100200\n",
      "loss:  0.000154690191\n",
      "training step: 100300\n",
      "loss:  0.000353948359\n",
      "training step: 100400\n",
      "loss:  3.05598114e-05\n",
      "training step: 100500\n",
      "loss:  1.37591269e-05\n",
      "training step: 100600\n",
      "loss:  2.8146922e-05\n",
      "training step: 100700\n",
      "loss:  6.07721e-05\n",
      "training step: 100800\n",
      "loss:  3.95629104e-05\n",
      "training step: 100900\n",
      "loss:  0.000199624701\n",
      "training step: 101000\n",
      "loss:  6.20786377e-05\n",
      "training step: 101100\n",
      "loss:  2.68152544e-05\n",
      "training step: 101200\n",
      "loss:  2.96857434e-05\n",
      "training step: 101300\n",
      "loss:  1.58552666e-05\n",
      "training step: 101400\n",
      "loss:  0.000155562157\n",
      "training step: 101500\n",
      "loss:  8.84084e-05\n",
      "training step: 101600\n",
      "loss:  2.11603437e-05\n",
      "training step: 101700\n",
      "loss:  1.94592376e-05\n",
      "training step: 101800\n",
      "loss:  1.38279311e-05\n",
      "training step: 101900\n",
      "loss:  1.81036939e-05\n",
      "training step: 102000\n",
      "loss:  1.49086181e-05\n",
      "training step: 102100\n",
      "loss:  1.967766e-05\n",
      "training step: 102200\n",
      "loss:  1.66489026e-05\n",
      "training step: 102300\n",
      "loss:  1.43595289e-05\n",
      "training step: 102400\n",
      "loss:  1.9798812e-05\n",
      "training step: 102500\n",
      "loss:  1.7890583e-05\n",
      "training step: 102600\n",
      "loss:  0.000170586281\n",
      "training step: 102700\n",
      "loss:  2.07457633e-05\n",
      "training step: 102800\n",
      "loss:  1.80354873e-05\n",
      "training step: 102900\n",
      "loss:  1.33866324e-05\n",
      "training step: 103000\n",
      "loss:  1.77881175e-05\n",
      "training step: 103100\n",
      "loss:  1.77313577e-05\n",
      "training step: 103200\n",
      "loss:  3.50860864e-05\n",
      "training step: 103300\n",
      "loss:  2.32946131e-05\n",
      "training step: 103400\n",
      "loss:  6.40908911e-05\n",
      "training step: 103500\n",
      "loss:  1.55018515e-05\n",
      "training step: 103600\n",
      "loss:  2.19785288e-05\n",
      "training step: 103700\n",
      "loss:  1.36498438e-05\n",
      "training step: 103800\n",
      "loss:  7.38387171e-05\n",
      "training step: 103900\n",
      "loss:  4.70319392e-05\n",
      "training step: 104000\n",
      "loss:  2.21103855e-05\n",
      "training step: 104100\n",
      "loss:  1.58323255e-05\n",
      "training step: 104200\n",
      "loss:  3.32496675e-05\n",
      "training step: 104300\n",
      "loss:  3.69246845e-05\n",
      "training step: 104400\n",
      "loss:  1.30367807e-05\n",
      "training step: 104500\n",
      "loss:  0.000326331035\n",
      "training step: 104600\n",
      "loss:  1.96374913e-05\n",
      "training step: 104700\n",
      "loss:  4.16010917e-05\n",
      "training step: 104800\n",
      "loss:  2.15467498e-05\n",
      "training step: 104900\n",
      "loss:  1.46845887e-05\n",
      "training step: 105000\n",
      "loss:  7.0907794e-05\n",
      "training step: 105100\n",
      "loss:  1.32564655e-05\n",
      "training step: 105200\n",
      "loss:  0.000359138881\n",
      "training step: 105300\n",
      "loss:  1.80365605e-05\n",
      "training step: 105400\n",
      "loss:  1.87257283e-05\n",
      "training step: 105500\n",
      "loss:  6.82445825e-05\n",
      "training step: 105600\n",
      "loss:  5.22242372e-05\n",
      "training step: 105700\n",
      "loss:  6.65965345e-05\n",
      "training step: 105800\n",
      "loss:  9.50035101e-05\n",
      "training step: 105900\n",
      "loss:  1.32180348e-05\n",
      "training step: 106000\n",
      "loss:  2.25066742e-05\n",
      "training step: 106100\n",
      "loss:  1.22634183e-05\n",
      "training step: 106200\n",
      "loss:  2.17259894e-05\n",
      "training step: 106300\n",
      "loss:  3.47555797e-05\n",
      "training step: 106400\n",
      "loss:  6.12999647e-05\n",
      "training step: 106500\n",
      "loss:  1.98015841e-05\n",
      "training step: 106600\n",
      "loss:  1.35418695e-05\n",
      "training step: 106700\n",
      "loss:  0.000104444051\n",
      "training step: 106800\n",
      "loss:  1.76658195e-05\n",
      "training step: 106900\n",
      "loss:  7.44651e-05\n",
      "training step: 107000\n",
      "loss:  5.68718642e-05\n",
      "training step: 107100\n",
      "loss:  3.20766594e-05\n",
      "training step: 107200\n",
      "loss:  1.92660027e-05\n",
      "training step: 107300\n",
      "loss:  2.02595056e-05\n",
      "training step: 107400\n",
      "loss:  1.67049802e-05\n",
      "training step: 107500\n",
      "loss:  1.22706269e-05\n",
      "training step: 107600\n",
      "loss:  1.85630852e-05\n",
      "training step: 107700\n",
      "loss:  1.29093351e-05\n",
      "training step: 107800\n",
      "loss:  5.54705548e-05\n",
      "training step: 107900\n",
      "loss:  1.36109566e-05\n",
      "training step: 108000\n",
      "loss:  0.0002747791\n",
      "training step: 108100\n",
      "loss:  1.29676855e-05\n",
      "training step: 108200\n",
      "loss:  1.44166097e-05\n",
      "training step: 108300\n",
      "loss:  2.43664836e-05\n",
      "training step: 108400\n",
      "loss:  3.22061e-05\n",
      "training step: 108500\n",
      "loss:  1.58370221e-05\n",
      "training step: 108600\n",
      "loss:  0.00016158713\n",
      "training step: 108700\n",
      "loss:  5.87869945e-05\n",
      "training step: 108800\n",
      "loss:  3.83779152e-05\n",
      "training step: 108900\n",
      "loss:  9.75248695e-05\n",
      "training step: 109000\n",
      "loss:  1.54116588e-05\n",
      "training step: 109100\n",
      "loss:  7.4936077e-05\n",
      "training step: 109200\n",
      "loss:  1.41645751e-05\n",
      "training step: 109300\n",
      "loss:  1.73343105e-05\n",
      "training step: 109400\n",
      "loss:  5.1612762e-05\n",
      "training step: 109500\n",
      "loss:  3.39113976e-05\n",
      "training step: 109600\n",
      "loss:  5.89406482e-05\n",
      "training step: 109700\n",
      "loss:  1.49832422e-05\n",
      "training step: 109800\n",
      "loss:  5.95159145e-05\n",
      "training step: 109900\n",
      "loss:  0.000112050315\n",
      "training step: 110000\n",
      "loss:  1.16910096e-05\n",
      "training step: 110100\n",
      "loss:  1.22471674e-05\n",
      "training step: 110200\n",
      "loss:  3.55405646e-05\n",
      "training step: 110300\n",
      "loss:  1.40095317e-05\n",
      "training step: 110400\n",
      "loss:  4.43567224e-05\n",
      "training step: 110500\n",
      "loss:  8.56955085e-05\n",
      "training step: 110600\n",
      "loss:  0.000121534293\n",
      "training step: 110700\n",
      "loss:  1.73394747e-05\n",
      "training step: 110800\n",
      "loss:  0.000149083236\n",
      "training step: 110900\n",
      "loss:  5.05282551e-05\n",
      "training step: 111000\n",
      "loss:  1.90206683e-05\n",
      "training step: 111100\n",
      "loss:  6.46557237e-05\n",
      "training step: 111200\n",
      "loss:  3.17495178e-05\n",
      "training step: 111300\n",
      "loss:  1.34114907e-05\n",
      "training step: 111400\n",
      "loss:  7.6475e-05\n",
      "training step: 111500\n",
      "loss:  4.35890106e-05\n",
      "training step: 111600\n",
      "loss:  2.03381715e-05\n",
      "training step: 111700\n",
      "loss:  0.000140380682\n",
      "training step: 111800\n",
      "loss:  0.000112435395\n",
      "training step: 111900\n",
      "loss:  0.00018959833\n",
      "training step: 112000\n",
      "loss:  3.90239e-05\n",
      "training step: 112100\n",
      "loss:  2.5244075e-05\n",
      "training step: 112200\n",
      "loss:  1.66703267e-05\n",
      "training step: 112300\n",
      "loss:  1.79046383e-05\n",
      "training step: 112400\n",
      "loss:  1.08432323e-05\n",
      "training step: 112500\n",
      "loss:  0.000198137015\n",
      "training step: 112600\n",
      "loss:  6.45294931e-05\n",
      "training step: 112700\n",
      "loss:  2.33518276e-05\n",
      "training step: 112800\n",
      "loss:  9.91958859e-06\n",
      "training step: 112900\n",
      "loss:  0.000174859713\n",
      "training step: 113000\n",
      "loss:  4.2242882e-05\n",
      "training step: 113100\n",
      "loss:  1.69994528e-05\n",
      "training step: 113200\n",
      "loss:  1.10200281e-05\n",
      "training step: 113300\n",
      "loss:  2.63653073e-05\n",
      "training step: 113400\n",
      "loss:  8.80584485e-05\n",
      "training step: 113500\n",
      "loss:  1.80931675e-05\n",
      "training step: 113600\n",
      "loss:  1.04505725e-05\n",
      "training step: 113700\n",
      "loss:  3.74096817e-05\n",
      "training step: 113800\n",
      "loss:  3.76755124e-05\n",
      "training step: 113900\n",
      "loss:  1.83552511e-05\n",
      "training step: 114000\n",
      "loss:  3.00068441e-05\n",
      "training step: 114100\n",
      "loss:  2.3304292e-05\n",
      "training step: 114200\n",
      "loss:  1.05029094e-05\n",
      "training step: 114300\n",
      "loss:  1.11711479e-05\n",
      "training step: 114400\n",
      "loss:  1.21723979e-05\n",
      "training step: 114500\n",
      "loss:  1.0353102e-05\n",
      "training step: 114600\n",
      "loss:  1.93467877e-05\n",
      "training step: 114700\n",
      "loss:  3.01611544e-05\n",
      "training step: 114800\n",
      "loss:  1.14050217e-05\n",
      "training step: 114900\n",
      "loss:  5.48051685e-05\n",
      "training step: 115000\n",
      "loss:  3.78422519e-05\n",
      "training step: 115100\n",
      "loss:  1.23155878e-05\n",
      "training step: 115200\n",
      "loss:  1.5415193e-05\n",
      "training step: 115300\n",
      "loss:  2.72951966e-05\n",
      "training step: 115400\n",
      "loss:  1.80913667e-05\n",
      "training step: 115500\n",
      "loss:  1.67116195e-05\n",
      "training step: 115600\n",
      "loss:  3.88425397e-05\n",
      "training step: 115700\n",
      "loss:  9.72162434e-06\n",
      "training step: 115800\n",
      "loss:  1.35856972e-05\n",
      "training step: 115900\n",
      "loss:  1.12117768e-05\n",
      "training step: 116000\n",
      "loss:  2.49459408e-05\n",
      "training step: 116100\n",
      "loss:  4.82478172e-05\n",
      "training step: 116200\n",
      "loss:  1.97583868e-05\n",
      "training step: 116300\n",
      "loss:  1.13839369e-05\n",
      "training step: 116400\n",
      "loss:  7.0396236e-05\n",
      "training step: 116500\n",
      "loss:  1.21399371e-05\n",
      "training step: 116600\n",
      "loss:  0.000127232546\n",
      "training step: 116700\n",
      "loss:  2.66547813e-05\n",
      "training step: 116800\n",
      "loss:  1.05816043e-05\n",
      "training step: 116900\n",
      "loss:  9.937291e-06\n",
      "training step: 117000\n",
      "loss:  0.00026635424\n",
      "training step: 117100\n",
      "loss:  2.23670613e-05\n",
      "training step: 117200\n",
      "loss:  1.16360789e-05\n",
      "training step: 117300\n",
      "loss:  2.28012741e-05\n",
      "training step: 117400\n",
      "loss:  5.10135724e-05\n",
      "training step: 117500\n",
      "loss:  3.9386774e-05\n",
      "training step: 117600\n",
      "loss:  2.92481855e-05\n",
      "training step: 117700\n",
      "loss:  1.56479109e-05\n",
      "training step: 117800\n",
      "loss:  1.00384168e-05\n",
      "training step: 117900\n",
      "loss:  0.000170519284\n",
      "training step: 118000\n",
      "loss:  2.39992e-05\n",
      "training step: 118100\n",
      "loss:  1.10714118e-05\n",
      "training step: 118200\n",
      "loss:  1.16817137e-05\n",
      "training step: 118300\n",
      "loss:  0.000181783122\n",
      "training step: 118400\n",
      "loss:  1.07813239e-05\n",
      "training step: 118500\n",
      "loss:  3.85687126e-05\n",
      "training step: 118600\n",
      "loss:  2.01241746e-05\n",
      "training step: 118700\n",
      "loss:  9.31056929e-05\n",
      "training step: 118800\n",
      "loss:  2.01042949e-05\n",
      "training step: 118900\n",
      "loss:  1.69656778e-05\n",
      "training step: 119000\n",
      "loss:  1.98438665e-05\n",
      "training step: 119100\n",
      "loss:  1.82122803e-05\n",
      "training step: 119200\n",
      "loss:  9.33739739e-06\n",
      "training step: 119300\n",
      "loss:  2.57989868e-05\n",
      "training step: 119400\n",
      "loss:  2.99218973e-05\n",
      "training step: 119500\n",
      "loss:  1.82824751e-05\n",
      "training step: 119600\n",
      "loss:  5.91676762e-05\n",
      "training step: 119700\n",
      "loss:  5.22913033e-05\n",
      "training step: 119800\n",
      "loss:  0.000103301536\n",
      "training step: 119900\n",
      "loss:  1.55587932e-05\n",
      "training step: 120000\n",
      "loss:  5.31853912e-05\n",
      "training step: 120100\n",
      "loss:  4.91043211e-05\n",
      "training step: 120200\n",
      "loss:  1.16273959e-05\n",
      "training step: 120300\n",
      "loss:  0.00019450151\n",
      "training step: 120400\n",
      "loss:  2.65556573e-05\n",
      "training step: 120500\n",
      "loss:  1.50174365e-05\n",
      "training step: 120600\n",
      "loss:  8.72676173e-06\n",
      "training step: 120700\n",
      "loss:  6.52412855e-05\n",
      "training step: 120800\n",
      "loss:  2.44735111e-05\n",
      "training step: 120900\n",
      "loss:  8.7210683e-05\n",
      "training step: 121000\n",
      "loss:  2.64923328e-05\n",
      "training step: 121100\n",
      "loss:  5.25405558e-05\n",
      "training step: 121200\n",
      "loss:  1.56995811e-05\n",
      "training step: 121300\n",
      "loss:  6.24418317e-05\n",
      "training step: 121400\n",
      "loss:  1.23620612e-05\n",
      "training step: 121500\n",
      "loss:  1.23030177e-05\n",
      "training step: 121600\n",
      "loss:  9.99707754e-06\n",
      "training step: 121700\n",
      "loss:  1.42733779e-05\n",
      "training step: 121800\n",
      "loss:  7.2397e-05\n",
      "training step: 121900\n",
      "loss:  2.09289556e-05\n",
      "training step: 122000\n",
      "loss:  1.31465731e-05\n",
      "training step: 122100\n",
      "loss:  0.000132914109\n",
      "training step: 122200\n",
      "loss:  1.60155141e-05\n",
      "training step: 122300\n",
      "loss:  1.30547069e-05\n",
      "training step: 122400\n",
      "loss:  1.01267315e-05\n",
      "training step: 122500\n",
      "loss:  5.30158723e-05\n",
      "training step: 122600\n",
      "loss:  2.9234443e-05\n",
      "training step: 122700\n",
      "loss:  1.9813031e-05\n",
      "training step: 122800\n",
      "loss:  2.39893343e-05\n",
      "training step: 122900\n",
      "loss:  1.52418297e-05\n",
      "training step: 123000\n",
      "loss:  8.03544463e-05\n",
      "training step: 123100\n",
      "loss:  1.11751724e-05\n",
      "training step: 123200\n",
      "loss:  0.000133383641\n",
      "training step: 123300\n",
      "loss:  1.35400614e-05\n",
      "training step: 123400\n",
      "loss:  2.15413074e-05\n",
      "training step: 123500\n",
      "loss:  1.49519465e-05\n",
      "training step: 123600\n",
      "loss:  3.41920131e-05\n",
      "training step: 123700\n",
      "loss:  1.38440973e-05\n",
      "training step: 123800\n",
      "loss:  1.07838869e-05\n",
      "training step: 123900\n",
      "loss:  3.88915651e-05\n",
      "training step: 124000\n",
      "loss:  1.09504053e-05\n",
      "training step: 124100\n",
      "loss:  8.44261831e-06\n",
      "training step: 124200\n",
      "loss:  1.31754177e-05\n",
      "training step: 124300\n",
      "loss:  2.54053812e-05\n",
      "training step: 124400\n",
      "loss:  6.68448556e-05\n",
      "training step: 124500\n",
      "loss:  3.24967368e-05\n",
      "training step: 124600\n",
      "loss:  1.22033434e-05\n",
      "training step: 124700\n",
      "loss:  7.58315364e-05\n",
      "training step: 124800\n",
      "loss:  3.12709235e-05\n",
      "training step: 124900\n",
      "loss:  3.29814975e-05\n",
      "training step: 125000\n",
      "loss:  7.48671882e-05\n",
      "training step: 125100\n",
      "loss:  1.42632025e-05\n",
      "training step: 125200\n",
      "loss:  0.000106834646\n",
      "training step: 125300\n",
      "loss:  2.0439169e-05\n",
      "training step: 125400\n",
      "loss:  2.88765132e-05\n",
      "training step: 125500\n",
      "loss:  7.85428711e-05\n",
      "training step: 125600\n",
      "loss:  3.06657566e-05\n",
      "training step: 125700\n",
      "loss:  0.000107069733\n",
      "training step: 125800\n",
      "loss:  5.77200444e-05\n",
      "training step: 125900\n",
      "loss:  1.40897e-05\n",
      "training step: 126000\n",
      "loss:  9.88813e-05\n",
      "training step: 126100\n",
      "loss:  3.21839e-05\n",
      "training step: 126200\n",
      "loss:  3.42733656e-05\n",
      "training step: 126300\n",
      "loss:  1.29956443e-05\n",
      "training step: 126400\n",
      "loss:  1.11855634e-05\n",
      "training step: 126500\n",
      "loss:  2.35506704e-05\n",
      "training step: 126600\n",
      "loss:  2.06474797e-05\n",
      "training step: 126700\n",
      "loss:  1.1758103e-05\n",
      "training step: 126800\n",
      "loss:  1.22351521e-05\n",
      "training step: 126900\n",
      "loss:  9.8666369e-06\n",
      "training step: 127000\n",
      "loss:  2.27236051e-05\n",
      "training step: 127100\n",
      "loss:  8.90832e-06\n",
      "training step: 127200\n",
      "loss:  1.06647358e-05\n",
      "training step: 127300\n",
      "loss:  1.21140538e-05\n",
      "training step: 127400\n",
      "loss:  7.66471421e-05\n",
      "training step: 127500\n",
      "loss:  7.03322512e-05\n",
      "training step: 127600\n",
      "loss:  8.1307342e-05\n",
      "training step: 127700\n",
      "loss:  7.49222318e-06\n",
      "training step: 127800\n",
      "loss:  1.46643415e-05\n",
      "training step: 127900\n",
      "loss:  5.60842564e-05\n",
      "training step: 128000\n",
      "loss:  1.5006216e-05\n",
      "training step: 128100\n",
      "loss:  8.65653692e-06\n",
      "training step: 128200\n",
      "loss:  8.4433741e-05\n",
      "training step: 128300\n",
      "loss:  9.13252152e-05\n",
      "training step: 128400\n",
      "loss:  9.48767683e-06\n",
      "training step: 128500\n",
      "loss:  2.88264328e-05\n",
      "training step: 128600\n",
      "loss:  8.72011242e-06\n",
      "training step: 128700\n",
      "loss:  0.000164183992\n",
      "training step: 128800\n",
      "loss:  1.07493543e-05\n",
      "training step: 128900\n",
      "loss:  1.55433336e-05\n",
      "training step: 129000\n",
      "loss:  8.63860259e-06\n",
      "training step: 129100\n",
      "loss:  7.14429189e-06\n",
      "training step: 129200\n",
      "loss:  0.000331735238\n",
      "training step: 129300\n",
      "loss:  3.31550327e-05\n",
      "training step: 129400\n",
      "loss:  7.88616308e-06\n",
      "training step: 129500\n",
      "loss:  2.45871561e-05\n",
      "training step: 129600\n",
      "loss:  1.03975035e-05\n",
      "training step: 129700\n",
      "loss:  6.41569713e-05\n",
      "training step: 129800\n",
      "loss:  8.55769304e-06\n",
      "training step: 129900\n",
      "loss:  7.32454064e-05\n",
      "training step: 130000\n",
      "loss:  1.11036716e-05\n",
      "training step: 130100\n",
      "loss:  3.41879932e-05\n",
      "training step: 130200\n",
      "loss:  5.00733659e-05\n",
      "training step: 130300\n",
      "loss:  1.95687426e-05\n",
      "training step: 130400\n",
      "loss:  2.93771045e-05\n",
      "training step: 130500\n",
      "loss:  0.000120908124\n",
      "training step: 130600\n",
      "loss:  8.86118687e-06\n",
      "training step: 130700\n",
      "loss:  2.81878565e-05\n",
      "training step: 130800\n",
      "loss:  1.0469781e-05\n",
      "training step: 130900\n",
      "loss:  0.000113743394\n",
      "training step: 131000\n",
      "loss:  4.79793671e-05\n",
      "training step: 131100\n",
      "loss:  7.23627409e-06\n",
      "training step: 131200\n",
      "loss:  5.68418691e-05\n",
      "training step: 131300\n",
      "loss:  1.67900198e-05\n",
      "training step: 131400\n",
      "loss:  1.38004898e-05\n",
      "training step: 131500\n",
      "loss:  2.70538803e-05\n",
      "training step: 131600\n",
      "loss:  1.86150646e-05\n",
      "training step: 131700\n",
      "loss:  1.47452529e-05\n",
      "training step: 131800\n",
      "loss:  9.58285455e-06\n",
      "training step: 131900\n",
      "loss:  0.000227171055\n",
      "training step: 132000\n",
      "loss:  1.92069474e-05\n",
      "training step: 132100\n",
      "loss:  1.59689498e-05\n",
      "training step: 132200\n",
      "loss:  1.11955724e-05\n",
      "training step: 132300\n",
      "loss:  1.43812704e-05\n",
      "training step: 132400\n",
      "loss:  0.000128768283\n",
      "training step: 132500\n",
      "loss:  1.75264231e-05\n",
      "training step: 132600\n",
      "loss:  1.43866891e-05\n",
      "training step: 132700\n",
      "loss:  1.37575844e-05\n",
      "training step: 132800\n",
      "loss:  1.46777893e-05\n",
      "training step: 132900\n",
      "loss:  1.10482415e-05\n",
      "training step: 133000\n",
      "loss:  1.21071316e-05\n",
      "training step: 133100\n",
      "loss:  1.22995089e-05\n",
      "training step: 133200\n",
      "loss:  1.27459489e-05\n",
      "training step: 133300\n",
      "loss:  8.742315e-06\n",
      "training step: 133400\n",
      "loss:  4.5467008e-05\n",
      "training step: 133500\n",
      "loss:  6.30024297e-05\n",
      "training step: 133600\n",
      "loss:  1.01685291e-05\n",
      "training step: 133700\n",
      "loss:  1.00226844e-05\n",
      "training step: 133800\n",
      "loss:  8.41428846e-05\n",
      "training step: 133900\n",
      "loss:  7.65787263e-06\n",
      "training step: 134000\n",
      "loss:  4.94114611e-05\n",
      "training step: 134100\n",
      "loss:  0.000121109384\n",
      "training step: 134200\n",
      "loss:  7.41138574e-05\n",
      "training step: 134300\n",
      "loss:  8.54189784e-05\n",
      "training step: 134400\n",
      "loss:  1.14073218e-05\n",
      "training step: 134500\n",
      "loss:  7.94848311e-05\n",
      "training step: 134600\n",
      "loss:  3.12661359e-05\n",
      "training step: 134700\n",
      "loss:  8.63557216e-05\n",
      "training step: 134800\n",
      "loss:  4.13683665e-05\n",
      "training step: 134900\n",
      "loss:  8.76959439e-06\n",
      "training step: 135000\n",
      "loss:  9.6129188e-06\n",
      "training step: 135100\n",
      "loss:  7.41757776e-06\n",
      "training step: 135200\n",
      "loss:  9.82184883e-06\n",
      "training step: 135300\n",
      "loss:  2.68772128e-05\n",
      "training step: 135400\n",
      "loss:  1.24745929e-05\n",
      "training step: 135500\n",
      "loss:  7.69707549e-06\n",
      "training step: 135600\n",
      "loss:  6.42455489e-05\n",
      "training step: 135700\n",
      "loss:  7.11035391e-05\n",
      "training step: 135800\n",
      "loss:  1.37504949e-05\n",
      "training step: 135900\n",
      "loss:  7.41158283e-06\n",
      "training step: 136000\n",
      "loss:  0.000100664911\n",
      "training step: 136100\n",
      "loss:  1.09162311e-05\n",
      "training step: 136200\n",
      "loss:  0.000162052253\n",
      "training step: 136300\n",
      "loss:  9.42895076e-06\n",
      "training step: 136400\n",
      "loss:  5.05619792e-05\n",
      "training step: 136500\n",
      "loss:  7.4202444e-06\n",
      "training step: 136600\n",
      "loss:  7.45972466e-06\n",
      "training step: 136700\n",
      "loss:  9.61761634e-06\n",
      "training step: 136800\n",
      "loss:  1.83769152e-05\n",
      "training step: 136900\n",
      "loss:  9.81059929e-06\n",
      "training step: 137000\n",
      "loss:  1.77684815e-05\n",
      "training step: 137100\n",
      "loss:  1.62312826e-05\n",
      "training step: 137200\n",
      "loss:  1.08925578e-05\n",
      "training step: 137300\n",
      "loss:  2.91264496e-05\n",
      "training step: 137400\n",
      "loss:  4.48637365e-05\n",
      "training step: 137500\n",
      "loss:  0.000104567262\n",
      "training step: 137600\n",
      "loss:  2.78296884e-05\n",
      "training step: 137700\n",
      "loss:  1.34856718e-05\n",
      "training step: 137800\n",
      "loss:  8.94647746e-06\n",
      "training step: 137900\n",
      "loss:  1.31696461e-05\n",
      "training step: 138000\n",
      "loss:  6.61712284e-06\n",
      "training step: 138100\n",
      "loss:  1.05610025e-05\n",
      "training step: 138200\n",
      "loss:  0.000119990902\n",
      "training step: 138300\n",
      "loss:  1.87416717e-05\n",
      "training step: 138400\n",
      "loss:  6.569572e-05\n",
      "training step: 138500\n",
      "loss:  4.0270952e-05\n",
      "training step: 138600\n",
      "loss:  9.57598531e-05\n",
      "training step: 138700\n",
      "loss:  1.34204347e-05\n",
      "training step: 138800\n",
      "loss:  6.15615281e-05\n",
      "training step: 138900\n",
      "loss:  4.48106111e-05\n",
      "training step: 139000\n",
      "loss:  1.57519589e-05\n",
      "training step: 139100\n",
      "loss:  9.34775162e-05\n",
      "training step: 139200\n",
      "loss:  7.37033633e-06\n",
      "training step: 139300\n",
      "loss:  4.79179798e-05\n",
      "training step: 139400\n",
      "loss:  3.94875133e-05\n",
      "training step: 139500\n",
      "loss:  0.000135176699\n",
      "training step: 139600\n",
      "loss:  2.23511561e-05\n",
      "training step: 139700\n",
      "loss:  4.6553454e-05\n",
      "training step: 139800\n",
      "loss:  1.53710498e-05\n",
      "training step: 139900\n",
      "loss:  6.0930488e-05\n",
      "training step: 140000\n",
      "loss:  1.3234966e-05\n",
      "training step: 140100\n",
      "loss:  7.03251453e-06\n",
      "training step: 140200\n",
      "loss:  1.87147634e-05\n",
      "training step: 140300\n",
      "loss:  8.05287618e-06\n",
      "training step: 140400\n",
      "loss:  6.8666368e-06\n",
      "training step: 140500\n",
      "loss:  2.65586586e-05\n",
      "training step: 140600\n",
      "loss:  1.69600135e-05\n",
      "training step: 140700\n",
      "loss:  8.78157152e-06\n",
      "training step: 140800\n",
      "loss:  6.09746667e-06\n",
      "training step: 140900\n",
      "loss:  3.83169136e-05\n",
      "training step: 141000\n",
      "loss:  8.06625903e-05\n",
      "training step: 141100\n",
      "loss:  1.56790138e-05\n",
      "training step: 141200\n",
      "loss:  2.22737708e-05\n",
      "training step: 141300\n",
      "loss:  7.33096476e-06\n",
      "training step: 141400\n",
      "loss:  6.10062398e-06\n",
      "training step: 141500\n",
      "loss:  0.000224204981\n",
      "training step: 141600\n",
      "loss:  0.000235556043\n",
      "training step: 141700\n",
      "loss:  1.83501415e-05\n",
      "training step: 141800\n",
      "loss:  8.58669318e-06\n",
      "training step: 141900\n",
      "loss:  8.17824275e-06\n",
      "training step: 142000\n",
      "loss:  3.04229306e-05\n",
      "training step: 142100\n",
      "loss:  1.01135993e-05\n",
      "training step: 142200\n",
      "loss:  6.53332318e-05\n",
      "training step: 142300\n",
      "loss:  9.12262749e-06\n",
      "training step: 142400\n",
      "loss:  0.000112959206\n",
      "training step: 142500\n",
      "loss:  7.01549e-06\n",
      "training step: 142600\n",
      "loss:  0.000122183803\n",
      "training step: 142700\n",
      "loss:  1.08046106e-05\n",
      "training step: 142800\n",
      "loss:  7.23862422e-06\n",
      "training step: 142900\n",
      "loss:  5.60361041e-05\n",
      "training step: 143000\n",
      "loss:  3.83536e-05\n",
      "training step: 143100\n",
      "loss:  1.74495799e-05\n",
      "training step: 143200\n",
      "loss:  0.000139814889\n",
      "training step: 143300\n",
      "loss:  0.000139303302\n",
      "training step: 143400\n",
      "loss:  7.08492234e-06\n",
      "training step: 143500\n",
      "loss:  0.000147017665\n",
      "training step: 143600\n",
      "loss:  3.68229812e-05\n",
      "training step: 143700\n",
      "loss:  8.47522097e-05\n",
      "training step: 143800\n",
      "loss:  4.34554859e-05\n",
      "training step: 143900\n",
      "loss:  6.08523806e-05\n",
      "training step: 144000\n",
      "loss:  4.51942942e-05\n",
      "training step: 144100\n",
      "loss:  8.45647592e-05\n",
      "training step: 144200\n",
      "loss:  1.9566718e-05\n",
      "training step: 144300\n",
      "loss:  9.41597682e-06\n",
      "training step: 144400\n",
      "loss:  0.000199208065\n",
      "training step: 144500\n",
      "loss:  1.7493734e-05\n",
      "training step: 144600\n",
      "loss:  2.42823535e-05\n",
      "training step: 144700\n",
      "loss:  5.83541696e-05\n",
      "training step: 144800\n",
      "loss:  9.4812367e-06\n",
      "training step: 144900\n",
      "loss:  5.77834453e-06\n",
      "training step: 145000\n",
      "loss:  2.71558383e-05\n",
      "training step: 145100\n",
      "loss:  8.83271e-06\n",
      "training step: 145200\n",
      "loss:  7.46123769e-05\n",
      "training step: 145300\n",
      "loss:  2.31578142e-05\n",
      "training step: 145400\n",
      "loss:  1.00806419e-05\n",
      "training step: 145500\n",
      "loss:  1.05384042e-05\n",
      "training step: 145600\n",
      "loss:  5.07069453e-05\n",
      "training step: 145700\n",
      "loss:  3.59086334e-05\n",
      "training step: 145800\n",
      "loss:  9.88032043e-05\n",
      "training step: 145900\n",
      "loss:  3.63632244e-05\n",
      "training step: 146000\n",
      "loss:  1.53675082e-05\n",
      "training step: 146100\n",
      "loss:  4.63682591e-05\n",
      "training step: 146200\n",
      "loss:  5.62238165e-06\n",
      "training step: 146300\n",
      "loss:  7.80566097e-06\n",
      "training step: 146400\n",
      "loss:  1.00788866e-05\n",
      "training step: 146500\n",
      "loss:  3.01124128e-05\n",
      "training step: 146600\n",
      "loss:  3.91695867e-05\n",
      "training step: 146700\n",
      "loss:  1.45550612e-05\n",
      "training step: 146800\n",
      "loss:  0.000103559956\n",
      "training step: 146900\n",
      "loss:  9.10492417e-06\n",
      "training step: 147000\n",
      "loss:  1.74770903e-05\n",
      "training step: 147100\n",
      "loss:  3.98028e-05\n",
      "training step: 147200\n",
      "loss:  9.26298253e-06\n",
      "training step: 147300\n",
      "loss:  6.17462092e-06\n",
      "training step: 147400\n",
      "loss:  7.56608779e-06\n",
      "training step: 147500\n",
      "loss:  1.24236594e-05\n",
      "training step: 147600\n",
      "loss:  8.16161e-06\n",
      "training step: 147700\n",
      "loss:  4.54812e-05\n",
      "training step: 147800\n",
      "loss:  2.24802916e-05\n",
      "training step: 147900\n",
      "loss:  1.48154286e-05\n",
      "training step: 148000\n",
      "loss:  9.88836e-06\n",
      "training step: 148100\n",
      "loss:  2.24895666e-05\n",
      "training step: 148200\n",
      "loss:  0.000106952502\n",
      "training step: 148300\n",
      "loss:  2.7940745e-05\n",
      "training step: 148400\n",
      "loss:  4.05625069e-05\n",
      "training step: 148500\n",
      "loss:  6.06480944e-06\n",
      "training step: 148600\n",
      "loss:  7.00585442e-05\n",
      "training step: 148700\n",
      "loss:  6.10129282e-05\n",
      "training step: 148800\n",
      "loss:  1.24323988e-05\n",
      "training step: 148900\n",
      "loss:  1.20269387e-05\n",
      "training step: 149000\n",
      "loss:  5.62417654e-06\n",
      "training step: 149100\n",
      "loss:  2.29131165e-05\n",
      "training step: 149200\n",
      "loss:  7.14647422e-06\n",
      "training step: 149300\n",
      "loss:  1.62148899e-05\n",
      "training step: 149400\n",
      "loss:  1.67142916e-05\n",
      "training step: 149500\n",
      "loss:  7.69341477e-06\n",
      "training step: 149600\n",
      "loss:  6.89308763e-06\n",
      "training step: 149700\n",
      "loss:  1.44858486e-05\n",
      "training step: 149800\n",
      "loss:  4.37035742e-05\n",
      "training step: 149900\n",
      "loss:  8.60769887e-06\n",
      "training step: 150000\n",
      "loss:  5.0603885e-05\n",
      "training step: 150100\n",
      "loss:  6.28791531e-06\n",
      "training step: 150200\n",
      "loss:  0.000105115549\n",
      "training step: 150300\n",
      "loss:  1.33532521e-05\n",
      "training step: 150400\n",
      "loss:  0.0001662633\n",
      "training step: 150500\n",
      "loss:  1.48368126e-05\n",
      "training step: 150600\n",
      "loss:  4.66297133e-05\n",
      "training step: 150700\n",
      "loss:  2.21207265e-05\n",
      "training step: 150800\n",
      "loss:  4.567836e-05\n",
      "training step: 150900\n",
      "loss:  1.43851839e-05\n",
      "training step: 151000\n",
      "loss:  2.12998202e-05\n",
      "training step: 151100\n",
      "loss:  6.63422043e-06\n",
      "training step: 151200\n",
      "loss:  6.43344829e-05\n",
      "training step: 151300\n",
      "loss:  5.44863906e-06\n",
      "training step: 151400\n",
      "loss:  8.56525821e-06\n",
      "training step: 151500\n",
      "loss:  1.61158168e-05\n",
      "training step: 151600\n",
      "loss:  2.32984e-05\n",
      "training step: 151700\n",
      "loss:  3.20548679e-05\n",
      "training step: 151800\n",
      "loss:  2.91425331e-05\n",
      "training step: 151900\n",
      "loss:  8.70565e-05\n",
      "training step: 152000\n",
      "loss:  6.2001177e-06\n",
      "training step: 152100\n",
      "loss:  0.000110278481\n",
      "training step: 152200\n",
      "loss:  7.9604979e-05\n",
      "training step: 152300\n",
      "loss:  2.72247471e-05\n",
      "training step: 152400\n",
      "loss:  8.97342761e-06\n",
      "training step: 152500\n",
      "loss:  6.39149148e-05\n",
      "training step: 152600\n",
      "loss:  2.63228576e-05\n",
      "training step: 152700\n",
      "loss:  4.44214274e-05\n",
      "training step: 152800\n",
      "loss:  4.56668822e-05\n",
      "training step: 152900\n",
      "loss:  1.05624149e-05\n",
      "training step: 153000\n",
      "loss:  8.69741325e-06\n",
      "training step: 153100\n",
      "loss:  3.14627905e-05\n",
      "training step: 153200\n",
      "loss:  9.92141e-06\n",
      "training step: 153300\n",
      "loss:  6.34922617e-05\n",
      "training step: 153400\n",
      "loss:  4.10757275e-05\n",
      "training step: 153500\n",
      "loss:  5.88652847e-06\n",
      "training step: 153600\n",
      "loss:  6.06262256e-06\n",
      "training step: 153700\n",
      "loss:  4.49759318e-05\n",
      "training step: 153800\n",
      "loss:  1.00830712e-05\n",
      "training step: 153900\n",
      "loss:  5.97983126e-06\n",
      "training step: 154000\n",
      "loss:  6.60947126e-06\n",
      "training step: 154100\n",
      "loss:  9.71348163e-06\n",
      "training step: 154200\n",
      "loss:  0.000140769276\n",
      "training step: 154300\n",
      "loss:  8.69985797e-06\n",
      "training step: 154400\n",
      "loss:  7.63165463e-06\n",
      "training step: 154500\n",
      "loss:  3.68603578e-05\n",
      "training step: 154600\n",
      "loss:  6.26442079e-06\n",
      "training step: 154700\n",
      "loss:  2.2892109e-05\n",
      "training step: 154800\n",
      "loss:  1.29037162e-05\n",
      "training step: 154900\n",
      "loss:  1.91291801e-05\n",
      "training step: 155000\n",
      "loss:  2.90624666e-05\n",
      "training step: 155100\n",
      "loss:  1.47535775e-05\n",
      "training step: 155200\n",
      "loss:  1.07158394e-05\n",
      "training step: 155300\n",
      "loss:  8.27654076e-05\n",
      "training step: 155400\n",
      "loss:  2.77702511e-05\n",
      "training step: 155500\n",
      "loss:  1.94945314e-05\n",
      "training step: 155600\n",
      "loss:  7.77528658e-06\n",
      "training step: 155700\n",
      "loss:  8.1621e-05\n",
      "training step: 155800\n",
      "loss:  7.04230797e-06\n",
      "training step: 155900\n",
      "loss:  2.09797508e-05\n",
      "training step: 156000\n",
      "loss:  5.47750969e-05\n",
      "training step: 156100\n",
      "loss:  1.39278363e-05\n",
      "training step: 156200\n",
      "loss:  6.64211693e-05\n",
      "training step: 156300\n",
      "loss:  7.28036594e-06\n",
      "training step: 156400\n",
      "loss:  1.25101242e-05\n",
      "training step: 156500\n",
      "loss:  1.29558712e-05\n",
      "training step: 156600\n",
      "loss:  3.23974164e-05\n",
      "training step: 156700\n",
      "loss:  6.94695382e-06\n",
      "training step: 156800\n",
      "loss:  9.30169153e-06\n",
      "training step: 156900\n",
      "loss:  5.51623816e-06\n",
      "training step: 157000\n",
      "loss:  9.01198364e-05\n",
      "training step: 157100\n",
      "loss:  6.18405647e-06\n",
      "training step: 157200\n",
      "loss:  5.82543635e-06\n",
      "training step: 157300\n",
      "loss:  5.71812197e-05\n",
      "training step: 157400\n",
      "loss:  1.86657689e-05\n",
      "training step: 157500\n",
      "loss:  1.30537774e-05\n",
      "training step: 157600\n",
      "loss:  2.62280519e-05\n",
      "training step: 157700\n",
      "loss:  4.53329631e-05\n",
      "training step: 157800\n",
      "loss:  9.78022217e-06\n",
      "training step: 157900\n",
      "loss:  2.4044024e-05\n",
      "training step: 158000\n",
      "loss:  0.000148893116\n",
      "training step: 158100\n",
      "loss:  7.49177343e-05\n",
      "training step: 158200\n",
      "loss:  3.30454641e-05\n",
      "training step: 158300\n",
      "loss:  4.43110912e-05\n",
      "training step: 158400\n",
      "loss:  2.34296404e-05\n",
      "training step: 158500\n",
      "loss:  1.79328235e-05\n",
      "training step: 158600\n",
      "loss:  4.0583127e-05\n",
      "training step: 158700\n",
      "loss:  1.16420724e-05\n",
      "training step: 158800\n",
      "loss:  9.19668601e-05\n",
      "training step: 158900\n",
      "loss:  6.82998143e-06\n",
      "training step: 159000\n",
      "loss:  8.71781e-06\n",
      "training step: 159100\n",
      "loss:  5.49497345e-05\n",
      "training step: 159200\n",
      "loss:  6.56405828e-05\n",
      "training step: 159300\n",
      "loss:  1.05667486e-05\n",
      "training step: 159400\n",
      "loss:  6.10490315e-06\n",
      "training step: 159500\n",
      "loss:  3.77096221e-05\n",
      "training step: 159600\n",
      "loss:  1.34803204e-05\n",
      "training step: 159700\n",
      "loss:  8.22853235e-06\n",
      "training step: 159800\n",
      "loss:  1.87231108e-05\n",
      "training step: 159900\n",
      "loss:  9.01221865e-05\n",
      "training step: 160000\n",
      "loss:  1.98049129e-05\n",
      "training step: 160100\n",
      "loss:  2.3593193e-05\n",
      "training step: 160200\n",
      "loss:  3.19051651e-05\n",
      "training step: 160300\n",
      "loss:  6.84464112e-06\n",
      "training step: 160400\n",
      "loss:  2.95224345e-05\n",
      "training step: 160500\n",
      "loss:  1.17709824e-05\n",
      "training step: 160600\n",
      "loss:  3.57700919e-05\n",
      "training step: 160700\n",
      "loss:  6.91698688e-06\n",
      "training step: 160800\n",
      "loss:  6.2650397e-06\n",
      "training step: 160900\n",
      "loss:  5.76103121e-05\n",
      "training step: 161000\n",
      "loss:  1.4631124e-05\n",
      "training step: 161100\n",
      "loss:  0.000104167855\n",
      "training step: 161200\n",
      "loss:  1.12512898e-05\n",
      "training step: 161300\n",
      "loss:  1.34635566e-05\n",
      "training step: 161400\n",
      "loss:  4.61566342e-05\n",
      "training step: 161500\n",
      "loss:  8.46815874e-06\n",
      "training step: 161600\n",
      "loss:  5.68854239e-06\n",
      "training step: 161700\n",
      "loss:  1.54221052e-05\n",
      "training step: 161800\n",
      "loss:  1.08002569e-05\n",
      "training step: 161900\n",
      "loss:  9.28759437e-06\n",
      "training step: 162000\n",
      "loss:  2.07874e-05\n",
      "training step: 162100\n",
      "loss:  1.03870425e-05\n",
      "training step: 162200\n",
      "loss:  1.7672779e-05\n",
      "training step: 162300\n",
      "loss:  3.71798742e-05\n",
      "training step: 162400\n",
      "loss:  1.75000114e-05\n",
      "training step: 162500\n",
      "loss:  1.73712178e-05\n",
      "training step: 162600\n",
      "loss:  3.29098148e-05\n",
      "training step: 162700\n",
      "loss:  3.21799198e-05\n",
      "training step: 162800\n",
      "loss:  3.98993943e-05\n",
      "training step: 162900\n",
      "loss:  6.59993839e-06\n",
      "training step: 163000\n",
      "loss:  1.2119388e-05\n",
      "training step: 163100\n",
      "loss:  9.44584463e-05\n",
      "training step: 163200\n",
      "loss:  1.48147219e-05\n",
      "training step: 163300\n",
      "loss:  5.12445058e-06\n",
      "training step: 163400\n",
      "loss:  4.58366849e-05\n",
      "training step: 163500\n",
      "loss:  1.97301179e-05\n",
      "training step: 163600\n",
      "loss:  5.61119668e-06\n",
      "training step: 163700\n",
      "loss:  6.24351378e-06\n",
      "training step: 163800\n",
      "loss:  9.47227e-06\n",
      "training step: 163900\n",
      "loss:  8.14613668e-06\n",
      "training step: 164000\n",
      "loss:  3.0201285e-05\n",
      "training step: 164100\n",
      "loss:  5.4589349e-05\n",
      "training step: 164200\n",
      "loss:  3.18621787e-05\n",
      "training step: 164300\n",
      "loss:  4.96885559e-06\n",
      "training step: 164400\n",
      "loss:  2.55715622e-05\n",
      "training step: 164500\n",
      "loss:  1.10586052e-05\n",
      "training step: 164600\n",
      "loss:  8.91275e-05\n",
      "training step: 164700\n",
      "loss:  9.64349783e-06\n",
      "training step: 164800\n",
      "loss:  3.00783267e-05\n",
      "training step: 164900\n",
      "loss:  2.97452752e-05\n",
      "training step: 165000\n",
      "loss:  5.19631067e-05\n",
      "training step: 165100\n",
      "loss:  1.24978824e-05\n",
      "training step: 165200\n",
      "loss:  2.39735709e-05\n",
      "training step: 165300\n",
      "loss:  2.0286634e-05\n",
      "training step: 165400\n",
      "loss:  6.40980215e-06\n",
      "training step: 165500\n",
      "loss:  4.59342955e-05\n",
      "training step: 165600\n",
      "loss:  5.56014238e-05\n",
      "training step: 165700\n",
      "loss:  8.21288631e-06\n",
      "training step: 165800\n",
      "loss:  3.24665561e-05\n",
      "training step: 165900\n",
      "loss:  6.04576599e-06\n",
      "training step: 166000\n",
      "loss:  8.12063e-06\n",
      "training step: 166100\n",
      "loss:  1.26273035e-05\n",
      "training step: 166200\n",
      "loss:  0.000196569177\n",
      "training step: 166300\n",
      "loss:  4.6069883e-05\n",
      "training step: 166400\n",
      "loss:  0.000111858135\n",
      "training step: 166500\n",
      "loss:  8.22683141e-05\n",
      "training step: 166600\n",
      "loss:  1.40766424e-05\n",
      "training step: 166700\n",
      "loss:  5.36350899e-06\n",
      "training step: 166800\n",
      "loss:  6.71537282e-06\n",
      "training step: 166900\n",
      "loss:  5.08358826e-05\n",
      "training step: 167000\n",
      "loss:  5.98813267e-06\n",
      "training step: 167100\n",
      "loss:  2.23727275e-05\n",
      "training step: 167200\n",
      "loss:  1.666752e-05\n",
      "training step: 167300\n",
      "loss:  7.98583133e-05\n",
      "training step: 167400\n",
      "loss:  3.45384797e-05\n",
      "training step: 167500\n",
      "loss:  1.53266428e-05\n",
      "training step: 167600\n",
      "loss:  6.80755375e-06\n",
      "training step: 167700\n",
      "loss:  2.20097609e-05\n",
      "training step: 167800\n",
      "loss:  2.639388e-05\n",
      "training step: 167900\n",
      "loss:  5.25988571e-06\n",
      "training step: 168000\n",
      "loss:  8.395642e-06\n",
      "training step: 168100\n",
      "loss:  1.92274529e-05\n",
      "training step: 168200\n",
      "loss:  1.85005065e-05\n",
      "training step: 168300\n",
      "loss:  2.1195774e-05\n",
      "training step: 168400\n",
      "loss:  3.66450549e-05\n",
      "training step: 168500\n",
      "loss:  1.42664776e-05\n",
      "training step: 168600\n",
      "loss:  1.60642339e-05\n",
      "training step: 168700\n",
      "loss:  5.0034273e-06\n",
      "training step: 168800\n",
      "loss:  1.53775363e-05\n",
      "training step: 168900\n",
      "loss:  5.74926935e-06\n",
      "training step: 169000\n",
      "loss:  4.82252617e-05\n",
      "training step: 169100\n",
      "loss:  2.06163659e-05\n",
      "training step: 169200\n",
      "loss:  1.1187517e-05\n",
      "training step: 169300\n",
      "loss:  5.6335939e-05\n",
      "training step: 169400\n",
      "loss:  2.3487959e-05\n",
      "training step: 169500\n",
      "loss:  1.89076654e-05\n",
      "training step: 169600\n",
      "loss:  5.68925534e-05\n",
      "training step: 169700\n",
      "loss:  1.50740989e-05\n",
      "training step: 169800\n",
      "loss:  1.01978158e-05\n",
      "training step: 169900\n",
      "loss:  1.09257116e-05\n",
      "training step: 170000\n",
      "loss:  0.000151264889\n",
      "training step: 170100\n",
      "loss:  2.14033644e-05\n",
      "training step: 170200\n",
      "loss:  4.41255816e-06\n",
      "training step: 170300\n",
      "loss:  4.04041239e-05\n",
      "training step: 170400\n",
      "loss:  1.24580474e-05\n",
      "training step: 170500\n",
      "loss:  1.12992821e-05\n",
      "training step: 170600\n",
      "loss:  1.56657279e-05\n",
      "training step: 170700\n",
      "loss:  5.721872e-05\n",
      "training step: 170800\n",
      "loss:  2.60456891e-05\n",
      "training step: 170900\n",
      "loss:  5.96062591e-06\n",
      "training step: 171000\n",
      "loss:  4.76408422e-05\n",
      "training step: 171100\n",
      "loss:  9.34019408e-06\n",
      "training step: 171200\n",
      "loss:  0.000170365194\n",
      "training step: 171300\n",
      "loss:  7.63152057e-05\n",
      "training step: 171400\n",
      "loss:  3.41375344e-05\n",
      "training step: 171500\n",
      "loss:  6.93822367e-05\n",
      "training step: 171600\n",
      "loss:  6.89324734e-05\n",
      "training step: 171700\n",
      "loss:  8.61437547e-06\n",
      "training step: 171800\n",
      "loss:  4.94038295e-06\n",
      "training step: 171900\n",
      "loss:  9.56696113e-06\n",
      "training step: 172000\n",
      "loss:  5.01051e-05\n",
      "training step: 172100\n",
      "loss:  0.000111585206\n",
      "training step: 172200\n",
      "loss:  4.51355027e-06\n",
      "training step: 172300\n",
      "loss:  0.000137220457\n",
      "training step: 172400\n",
      "loss:  7.62863556e-06\n",
      "training step: 172500\n",
      "loss:  6.45339969e-06\n",
      "training step: 172600\n",
      "loss:  6.3886373e-06\n",
      "training step: 172700\n",
      "loss:  6.88578302e-06\n",
      "training step: 172800\n",
      "loss:  7.26691769e-06\n",
      "training step: 172900\n",
      "loss:  4.71596286e-05\n",
      "training step: 173000\n",
      "loss:  6.7513497e-06\n",
      "training step: 173100\n",
      "loss:  2.12657924e-05\n",
      "training step: 173200\n",
      "loss:  2.71664721e-05\n",
      "training step: 173300\n",
      "loss:  4.92954268e-06\n",
      "training step: 173400\n",
      "loss:  9.69055545e-05\n",
      "training step: 173500\n",
      "loss:  4.70065679e-05\n",
      "training step: 173600\n",
      "loss:  5.30187799e-06\n",
      "training step: 173700\n",
      "loss:  0.000195456712\n",
      "training step: 173800\n",
      "loss:  4.83304939e-06\n",
      "training step: 173900\n",
      "loss:  6.08088976e-06\n",
      "training step: 174000\n",
      "loss:  5.73105e-06\n",
      "training step: 174100\n",
      "loss:  8.99794904e-05\n",
      "training step: 174200\n",
      "loss:  4.40569784e-05\n",
      "training step: 174300\n",
      "loss:  3.3999695e-05\n",
      "training step: 174400\n",
      "loss:  8.40613666e-06\n",
      "training step: 174500\n",
      "loss:  1.42019799e-05\n",
      "training step: 174600\n",
      "loss:  3.71715105e-05\n",
      "training step: 174700\n",
      "loss:  4.7902864e-05\n",
      "training step: 174800\n",
      "loss:  4.33949344e-06\n",
      "training step: 174900\n",
      "loss:  6.1679707e-06\n",
      "training step: 175000\n",
      "loss:  1.42638883e-05\n",
      "training step: 175100\n",
      "loss:  2.85444276e-05\n",
      "training step: 175200\n",
      "loss:  2.78924399e-05\n",
      "training step: 175300\n",
      "loss:  6.81595247e-06\n",
      "training step: 175400\n",
      "loss:  3.26649861e-05\n",
      "training step: 175500\n",
      "loss:  7.11548455e-06\n",
      "training step: 175600\n",
      "loss:  6.68307548e-05\n",
      "training step: 175700\n",
      "loss:  4.16902731e-06\n",
      "training step: 175800\n",
      "loss:  3.86892225e-06\n",
      "training step: 175900\n",
      "loss:  6.60951036e-06\n",
      "training step: 176000\n",
      "loss:  9.73276929e-06\n",
      "training step: 176100\n",
      "loss:  1.87523437e-05\n",
      "training step: 176200\n",
      "loss:  1.85958252e-05\n",
      "training step: 176300\n",
      "loss:  4.15854402e-06\n",
      "training step: 176400\n",
      "loss:  3.94525023e-06\n",
      "training step: 176500\n",
      "loss:  1.17474674e-05\n",
      "training step: 176600\n",
      "loss:  4.54305e-06\n",
      "training step: 176700\n",
      "loss:  7.28223949e-06\n",
      "training step: 176800\n",
      "loss:  1.1750305e-05\n",
      "training step: 176900\n",
      "loss:  1.80416046e-05\n",
      "training step: 177000\n",
      "loss:  2.31849535e-05\n",
      "training step: 177100\n",
      "loss:  9.9737581e-06\n",
      "training step: 177200\n",
      "loss:  1.67044182e-05\n",
      "training step: 177300\n",
      "loss:  3.69108857e-05\n",
      "training step: 177400\n",
      "loss:  6.64222825e-05\n",
      "training step: 177500\n",
      "loss:  2.66911611e-05\n",
      "training step: 177600\n",
      "loss:  6.86503472e-06\n",
      "training step: 177700\n",
      "loss:  3.30517942e-05\n",
      "training step: 177800\n",
      "loss:  1.31597481e-05\n",
      "training step: 177900\n",
      "loss:  4.44778334e-06\n",
      "training step: 178000\n",
      "loss:  3.38465725e-05\n",
      "training step: 178100\n",
      "loss:  3.66198583e-05\n",
      "training step: 178200\n",
      "loss:  6.4194096e-06\n",
      "training step: 178300\n",
      "loss:  7.32882254e-05\n",
      "training step: 178400\n",
      "loss:  5.64794254e-06\n",
      "training step: 178500\n",
      "loss:  1.06020752e-05\n",
      "training step: 178600\n",
      "loss:  2.8546192e-05\n",
      "training step: 178700\n",
      "loss:  6.66782944e-05\n",
      "training step: 178800\n",
      "loss:  3.03049874e-05\n",
      "training step: 178900\n",
      "loss:  5.49578654e-05\n",
      "training step: 179000\n",
      "loss:  7.2186449e-06\n",
      "training step: 179100\n",
      "loss:  1.16257597e-05\n",
      "training step: 179200\n",
      "loss:  2.16073477e-05\n",
      "training step: 179300\n",
      "loss:  8.35021092e-06\n",
      "training step: 179400\n",
      "loss:  1.03743632e-05\n",
      "training step: 179500\n",
      "loss:  9.57643624e-06\n",
      "training step: 179600\n",
      "loss:  8.66259597e-06\n",
      "training step: 179700\n",
      "loss:  1.12602629e-05\n",
      "training step: 179800\n",
      "loss:  1.4043384e-05\n",
      "training step: 179900\n",
      "loss:  1.06391208e-05\n",
      "training step: 180000\n",
      "loss:  4.85420787e-05\n",
      "training step: 180100\n",
      "loss:  4.36327682e-05\n",
      "training step: 180200\n",
      "loss:  6.79962e-05\n",
      "training step: 180300\n",
      "loss:  1.93080577e-05\n",
      "training step: 180400\n",
      "loss:  9.72760754e-05\n",
      "training step: 180500\n",
      "loss:  2.22492909e-05\n",
      "training step: 180600\n",
      "loss:  4.22235871e-06\n",
      "training step: 180700\n",
      "loss:  1.31883162e-05\n",
      "training step: 180800\n",
      "loss:  1.27402764e-05\n",
      "training step: 180900\n",
      "loss:  1.56168226e-05\n",
      "training step: 181000\n",
      "loss:  6.21048548e-06\n",
      "training step: 181100\n",
      "loss:  2.86744726e-05\n",
      "training step: 181200\n",
      "loss:  2.81561752e-05\n",
      "training step: 181300\n",
      "loss:  1.38854684e-05\n",
      "training step: 181400\n",
      "loss:  1.01639125e-05\n",
      "training step: 181500\n",
      "loss:  1.82095955e-05\n",
      "training step: 181600\n",
      "loss:  6.96926691e-06\n",
      "training step: 181700\n",
      "loss:  3.57226272e-05\n",
      "training step: 181800\n",
      "loss:  1.36507615e-05\n",
      "training step: 181900\n",
      "loss:  6.63592346e-06\n",
      "training step: 182000\n",
      "loss:  1.56697297e-05\n",
      "training step: 182100\n",
      "loss:  8.24134622e-06\n",
      "training step: 182200\n",
      "loss:  1.38381029e-05\n",
      "training step: 182300\n",
      "loss:  3.02615736e-05\n",
      "training step: 182400\n",
      "loss:  2.20179227e-05\n",
      "training step: 182500\n",
      "loss:  8.91019154e-05\n",
      "training step: 182600\n",
      "loss:  1.1801897e-05\n",
      "training step: 182700\n",
      "loss:  5.68707492e-06\n",
      "training step: 182800\n",
      "loss:  5.04604504e-05\n",
      "training step: 182900\n",
      "loss:  3.2527616e-05\n",
      "training step: 183000\n",
      "loss:  1.84517939e-05\n",
      "training step: 183100\n",
      "loss:  3.65275628e-05\n",
      "training step: 183200\n",
      "loss:  0.000105320796\n",
      "training step: 183300\n",
      "loss:  2.30856531e-05\n",
      "training step: 183400\n",
      "loss:  4.46227641e-05\n",
      "training step: 183500\n",
      "loss:  7.82477364e-05\n",
      "training step: 183600\n",
      "loss:  1.0848541e-05\n",
      "training step: 183700\n",
      "loss:  4.45689784e-06\n",
      "training step: 183800\n",
      "loss:  6.34416065e-05\n",
      "training step: 183900\n",
      "loss:  8.02345858e-06\n",
      "training step: 184000\n",
      "loss:  0.000103987768\n",
      "training step: 184100\n",
      "loss:  1.14248878e-05\n",
      "training step: 184200\n",
      "loss:  4.57005899e-06\n",
      "training step: 184300\n",
      "loss:  5.00285023e-06\n",
      "training step: 184400\n",
      "loss:  6.14824676e-05\n",
      "training step: 184500\n",
      "loss:  1.11965246e-05\n",
      "training step: 184600\n",
      "loss:  8.69527594e-06\n",
      "training step: 184700\n",
      "loss:  6.44146576e-06\n",
      "training step: 184800\n",
      "loss:  4.83535805e-05\n",
      "training step: 184900\n",
      "loss:  6.25099847e-06\n",
      "training step: 185000\n",
      "loss:  1.63837376e-05\n",
      "training step: 185100\n",
      "loss:  1.38421856e-05\n",
      "training step: 185200\n",
      "loss:  1.16680085e-05\n",
      "training step: 185300\n",
      "loss:  7.90984359e-06\n",
      "training step: 185400\n",
      "loss:  3.90943524e-06\n",
      "training step: 185500\n",
      "loss:  8.09054109e-05\n",
      "training step: 185600\n",
      "loss:  6.48225523e-06\n",
      "training step: 185700\n",
      "loss:  2.93645262e-05\n",
      "training step: 185800\n",
      "loss:  6.97235419e-06\n",
      "training step: 185900\n",
      "loss:  8.31224952e-06\n",
      "training step: 186000\n",
      "loss:  1.5340931e-05\n",
      "training step: 186100\n",
      "loss:  5.37628e-06\n",
      "training step: 186200\n",
      "loss:  7.20942217e-06\n",
      "training step: 186300\n",
      "loss:  3.8668597e-05\n",
      "training step: 186400\n",
      "loss:  3.55277261e-05\n",
      "training step: 186500\n",
      "loss:  1.66675036e-05\n",
      "training step: 186600\n",
      "loss:  7.04470176e-06\n",
      "training step: 186700\n",
      "loss:  1.09904713e-05\n",
      "training step: 186800\n",
      "loss:  6.38369556e-06\n",
      "training step: 186900\n",
      "loss:  5.2782485e-05\n",
      "training step: 187000\n",
      "loss:  7.15857168e-05\n",
      "training step: 187100\n",
      "loss:  3.13484161e-05\n",
      "training step: 187200\n",
      "loss:  1.81197847e-05\n",
      "training step: 187300\n",
      "loss:  1.39702543e-05\n",
      "training step: 187400\n",
      "loss:  4.32977067e-05\n",
      "training step: 187500\n",
      "loss:  3.9856739e-05\n",
      "training step: 187600\n",
      "loss:  1.39273207e-05\n",
      "training step: 187700\n",
      "loss:  1.10542469e-05\n",
      "training step: 187800\n",
      "loss:  5.15546271e-05\n",
      "training step: 187900\n",
      "loss:  9.24483447e-06\n",
      "training step: 188000\n",
      "loss:  1.68653914e-05\n",
      "training step: 188100\n",
      "loss:  5.90908257e-06\n",
      "training step: 188200\n",
      "loss:  5.94285257e-06\n",
      "training step: 188300\n",
      "loss:  1.2008717e-05\n",
      "training step: 188400\n",
      "loss:  0.000128617889\n",
      "training step: 188500\n",
      "loss:  6.28331e-06\n",
      "training step: 188600\n",
      "loss:  2.21848913e-05\n",
      "training step: 188700\n",
      "loss:  2.3867351e-05\n",
      "training step: 188800\n",
      "loss:  2.68881358e-05\n",
      "training step: 188900\n",
      "loss:  3.4814635e-05\n",
      "training step: 189000\n",
      "loss:  0.000129609063\n",
      "training step: 189100\n",
      "loss:  1.4904228e-05\n",
      "training step: 189200\n",
      "loss:  2.31805716e-05\n",
      "training step: 189300\n",
      "loss:  5.97443841e-06\n",
      "training step: 189400\n",
      "loss:  1.24631106e-05\n",
      "training step: 189500\n",
      "loss:  1.61587541e-05\n",
      "training step: 189600\n",
      "loss:  8.69524683e-06\n",
      "training step: 189700\n",
      "loss:  5.62070054e-05\n",
      "training step: 189800\n",
      "loss:  4.23872825e-06\n",
      "training step: 189900\n",
      "loss:  3.3714874e-05\n",
      "training step: 190000\n",
      "loss:  6.85238956e-06\n",
      "training step: 190100\n",
      "loss:  4.92146955e-06\n",
      "training step: 190200\n",
      "loss:  1.45834965e-05\n",
      "training step: 190300\n",
      "loss:  0.000107151442\n",
      "training step: 190400\n",
      "loss:  5.42447233e-05\n",
      "training step: 190500\n",
      "loss:  3.37130878e-05\n",
      "training step: 190600\n",
      "loss:  4.61981244e-06\n",
      "training step: 190700\n",
      "loss:  1.13383958e-05\n",
      "training step: 190800\n",
      "loss:  9.17057e-06\n",
      "training step: 190900\n",
      "loss:  3.6226611e-05\n",
      "training step: 191000\n",
      "loss:  9.82581259e-05\n",
      "training step: 191100\n",
      "loss:  4.77166532e-06\n",
      "training step: 191200\n",
      "loss:  5.39930552e-05\n",
      "training step: 191300\n",
      "loss:  5.56583436e-05\n",
      "training step: 191400\n",
      "loss:  2.93424291e-05\n",
      "training step: 191500\n",
      "loss:  5.20236108e-06\n",
      "training step: 191600\n",
      "loss:  1.13719534e-05\n",
      "training step: 191700\n",
      "loss:  4.37374583e-05\n",
      "training step: 191800\n",
      "loss:  1.09352532e-05\n",
      "training step: 191900\n",
      "loss:  4.16615376e-06\n",
      "training step: 192000\n",
      "loss:  3.00238662e-05\n",
      "training step: 192100\n",
      "loss:  3.14191311e-05\n",
      "training step: 192200\n",
      "loss:  4.99969838e-06\n",
      "training step: 192300\n",
      "loss:  4.94700653e-05\n",
      "training step: 192400\n",
      "loss:  8.52096273e-05\n",
      "training step: 192500\n",
      "loss:  6.96522648e-06\n",
      "training step: 192600\n",
      "loss:  1.39367367e-05\n",
      "training step: 192700\n",
      "loss:  1.24176222e-05\n",
      "training step: 192800\n",
      "loss:  5.46958472e-05\n",
      "training step: 192900\n",
      "loss:  7.47794229e-06\n",
      "training step: 193000\n",
      "loss:  1.71873835e-05\n",
      "training step: 193100\n",
      "loss:  2.0487445e-05\n",
      "training step: 193200\n",
      "loss:  6.39464361e-06\n",
      "training step: 193300\n",
      "loss:  0.00018565463\n",
      "training step: 193400\n",
      "loss:  6.83010421e-06\n",
      "training step: 193500\n",
      "loss:  7.94495918e-06\n",
      "training step: 193600\n",
      "loss:  8.56126189e-06\n",
      "training step: 193700\n",
      "loss:  1.66354548e-05\n",
      "training step: 193800\n",
      "loss:  2.56784333e-05\n",
      "training step: 193900\n",
      "loss:  4.24919608e-06\n",
      "training step: 194000\n",
      "loss:  4.93274138e-06\n",
      "training step: 194100\n",
      "loss:  2.53440776e-05\n",
      "training step: 194200\n",
      "loss:  1.43814432e-05\n",
      "training step: 194300\n",
      "loss:  5.24717552e-06\n",
      "training step: 194400\n",
      "loss:  1.11538184e-05\n",
      "training step: 194500\n",
      "loss:  5.96125428e-05\n",
      "training step: 194600\n",
      "loss:  2.4892217e-05\n",
      "training step: 194700\n",
      "loss:  6.34618937e-06\n",
      "training step: 194800\n",
      "loss:  0.000132146932\n",
      "training step: 194900\n",
      "loss:  5.62077412e-06\n",
      "training step: 195000\n",
      "loss:  5.54491453e-06\n",
      "training step: 195100\n",
      "loss:  2.55813793e-05\n",
      "training step: 195200\n",
      "loss:  1.09983184e-05\n",
      "training step: 195300\n",
      "loss:  3.93427854e-06\n",
      "training step: 195400\n",
      "loss:  4.50439511e-05\n",
      "training step: 195500\n",
      "loss:  6.21371873e-05\n",
      "training step: 195600\n",
      "loss:  6.89226272e-06\n",
      "training step: 195700\n",
      "loss:  0.000114144481\n",
      "training step: 195800\n",
      "loss:  4.72933825e-05\n",
      "training step: 195900\n",
      "loss:  1.01539299e-05\n",
      "training step: 196000\n",
      "loss:  1.8604147e-05\n",
      "training step: 196100\n",
      "loss:  2.9444278e-05\n",
      "training step: 196200\n",
      "loss:  5.10682548e-06\n",
      "training step: 196300\n",
      "loss:  6.47572961e-06\n",
      "training step: 196400\n",
      "loss:  1.25572305e-05\n",
      "training step: 196500\n",
      "loss:  6.81350775e-06\n",
      "training step: 196600\n",
      "loss:  1.6807573e-05\n",
      "training step: 196700\n",
      "loss:  2.86165068e-05\n",
      "training step: 196800\n",
      "loss:  1.9945337e-05\n",
      "training step: 196900\n",
      "loss:  1.10358696e-05\n",
      "training step: 197000\n",
      "loss:  7.7049217e-06\n",
      "training step: 197100\n",
      "loss:  5.50275763e-06\n",
      "training step: 197200\n",
      "loss:  3.26022055e-05\n",
      "training step: 197300\n",
      "loss:  4.87856141e-06\n",
      "training step: 197400\n",
      "loss:  4.18720629e-06\n",
      "training step: 197500\n",
      "loss:  2.13603671e-05\n",
      "training step: 197600\n",
      "loss:  4.2288134e-06\n",
      "training step: 197700\n",
      "loss:  3.1363063e-06\n",
      "training step: 197800\n",
      "loss:  3.8212811e-06\n",
      "training step: 197900\n",
      "loss:  5.92506422e-06\n",
      "training step: 198000\n",
      "loss:  1.24619501e-05\n",
      "training step: 198100\n"
     ]
    }
   ],
   "source": [
    "setting = {'activation':'tanh', 'nn_shape':(256,256,256), 'batch_size':len(train_chomo_), 'training_steps':200000,\\\n",
    "'learning_rate': 0.00001, 'decay_rate':0.95, 'decay_per_steps':1000, 'save_step':100, 'drop_rate':0, 'save_path':'./save',\\\n",
    "'seed':None, 'debug_traj':False}\n",
    "NN = nn.NN(setting_dict=setting)\n",
    "NN.train(train_homo_,train_chomo_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e48223-a320-45aa-b136-5d7f3a7b1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iall = np.arange(1681)\n",
    "idiff = np.setdiff1d(iall,index)\n",
    "test_homo = np.delete(copy.deepcopy(train_homo_pairs),idiff,0)\n",
    "test_chomo = np.delete(copy.deepcopy(train_chomo),idiff,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143ab74-322f-4b26-9ee5-7f7d433b2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "error = np.mean(np.multiply(NN.model(train_homo_pairs, training=False).numpy().reshape((len(train_homo_pairs),))-train_c_homo, np.power(train_c_homo,-1))*100)\n",
    "x = np.linspace(0, 4, 41)\n",
    "y = np.linspace(0, 4, 41)\n",
    "Z = NN.model(train_homo_pairs, training=False).numpy().reshape((41,41)).T\n",
    "Z1 = c_homo.reshape((41,41)).T\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "ax.contourf(x,y, np.exp(-Z))\n",
    "ax.set_title('Error: %5.3f%%'%error)\n",
    "ax.set_aspect('equal')\n",
    "# plt.savefig('homo_pred.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790e593-9420-43a7-8f9c-514740afee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "ax.contourf(x,y, Z1)\n",
    "ax.set_title('True')\n",
    "ax.set_aspect('equal')\n",
    "# plt.savefig('homo_true.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcdb5de-64a1-48c9-b3cf-e2a74567b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = []\n",
    "for i in x:\n",
    "    for j in y:\n",
    "        C_ = (i,j)\n",
    "        C.append(C_)\n",
    "C = np.array(C)\n",
    "C_ = copy.deepcopy(C)\n",
    "C = np.delete(C,index,0)\n",
    "C__ = np.delete(C_,idiff,0)\n",
    "print(len(C__))\n",
    "\n",
    "Z = NN.model(test_homo, training=False).numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(C__[:, 0], C__[:, 1], c=np.exp(-Z),s=62, marker=',')\n",
    "\n",
    "ax.set_xlabel('X Axis')\n",
    "ax.set_ylabel('Y Axis')\n",
    "\n",
    "error = np.mean(np.multiply(NN.model(test_homo, training=False).numpy().reshape((len(test_homo),))-test_chomo, np.power(test_chomo,-1))*100)\n",
    "plt.title('ML with error of test set: %5.3f %%'%error)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db72c7-b7b1-494a-85dd-a4056bb496f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = train_chomo\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(C_[:, 0], C_[:, 1], c=np.exp(-Z1),s=62, marker=',')\n",
    "\n",
    "ax.set_xlabel('X Axis')\n",
    "ax.set_ylabel('Y Axis')\n",
    "\n",
    "# plt.title('HOMO of Naphthalene')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeac264-1b61-4a8a-bb30-b368f5551f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c774cb-bb24-4f9c-ab7f-c15e3de25247",
   "metadata": {},
   "outputs": [],
   "source": [
    "error1 = np.mean(np.multiply(NN.model(train_homo_pairs, training=False).numpy().reshape((len(train_homo_pairs),))-train_c_homo, np.power(train_c_homo,-1))*100)\n",
    "error2 = np.mean(np.multiply(NN.model(train_homo_, training=False).numpy().reshape((len(train_homo_),))-train_chomo_, np.power(train_chomo_,-1))*100)\n",
    "error3 = np.mean(np.multiply(NN.model(test_homo, training=False).numpy().reshape((len(test_homo),))-test_chomo, np.power(test_chomo,-1))*100)\n",
    "print('Error of full data set: %5.3f %% \\nError of training set with %d samples: %5.3f %% \\nError of testing set with %d samples: %5.3f %% '%(error1,len(train_homo_),error2,len(test_homo),error3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353b9fe-b6a2-48ce-8bf0-398f84258f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = np.linspace(0,9,10)\n",
    "Y = np.linspace(0,9,10)\n",
    "bs = 3\n",
    "data_set = tf.data.Dataset.from_tensor_slices((X,Y)).shuffle(bs,reshuffle_each_iteration=None).batch(bs)\n",
    "for x,y in data_set:\n",
    "    print(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53644fb-a49c-41e9-8a0a-d86f8fd10ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
