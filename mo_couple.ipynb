{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65900bf3-c182-48a6-b6a0-efd08e340f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 23:44:10.832377: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-05 23:44:10.832506: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='export TF_INTRA_OP_PARALLELISM_THREADS=12', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    import mo_descriptor as md\n",
    "import nn_frame as nn\n",
    "import numpy as np\n",
    "import subprocess\n",
    "subprocess.run('export TF_INTRA_OP_PARALLELISM_THREADS=12', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3819da32-6769-405e-8d3f-1fa9c336c9b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "prepare data_set\n",
    "1. make mo_pair descriptor\n",
    "'''\n",
    "# x_shift = np.arange(0, 4.1, 0.1)\n",
    "# y_shift = np.arange(0, 4.1, 0.1)\n",
    "# z_shift = np.zeros(x_shift.shape)\n",
    "# # the original mo, e.g. homo\n",
    "# homo = md.MO_descriptor('data/homo-s0.cube').make()\n",
    "# lumo = md.MO_descriptor('data/lumo-s0.cube').make()\n",
    "\n",
    "# # for the original pair of one mo and itself\n",
    "# homo_pair = md.MO_pair_descriptor(homo, homo).make()\n",
    "# lumo_pair = md.MO_pair_descriptor(lumo, lumo).make()\n",
    "\n",
    "# homo_pairs = np.zeros((len(x_shift)*len(y_shift),) + homo_pair.shape)\n",
    "# lumo_pairs = np.zeros((len(x_shift)*len(y_shif  t),) + lumo_pair.shape)\n",
    "\n",
    "# homo_ = np.zeros(homo.shape)\n",
    "\n",
    "# for ii, i in enumerate(x_shift):\n",
    "#     for jj, j in enumerate(y_shift):\n",
    "#         idx = ii * len(y_shift) + jj\n",
    "#         homo_[:,0] = np.add(homo[:,0],0)\n",
    "#         homo_[:,1] = np.add(homo[:,1],i)\n",
    "#         homo_[:,2] = np.add(homo[:,2],j)\n",
    "#         homo_[:,3] = np.add(homo[:,3],0)\n",
    "        \n",
    "#         homo_pair_ = md.MO_pair_descriptor(homo, homo_).make()\n",
    "#         homo_pairs[idx] = homo_pair_\n",
    "\n",
    "        \n",
    "# def dir_mat(mat):\n",
    "#     mat_shape = mat.shape\n",
    "#     mat_ = mat.flatten()\n",
    "#     for ii, i in enumerate(mat_):\n",
    "#         if i > 1e-6:\n",
    "#             mat_[ii] = 1\n",
    "#         elif (i < 1e-6) and (i > -1e-6):\n",
    "#             mat_[ii] = -1\n",
    "#         elif i < -1e-6:\n",
    "#             mat_[ii] = -1\n",
    "#     return mat_.reshape(mat_shape)\n",
    "\n",
    "# direct = dir_mat(homo_pair)\n",
    "\n",
    "# # for the shifted pair\n",
    "# homo_pairs = np.zeros((len(x_shift)*len(y_shift),) + homo_pair.shape)\n",
    "# lumo_pairs = np.zeros((len(x_shift)*len(y_shift),) + lumo_pair.shape)\n",
    "# for ii, i in enumerate(x_shift):\n",
    "#     for jj, j in enumerate(y_shift):\n",
    "#         idx = ii * len(y_shift) + jj\n",
    "#         homo_pairs[idx][0] = homo_pair[0]\n",
    "#         homo_pairs[idx][1] = np.add(homo_pair[1],i*direct[1])\n",
    "#         homo_pairs[idx][2] = np.add(homo_pair[2],j*direct[2])\n",
    "#         homo_pairs[idx][3] = homo_pair[3]\n",
    "#         lumo_pairs[idx][0] = lumo_pair[0]\n",
    "#         lumo_pairs[idx][1] = np.add(lumo_pair[1],i)\n",
    "#         lumo_pairs[idx][2] = np.add(lumo_pair[2],j)\n",
    "#         lumo_pairs[idx][3] = lumo_pair[3]\n",
    "# np.save('homo_homo_pair.npy', homo_pairs)\n",
    "# np.save('lumo_lumo_pair.npy', lumo_pairs)\n",
    "homo_pairs = np.load('homo_homo_pair.npy')\n",
    "lumo_pairs = np.load('lumo_lumo_pair.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91a92a2-f076-46c8-835f-958569393703",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. read coupling\n",
    "'''\n",
    "raw_data = np.loadtxt('data/cdft-V1V2.dat')\n",
    "c_homo = np.add(raw_data[:,2], raw_data[:,3]) * 1/2\n",
    "c_lumo = np.add(raw_data[:,4], raw_data[:,5]) * 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda7ef17-1e21-42a4-9daf-27feba4fc51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x1 = homo_pairs[:,0,:,:]\n",
    "# x2 = homo_pairs[:,1,:,:]\n",
    "# x3 = homo_pairs[:,2,:,:]\n",
    "# x4 = homo_pairs[:,3,:,:]\n",
    "# x = np.einsum('aij,aij,aij,aij->aij', x1, x2, x3, x4)\n",
    "\n",
    "train_homo_pairs = homo_pairs\n",
    "train_lumo_pairs = lumo_pairs[0:1200]\n",
    "\n",
    "train_c_homo = -np.log(c_homo)\n",
    "train_c_lumo = c_lumo[0:1200]\n",
    "\n",
    "test_homo_pairs = homo_pairs[1200:]\n",
    "test_lumo_pairs = lumo_pairs[1200:]\n",
    "\n",
    "test_c_homo = c_homo[1200:].reshape((len(c_homo[1200:]),1))\n",
    "test_c_lumo = c_lumo[1200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "938d30ba-e044-469f-b2eb-25f05497f346",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 23:44:12.587646: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-05 23:44:12.587773: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Deng-PC): /proc/driver/nvidia/version does not exist\n",
      "2022-09-05 23:44:12.588366: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  8.67877865\n",
      "training step:     0\n",
      "loss:  7.46247721\n",
      "training step:  1000\n",
      "loss:  7.29858589\n",
      "training step:  2000\n",
      "loss:  7.15421534\n",
      "training step:  3000\n",
      "loss:  7.01879406\n",
      "training step:  4000\n",
      "loss:  6.88273621\n",
      "training step:  5000\n",
      "loss:  6.74740744\n",
      "training step:  6000\n",
      "loss:  6.61134672\n",
      "training step:  7000\n",
      "loss:  6.47506714\n",
      "training step:  8000\n",
      "loss:  6.33915424\n",
      "training step:  9000\n",
      "loss:  6.20345402\n",
      "training step: 10000\n",
      "loss:  6.06759214\n",
      "training step: 11000\n",
      "loss:  5.93141174\n",
      "training step: 12000\n",
      "loss:  5.79484034\n",
      "training step: 13000\n",
      "loss:  5.65792036\n",
      "training step: 14000\n",
      "loss:  5.52061749\n",
      "training step: 15000\n",
      "loss:  5.38291025\n",
      "training step: 16000\n",
      "loss:  5.24490881\n",
      "training step: 17000\n",
      "loss:  5.10673714\n",
      "training step: 18000\n",
      "loss:  4.96844912\n",
      "training step: 19000\n",
      "loss:  4.8301425\n",
      "training step: 20000\n",
      "loss:  4.69194508\n",
      "training step: 21000\n",
      "loss:  4.55392694\n",
      "training step: 22000\n",
      "loss:  4.41615438\n",
      "training step: 23000\n",
      "loss:  4.2786355\n",
      "training step: 24000\n",
      "loss:  4.1414423\n",
      "training step: 25000\n",
      "loss:  4.00469685\n",
      "training step: 26000\n",
      "loss:  3.86847186\n",
      "training step: 27000\n",
      "loss:  3.73288846\n",
      "training step: 28000\n",
      "loss:  3.59800768\n",
      "training step: 29000\n",
      "loss:  3.46394086\n",
      "training step: 30000\n",
      "loss:  3.33074284\n",
      "training step: 31000\n",
      "loss:  3.19848871\n",
      "training step: 32000\n",
      "loss:  3.06728673\n",
      "training step: 33000\n",
      "loss:  2.93731761\n",
      "training step: 34000\n",
      "loss:  2.80865121\n",
      "training step: 35000\n",
      "loss:  2.68142223\n",
      "training step: 36000\n",
      "loss:  2.55574226\n",
      "training step: 37000\n",
      "loss:  2.43165898\n",
      "training step: 38000\n",
      "loss:  2.3093493\n",
      "training step: 39000\n",
      "loss:  2.18886423\n",
      "training step: 40000\n",
      "loss:  2.07025027\n",
      "training step: 41000\n",
      "loss:  1.95368493\n",
      "training step: 42000\n",
      "loss:  1.8393327\n",
      "training step: 43000\n",
      "loss:  1.72726238\n",
      "training step: 44000\n",
      "loss:  1.61766922\n",
      "training step: 45000\n",
      "loss:  1.51072013\n",
      "training step: 46000\n",
      "loss:  1.40641022\n",
      "training step: 47000\n",
      "loss:  1.30491757\n",
      "training step: 48000\n",
      "loss:  1.20627117\n",
      "training step: 49000\n",
      "loss:  1.11068177\n",
      "training step: 50000\n",
      "loss:  1.01820779\n",
      "training step: 51000\n",
      "loss:  0.929143369\n",
      "training step: 52000\n",
      "loss:  0.84352994\n",
      "training step: 53000\n",
      "loss:  0.761434674\n",
      "training step: 54000\n",
      "loss:  0.682952583\n",
      "training step: 55000\n",
      "loss:  0.60826081\n",
      "training step: 56000\n",
      "loss:  0.537581742\n",
      "training step: 57000\n",
      "loss:  0.470891684\n",
      "training step: 58000\n",
      "loss:  0.408293456\n",
      "training step: 59000\n",
      "loss:  0.350018203\n",
      "training step: 60000\n",
      "loss:  0.296058238\n",
      "training step: 61000\n",
      "loss:  0.246580258\n",
      "training step: 62000\n",
      "loss:  0.201618478\n",
      "training step: 63000\n",
      "loss:  0.161302313\n",
      "training step: 64000\n",
      "loss:  0.125647336\n",
      "training step: 65000\n",
      "loss:  0.0947066471\n",
      "training step: 66000\n",
      "loss:  0.0684798807\n",
      "training step: 67000\n",
      "loss:  0.0469369255\n",
      "training step: 68000\n",
      "loss:  0.0299794469\n",
      "training step: 69000\n",
      "loss:  0.0174081288\n",
      "training step: 70000\n",
      "loss:  0.00889567472\n",
      "training step: 71000\n",
      "loss:  0.00391433714\n",
      "training step: 72000\n",
      "loss:  0.00162484124\n",
      "training step: 73000\n",
      "loss:  0.000939782418\n",
      "training step: 74000\n",
      "loss:  0.000831114885\n",
      "training step: 75000\n",
      "loss:  0.00080497\n",
      "training step: 76000\n",
      "loss:  0.000780962699\n",
      "training step: 77000\n",
      "loss:  0.000758958864\n",
      "training step: 78000\n",
      "loss:  0.000739800162\n",
      "training step: 79000\n",
      "loss:  0.000719614152\n",
      "training step: 80000\n",
      "loss:  0.00069952663\n",
      "training step: 81000\n",
      "loss:  0.000818012748\n",
      "training step: 82000\n",
      "loss:  0.000738650328\n",
      "training step: 83000\n",
      "loss:  0.000647026813\n",
      "training step: 84000\n",
      "loss:  0.000630616036\n",
      "training step: 85000\n",
      "loss:  0.000615603058\n",
      "training step: 86000\n",
      "loss:  0.000599908817\n",
      "training step: 87000\n",
      "loss:  0.000588397437\n",
      "training step: 88000\n",
      "loss:  0.000571982353\n",
      "training step: 89000\n",
      "loss:  0.000556852203\n",
      "training step: 90000\n",
      "loss:  0.000543859613\n",
      "training step: 91000\n",
      "loss:  0.000531431346\n",
      "training step: 92000\n",
      "loss:  0.000517777691\n",
      "training step: 93000\n",
      "loss:  0.000505594828\n",
      "training step: 94000\n",
      "loss:  0.000492875\n",
      "training step: 95000\n",
      "loss:  0.00055418472\n",
      "training step: 96000\n",
      "loss:  0.000469877647\n",
      "training step: 97000\n",
      "loss:  0.000459582312\n",
      "training step: 98000\n",
      "loss:  0.000449536077\n",
      "training step: 99000\n",
      "loss:  0.000439953408\n",
      "training step: 100000\n",
      "loss:  0.00043013657\n",
      "training step: 101000\n",
      "loss:  0.000420645025\n",
      "training step: 102000\n",
      "loss:  0.000411798159\n",
      "training step: 103000\n",
      "loss:  0.000401822239\n",
      "training step: 104000\n",
      "loss:  0.00040449691\n",
      "training step: 105000\n",
      "loss:  0.00038328045\n",
      "training step: 106000\n",
      "loss:  0.000375103526\n",
      "training step: 107000\n",
      "loss:  0.000367040688\n",
      "training step: 108000\n",
      "loss:  0.000359819212\n",
      "training step: 109000\n",
      "loss:  0.00035217384\n",
      "training step: 110000\n",
      "loss:  0.000344387168\n",
      "training step: 111000\n",
      "loss:  0.000337160687\n",
      "training step: 112000\n",
      "loss:  0.000455352478\n",
      "training step: 113000\n",
      "loss:  0.000322056585\n",
      "training step: 114000\n",
      "loss:  0.000315152341\n",
      "training step: 115000\n",
      "loss:  0.000309550902\n",
      "training step: 116000\n",
      "loss:  0.000301474123\n",
      "training step: 117000\n",
      "loss:  0.00029468516\n",
      "training step: 118000\n",
      "loss:  0.000287541508\n",
      "training step: 119000\n",
      "loss:  0.000281508866\n",
      "training step: 120000\n",
      "loss:  0.00027517823\n",
      "training step: 121000\n",
      "loss:  0.000268752425\n",
      "training step: 122000\n",
      "loss:  0.000262957037\n",
      "training step: 123000\n",
      "loss:  0.000257404055\n",
      "training step: 124000\n",
      "loss:  0.000251558173\n",
      "training step: 125000\n",
      "loss:  0.000245736592\n",
      "training step: 126000\n",
      "loss:  0.000240857175\n",
      "training step: 127000\n",
      "loss:  0.000235875617\n",
      "training step: 128000\n",
      "loss:  0.000230658305\n",
      "training step: 129000\n",
      "loss:  0.00022572717\n",
      "training step: 130000\n",
      "loss:  0.000220632035\n",
      "training step: 131000\n",
      "loss:  0.000215711625\n",
      "training step: 132000\n",
      "loss:  0.000211034247\n",
      "training step: 133000\n",
      "loss:  0.00020598105\n",
      "training step: 134000\n",
      "loss:  0.000201438175\n",
      "training step: 135000\n",
      "loss:  0.000197332294\n",
      "training step: 136000\n",
      "loss:  0.000192912281\n",
      "training step: 137000\n",
      "loss:  0.000188853446\n",
      "training step: 138000\n",
      "loss:  0.000185176119\n",
      "training step: 139000\n",
      "loss:  0.000180984571\n",
      "training step: 140000\n",
      "loss:  0.000177475202\n",
      "training step: 141000\n",
      "loss:  0.000173827051\n",
      "training step: 142000\n",
      "loss:  0.000170020561\n",
      "training step: 143000\n",
      "loss:  0.000166394952\n",
      "training step: 144000\n",
      "loss:  0.000162794502\n",
      "training step: 145000\n",
      "loss:  0.000159229661\n",
      "training step: 146000\n",
      "loss:  0.000155431961\n",
      "training step: 147000\n",
      "loss:  0.00015182003\n",
      "training step: 148000\n",
      "loss:  0.000148662279\n",
      "training step: 149000\n",
      "loss:  0.000145003098\n",
      "training step: 150000\n",
      "loss:  0.000142254532\n",
      "training step: 151000\n",
      "loss:  0.000139055119\n",
      "training step: 152000\n",
      "loss:  0.000135863287\n",
      "training step: 153000\n",
      "loss:  0.00013348028\n",
      "training step: 154000\n",
      "loss:  0.000130223212\n",
      "training step: 155000\n",
      "loss:  0.000127479318\n",
      "training step: 156000\n",
      "loss:  0.000124972328\n",
      "training step: 157000\n",
      "loss:  0.000121957513\n",
      "training step: 158000\n",
      "loss:  0.000118764052\n",
      "training step: 159000\n",
      "loss:  0.000116345778\n",
      "training step: 160000\n",
      "loss:  0.000114023118\n",
      "training step: 161000\n",
      "loss:  0.000111129048\n",
      "training step: 162000\n",
      "loss:  0.000109281907\n",
      "training step: 163000\n",
      "loss:  0.000106995445\n",
      "training step: 164000\n",
      "loss:  0.000104558458\n",
      "training step: 165000\n",
      "loss:  0.000102449223\n",
      "training step: 166000\n",
      "loss:  0.00010018437\n",
      "training step: 167000\n",
      "loss:  9.75140429e-05\n",
      "training step: 168000\n",
      "loss:  9.57909506e-05\n",
      "training step: 169000\n",
      "loss:  9.38940502e-05\n",
      "training step: 170000\n",
      "loss:  9.15458077e-05\n",
      "training step: 171000\n",
      "loss:  8.94636396e-05\n",
      "training step: 172000\n",
      "loss:  8.75838e-05\n",
      "training step: 173000\n",
      "loss:  8.57307678e-05\n",
      "training step: 174000\n",
      "loss:  8.40369e-05\n",
      "training step: 175000\n",
      "loss:  8.20324349e-05\n",
      "training step: 176000\n",
      "loss:  8.04341907e-05\n",
      "training step: 177000\n",
      "loss:  7.87333265e-05\n",
      "training step: 178000\n",
      "loss:  7.70078259e-05\n",
      "training step: 179000\n",
      "loss:  7.56670852e-05\n",
      "training step: 180000\n",
      "loss:  7.41137337e-05\n",
      "training step: 181000\n",
      "loss:  7.24426936e-05\n",
      "training step: 182000\n",
      "loss:  7.13216286e-05\n",
      "training step: 183000\n",
      "loss:  7.00548408e-05\n",
      "training step: 184000\n",
      "loss:  6.84735205e-05\n",
      "training step: 185000\n",
      "loss:  6.72291935e-05\n",
      "training step: 186000\n",
      "loss:  6.59434954e-05\n",
      "training step: 187000\n",
      "loss:  6.45794862e-05\n",
      "training step: 188000\n",
      "loss:  6.32247611e-05\n",
      "training step: 189000\n",
      "loss:  6.18984341e-05\n",
      "training step: 190000\n",
      "loss:  6.06544454e-05\n",
      "training step: 191000\n",
      "loss:  5.94268e-05\n",
      "training step: 192000\n",
      "loss:  5.84001318e-05\n",
      "training step: 193000\n",
      "loss:  5.74398618e-05\n",
      "training step: 194000\n",
      "loss:  5.60737171e-05\n",
      "training step: 195000\n",
      "loss:  5.52516321e-05\n",
      "training step: 196000\n",
      "loss:  5.42205344e-05\n",
      "training step: 197000\n",
      "loss:  5.32165977e-05\n",
      "training step: 198000\n",
      "loss:  5.21834045e-05\n",
      "training step: 199000\n",
      "loss:  5.12269617e-05\n",
      "training step: 200000\n",
      "loss:  5.02946823e-05\n",
      "training step: 201000\n",
      "loss:  4.92893632e-05\n",
      "training step: 202000\n",
      "loss:  4.85210294e-05\n",
      "training step: 203000\n",
      "loss:  4.75215747e-05\n",
      "training step: 204000\n",
      "loss:  4.67806458e-05\n",
      "training step: 205000\n",
      "loss:  4.59711518e-05\n",
      "training step: 206000\n",
      "loss:  4.522987e-05\n",
      "training step: 207000\n",
      "loss:  4.44404759e-05\n",
      "training step: 208000\n",
      "loss:  0.000216659304\n",
      "training step: 209000\n",
      "loss:  4.28722e-05\n",
      "training step: 210000\n",
      "loss:  4.20464057e-05\n",
      "training step: 211000\n",
      "loss:  4.12556292e-05\n",
      "training step: 212000\n",
      "loss:  4.06236868e-05\n",
      "training step: 213000\n",
      "loss:  3.99892087e-05\n",
      "training step: 214000\n",
      "loss:  3.94136732e-05\n",
      "training step: 215000\n",
      "loss:  3.8635786e-05\n",
      "training step: 216000\n",
      "loss:  3.8009508e-05\n",
      "training step: 217000\n",
      "loss:  3.7358921e-05\n",
      "training step: 218000\n",
      "loss:  3.68053152e-05\n",
      "training step: 219000\n",
      "loss:  3.61352613e-05\n",
      "training step: 220000\n",
      "loss:  3.54890799e-05\n",
      "training step: 221000\n",
      "loss:  3.49855545e-05\n",
      "training step: 222000\n",
      "loss:  3.43114298e-05\n",
      "training step: 223000\n",
      "loss:  3.39593644e-05\n",
      "training step: 224000\n",
      "loss:  3.33804164e-05\n",
      "training step: 225000\n",
      "loss:  3.29285431e-05\n",
      "training step: 226000\n",
      "loss:  3.23593886e-05\n",
      "training step: 227000\n",
      "loss:  3.19295e-05\n",
      "training step: 228000\n",
      "loss:  3.14258141e-05\n",
      "training step: 229000\n",
      "loss:  3.08262e-05\n",
      "training step: 230000\n",
      "loss:  3.04321493e-05\n",
      "training step: 231000\n",
      "loss:  3.00190459e-05\n",
      "training step: 232000\n",
      "loss:  2.95661648e-05\n",
      "training step: 233000\n",
      "loss:  2.91393953e-05\n",
      "training step: 234000\n",
      "loss:  2.87734583e-05\n",
      "training step: 235000\n",
      "loss:  2.82055298e-05\n",
      "training step: 236000\n",
      "loss:  2.78790558e-05\n",
      "training step: 237000\n",
      "loss:  2.74458744e-05\n",
      "training step: 238000\n",
      "loss:  2.71329045e-05\n",
      "training step: 239000\n",
      "loss:  2.67944215e-05\n",
      "training step: 240000\n",
      "loss:  2.64259197e-05\n",
      "training step: 241000\n",
      "loss:  2.60751112e-05\n",
      "training step: 242000\n",
      "loss:  2.57432112e-05\n",
      "training step: 243000\n",
      "loss:  2.53521121e-05\n",
      "training step: 244000\n",
      "loss:  2.50175817e-05\n",
      "training step: 245000\n",
      "loss:  2.46354175e-05\n",
      "training step: 246000\n",
      "loss:  2.43593822e-05\n",
      "training step: 247000\n",
      "loss:  2.39900946e-05\n",
      "training step: 248000\n",
      "loss:  2.37041477e-05\n",
      "training step: 249000\n",
      "loss:  2.33856299e-05\n",
      "training step: 250000\n",
      "loss:  2.31088143e-05\n",
      "training step: 251000\n",
      "loss:  2.27378096e-05\n",
      "training step: 252000\n",
      "loss:  2.24560372e-05\n",
      "training step: 253000\n",
      "loss:  2.21649934e-05\n",
      "training step: 254000\n",
      "loss:  2.18786208e-05\n",
      "training step: 255000\n",
      "loss:  2.15518266e-05\n",
      "training step: 256000\n",
      "loss:  2.13320582e-05\n",
      "training step: 257000\n",
      "loss:  2.10923681e-05\n",
      "training step: 258000\n",
      "loss:  2.08331221e-05\n",
      "training step: 259000\n",
      "loss:  2.0627911e-05\n",
      "training step: 260000\n",
      "loss:  2.03634954e-05\n",
      "training step: 261000\n",
      "loss:  2.01107541e-05\n",
      "training step: 262000\n",
      "loss:  1.98567577e-05\n",
      "training step: 263000\n",
      "loss:  1.97829286e-05\n",
      "training step: 264000\n",
      "loss:  1.94039731e-05\n",
      "training step: 265000\n",
      "loss:  1.91626532e-05\n",
      "training step: 266000\n",
      "loss:  1.89758084e-05\n",
      "training step: 267000\n",
      "loss:  1.87792684e-05\n",
      "training step: 268000\n",
      "loss:  1.85455483e-05\n",
      "training step: 269000\n",
      "loss:  1.83537504e-05\n",
      "training step: 270000\n",
      "loss:  1.81419018e-05\n",
      "training step: 271000\n",
      "loss:  1.79392737e-05\n",
      "training step: 272000\n",
      "loss:  1.77572856e-05\n",
      "training step: 273000\n",
      "loss:  1.75422119e-05\n",
      "training step: 274000\n",
      "loss:  1.73996195e-05\n",
      "training step: 275000\n",
      "loss:  1.71878874e-05\n",
      "training step: 276000\n",
      "loss:  1.70510702e-05\n",
      "training step: 277000\n",
      "loss:  1.72211658e-05\n",
      "training step: 278000\n",
      "loss:  1.67245307e-05\n",
      "training step: 279000\n",
      "loss:  1.65081328e-05\n",
      "training step: 280000\n",
      "loss:  1.63540371e-05\n",
      "training step: 281000\n",
      "loss:  1.61927292e-05\n",
      "training step: 282000\n",
      "loss:  1.60250474e-05\n",
      "training step: 283000\n",
      "loss:  1.58651437e-05\n",
      "training step: 284000\n",
      "loss:  1.57361337e-05\n",
      "training step: 285000\n",
      "loss:  1.55916114e-05\n",
      "training step: 286000\n",
      "loss:  1.54313057e-05\n",
      "training step: 287000\n",
      "loss:  1.52676075e-05\n",
      "training step: 288000\n",
      "loss:  1.51549393e-05\n",
      "training step: 289000\n",
      "loss:  1.49979514e-05\n",
      "training step: 290000\n",
      "loss:  1.4848034e-05\n",
      "training step: 291000\n",
      "loss:  1.47189421e-05\n",
      "training step: 292000\n",
      "loss:  1.45884906e-05\n",
      "training step: 293000\n",
      "loss:  1.44757178e-05\n",
      "training step: 294000\n",
      "loss:  1.52246239e-05\n",
      "training step: 295000\n",
      "loss:  1.42093413e-05\n",
      "training step: 296000\n",
      "loss:  2.89233685e-05\n",
      "training step: 297000\n",
      "loss:  1.39849481e-05\n",
      "training step: 298000\n",
      "loss:  2.40145637e-05\n",
      "training step: 299000\n",
      "loss:  1.37514426e-05\n",
      "training step: 300000\n",
      "loss:  1.36230419e-05\n",
      "training step: 301000\n",
      "loss:  1.35102127e-05\n",
      "training step: 302000\n",
      "loss:  1.33881285e-05\n",
      "training step: 303000\n",
      "loss:  1.32783662e-05\n",
      "training step: 304000\n",
      "loss:  1.31439456e-05\n",
      "training step: 305000\n",
      "loss:  1.30428753e-05\n",
      "training step: 306000\n",
      "loss:  1.29327955e-05\n",
      "training step: 307000\n",
      "loss:  1.28057754e-05\n",
      "training step: 308000\n",
      "loss:  1.27170033e-05\n",
      "training step: 309000\n",
      "loss:  1.25918195e-05\n",
      "training step: 310000\n",
      "loss:  1.25139068e-05\n",
      "training step: 311000\n",
      "loss:  1.23963737e-05\n",
      "training step: 312000\n",
      "loss:  1.23128239e-05\n",
      "training step: 313000\n",
      "loss:  1.22081074e-05\n",
      "training step: 314000\n",
      "loss:  1.21053363e-05\n",
      "training step: 315000\n",
      "loss:  1.2006426e-05\n",
      "training step: 316000\n",
      "loss:  1.19148708e-05\n",
      "training step: 317000\n",
      "loss:  1.18290491e-05\n",
      "training step: 318000\n",
      "loss:  1.17378258e-05\n",
      "training step: 319000\n",
      "loss:  1.16524125e-05\n",
      "training step: 320000\n",
      "loss:  1.15597231e-05\n",
      "training step: 321000\n",
      "loss:  1.14685026e-05\n",
      "training step: 322000\n",
      "loss:  1.14134691e-05\n",
      "training step: 323000\n",
      "loss:  1.13000888e-05\n",
      "training step: 324000\n",
      "loss:  1.12185153e-05\n",
      "training step: 325000\n",
      "loss:  1.11423287e-05\n",
      "training step: 326000\n",
      "loss:  1.76330905e-05\n",
      "training step: 327000\n",
      "loss:  1.0978827e-05\n",
      "training step: 328000\n",
      "loss:  1.09160364e-05\n",
      "training step: 329000\n",
      "loss:  1.08408021e-05\n",
      "training step: 330000\n",
      "loss:  1.45537433e-05\n",
      "training step: 331000\n",
      "loss:  1.06834341e-05\n",
      "training step: 332000\n",
      "loss:  1.06015568e-05\n",
      "training step: 333000\n",
      "loss:  1.05221479e-05\n",
      "training step: 334000\n",
      "loss:  1.04413693e-05\n",
      "training step: 335000\n",
      "loss:  1.03803131e-05\n",
      "training step: 336000\n",
      "loss:  1.03320835e-05\n",
      "training step: 337000\n",
      "loss:  1.02611939e-05\n",
      "training step: 338000\n",
      "loss:  1.0172369e-05\n",
      "training step: 339000\n",
      "loss:  1.01333071e-05\n",
      "training step: 340000\n",
      "loss:  1.00559046e-05\n",
      "training step: 341000\n",
      "loss:  9.99385156e-06\n",
      "training step: 342000\n",
      "loss:  9.89950331e-06\n",
      "training step: 343000\n",
      "loss:  9.85482347e-06\n",
      "training step: 344000\n",
      "loss:  9.77481068e-06\n",
      "training step: 345000\n",
      "loss:  9.72107227e-06\n",
      "training step: 346000\n",
      "loss:  9.6552385e-06\n",
      "training step: 347000\n",
      "loss:  9.59694353e-06\n",
      "training step: 348000\n",
      "loss:  9.5392e-06\n",
      "training step: 349000\n",
      "loss:  9.4789566e-06\n",
      "training step: 350000\n",
      "loss:  9.41281269e-06\n",
      "training step: 351000\n",
      "loss:  9.34815e-06\n",
      "training step: 352000\n",
      "loss:  9.29103135e-06\n",
      "training step: 353000\n",
      "loss:  9.23350945e-06\n",
      "training step: 354000\n",
      "loss:  9.18070145e-06\n",
      "training step: 355000\n",
      "loss:  9.14185512e-06\n",
      "training step: 356000\n",
      "loss:  9.08075253e-06\n",
      "training step: 357000\n",
      "loss:  9.00991108e-06\n",
      "training step: 358000\n",
      "loss:  8.95894e-06\n",
      "training step: 359000\n",
      "loss:  8.9004925e-06\n",
      "training step: 360000\n",
      "loss:  8.86082853e-06\n",
      "training step: 361000\n",
      "loss:  8.81518463e-06\n",
      "training step: 362000\n",
      "loss:  8.77924685e-06\n",
      "training step: 363000\n",
      "loss:  8.71406701e-06\n",
      "training step: 364000\n",
      "loss:  8.66922255e-06\n",
      "training step: 365000\n",
      "loss:  8.6204027e-06\n",
      "training step: 366000\n",
      "loss:  8.5751235e-06\n",
      "training step: 367000\n",
      "loss:  8.52079e-06\n",
      "training step: 368000\n",
      "loss:  8.47889896e-06\n",
      "training step: 369000\n",
      "loss:  8.44214082e-06\n",
      "training step: 370000\n",
      "loss:  0.000141584838\n",
      "training step: 371000\n",
      "loss:  8.35900755e-06\n",
      "training step: 372000\n",
      "loss:  8.29341479e-06\n",
      "training step: 373000\n",
      "loss:  8.25952065e-06\n",
      "training step: 374000\n",
      "loss:  8.22269431e-06\n",
      "training step: 375000\n",
      "loss:  8.1709386e-06\n",
      "training step: 376000\n",
      "loss:  8.131291e-06\n",
      "training step: 377000\n",
      "loss:  8.09676203e-06\n",
      "training step: 378000\n",
      "loss:  8.03861531e-06\n",
      "training step: 379000\n",
      "loss:  8.00262569e-06\n",
      "training step: 380000\n",
      "loss:  7.94531479e-06\n",
      "training step: 381000\n",
      "loss:  7.92741594e-06\n",
      "training step: 382000\n",
      "loss:  7.87907175e-06\n",
      "training step: 383000\n",
      "loss:  7.84751228e-06\n",
      "training step: 384000\n",
      "loss:  7.80631126e-06\n",
      "training step: 385000\n",
      "loss:  7.77256901e-06\n",
      "training step: 386000\n",
      "loss:  7.72884505e-06\n",
      "training step: 387000\n",
      "loss:  7.69446251e-06\n",
      "training step: 388000\n",
      "loss:  7.64317e-06\n",
      "training step: 389000\n",
      "loss:  7.60994863e-06\n",
      "training step: 390000\n",
      "loss:  7.56897043e-06\n",
      "training step: 391000\n",
      "loss:  7.54068105e-06\n",
      "training step: 392000\n",
      "loss:  7.49163246e-06\n",
      "training step: 393000\n",
      "loss:  7.46583237e-06\n",
      "training step: 394000\n",
      "loss:  7.4243585e-06\n",
      "training step: 395000\n",
      "loss:  7.39590405e-06\n",
      "training step: 396000\n",
      "loss:  7.35370577e-06\n",
      "training step: 397000\n",
      "loss:  7.33050365e-06\n",
      "training step: 398000\n",
      "loss:  7.29148496e-06\n",
      "training step: 399000\n",
      "loss:  0.000414751325\n",
      "training step: 400000\n",
      "loss:  7.22579352e-06\n",
      "training step: 401000\n",
      "loss:  7.17341027e-06\n",
      "training step: 402000\n",
      "loss:  7.16076602e-06\n",
      "training step: 403000\n",
      "loss:  7.12032352e-06\n",
      "training step: 404000\n",
      "loss:  7.09625465e-06\n",
      "training step: 405000\n",
      "loss:  7.05517414e-06\n",
      "training step: 406000\n",
      "loss:  7.03842625e-06\n",
      "training step: 407000\n",
      "loss:  7.00781766e-06\n",
      "training step: 408000\n",
      "loss:  6.96794768e-06\n",
      "training step: 409000\n",
      "loss:  6.93256925e-06\n",
      "training step: 410000\n",
      "loss:  6.90720253e-06\n",
      "training step: 411000\n",
      "loss:  6.88198952e-06\n",
      "training step: 412000\n",
      "loss:  1.87276655e-05\n",
      "training step: 413000\n",
      "loss:  6.81932124e-06\n",
      "training step: 414000\n",
      "loss:  6.79161531e-06\n",
      "training step: 415000\n",
      "loss:  6.7581509e-06\n",
      "training step: 416000\n",
      "loss:  6.72564829e-06\n",
      "training step: 417000\n",
      "loss:  6.69264409e-06\n",
      "training step: 418000\n",
      "loss:  6.65218158e-06\n",
      "training step: 419000\n",
      "loss:  6.63400306e-06\n",
      "training step: 420000\n",
      "loss:  6.60116802e-06\n",
      "training step: 421000\n",
      "loss:  6.57996497e-06\n",
      "training step: 422000\n",
      "loss:  6.54992118e-06\n",
      "training step: 423000\n",
      "loss:  6.52039398e-06\n",
      "training step: 424000\n",
      "loss:  6.4754272e-06\n",
      "training step: 425000\n",
      "loss:  6.46421631e-06\n",
      "training step: 426000\n",
      "loss:  6.44143756e-06\n",
      "training step: 427000\n",
      "loss:  6.40056214e-06\n",
      "training step: 428000\n",
      "loss:  6.3661837e-06\n",
      "training step: 429000\n",
      "loss:  6.34057551e-06\n",
      "training step: 430000\n",
      "loss:  6.32021147e-06\n",
      "training step: 431000\n",
      "loss:  6.28917132e-06\n",
      "training step: 432000\n",
      "loss:  6.26933297e-06\n",
      "training step: 433000\n",
      "loss:  6.22818516e-06\n",
      "training step: 434000\n",
      "loss:  6.20395e-06\n",
      "training step: 435000\n",
      "loss:  6.17961041e-06\n",
      "training step: 436000\n",
      "loss:  6.73985e-06\n",
      "training step: 437000\n",
      "loss:  6.13439124e-06\n",
      "training step: 438000\n",
      "loss:  6.11626137e-06\n",
      "training step: 439000\n",
      "loss:  6.08761911e-06\n",
      "training step: 440000\n",
      "loss:  6.05026435e-06\n",
      "training step: 441000\n",
      "loss:  6.03491435e-06\n",
      "training step: 442000\n",
      "loss:  6.00668e-06\n",
      "training step: 443000\n",
      "loss:  5.99231271e-06\n",
      "training step: 444000\n",
      "loss:  5.9676504e-06\n",
      "training step: 445000\n",
      "loss:  5.9520512e-06\n",
      "training step: 446000\n",
      "loss:  5.9234635e-06\n",
      "training step: 447000\n",
      "loss:  6.32527463e-06\n",
      "training step: 448000\n",
      "loss:  5.87809473e-06\n",
      "training step: 449000\n",
      "loss:  5.84691225e-06\n",
      "training step: 450000\n",
      "loss:  5.83429437e-06\n",
      "training step: 451000\n",
      "loss:  5.80748429e-06\n",
      "training step: 452000\n",
      "loss:  5.78856907e-06\n",
      "training step: 453000\n",
      "loss:  5.75646663e-06\n",
      "training step: 454000\n",
      "loss:  5.73941315e-06\n",
      "training step: 455000\n",
      "loss:  5.71666851e-06\n",
      "training step: 456000\n",
      "loss:  5.6923277e-06\n",
      "training step: 457000\n",
      "loss:  5.67443476e-06\n",
      "training step: 458000\n",
      "loss:  5.6459744e-06\n",
      "training step: 459000\n",
      "loss:  5.63567073e-06\n",
      "training step: 460000\n",
      "loss:  5.60953913e-06\n",
      "training step: 461000\n",
      "loss:  5.5823e-06\n",
      "training step: 462000\n",
      "loss:  5.5486621e-06\n",
      "training step: 463000\n",
      "loss:  5.53821974e-06\n",
      "training step: 464000\n",
      "loss:  5.58227885e-06\n",
      "training step: 465000\n",
      "loss:  5.50221239e-06\n",
      "training step: 466000\n",
      "loss:  5.49460765e-06\n",
      "training step: 467000\n",
      "loss:  5.47272066e-06\n",
      "training step: 468000\n",
      "loss:  5.45899229e-06\n",
      "training step: 469000\n",
      "loss:  5.42561156e-06\n",
      "training step: 470000\n",
      "loss:  5.3890808e-06\n",
      "training step: 471000\n",
      "loss:  5.3915187e-06\n",
      "training step: 472000\n",
      "loss:  5.37185861e-06\n",
      "training step: 473000\n",
      "loss:  5.36093967e-06\n",
      "training step: 474000\n",
      "loss:  5.32905096e-06\n",
      "training step: 475000\n",
      "loss:  5.31199839e-06\n",
      "training step: 476000\n",
      "loss:  5.28741293e-06\n",
      "training step: 477000\n",
      "loss:  5.28112969e-06\n",
      "training step: 478000\n",
      "loss:  5.25763471e-06\n",
      "training step: 479000\n",
      "loss:  5.25011819e-06\n",
      "training step: 480000\n",
      "loss:  5.23267772e-06\n",
      "training step: 481000\n",
      "loss:  5.22161918e-06\n",
      "training step: 482000\n",
      "loss:  5.19594187e-06\n",
      "training step: 483000\n",
      "loss:  5.18390971e-06\n",
      "training step: 484000\n",
      "loss:  5.1617958e-06\n",
      "training step: 485000\n",
      "loss:  0.000247081451\n",
      "training step: 486000\n",
      "loss:  5.12512815e-06\n",
      "training step: 487000\n",
      "loss:  5.10353129e-06\n",
      "training step: 488000\n",
      "loss:  5.09680603e-06\n",
      "training step: 489000\n",
      "loss:  5.0792587e-06\n",
      "training step: 490000\n",
      "loss:  5.06351535e-06\n",
      "training step: 491000\n",
      "loss:  5.03931051e-06\n",
      "training step: 492000\n",
      "loss:  5.03543197e-06\n",
      "training step: 493000\n",
      "loss:  5.01407612e-06\n",
      "training step: 494000\n",
      "loss:  4.9999976e-06\n",
      "training step: 495000\n",
      "loss:  4.9783539e-06\n",
      "training step: 496000\n",
      "loss:  4.96338271e-06\n",
      "training step: 497000\n",
      "loss:  4.94600863e-06\n",
      "training step: 498000\n",
      "loss:  4.92599702e-06\n",
      "training step: 499000\n",
      "loss:  4.91044602e-06\n",
      "training step: 500000\n",
      "loss:  4.89158538e-06\n",
      "training step: 501000\n",
      "loss:  4.87957959e-06\n",
      "training step: 502000\n",
      "loss:  4.85981309e-06\n",
      "training step: 503000\n",
      "loss:  4.84561815e-06\n",
      "training step: 504000\n",
      "loss:  4.8141419e-06\n",
      "training step: 505000\n",
      "loss:  4.81363e-06\n",
      "training step: 506000\n",
      "loss:  4.79435039e-06\n",
      "training step: 507000\n",
      "loss:  4.77882622e-06\n",
      "training step: 508000\n",
      "loss:  4.76149717e-06\n",
      "training step: 509000\n",
      "loss:  4.74340095e-06\n",
      "training step: 510000\n",
      "loss:  4.74074841e-06\n",
      "training step: 511000\n",
      "loss:  4.71346766e-06\n",
      "training step: 512000\n",
      "loss:  4.70548139e-06\n",
      "training step: 513000\n",
      "loss:  4.68729922e-06\n",
      "training step: 514000\n",
      "loss:  4.66033634e-06\n",
      "training step: 515000\n",
      "loss:  4.65851372e-06\n",
      "training step: 516000\n",
      "loss:  4.64035111e-06\n",
      "training step: 517000\n",
      "loss:  4.636825e-06\n",
      "training step: 518000\n",
      "loss:  4.61472064e-06\n",
      "training step: 519000\n",
      "loss:  4.60596584e-06\n",
      "training step: 520000\n",
      "loss:  4.58845534e-06\n",
      "training step: 521000\n",
      "loss:  4.57920214e-06\n",
      "training step: 522000\n",
      "loss:  4.56394946e-06\n",
      "training step: 523000\n",
      "loss:  4.54322253e-06\n",
      "training step: 524000\n",
      "loss:  4.52836548e-06\n",
      "training step: 525000\n",
      "loss:  4.51459209e-06\n",
      "training step: 526000\n",
      "loss:  4.50243897e-06\n",
      "training step: 527000\n",
      "loss:  4.48513674e-06\n",
      "training step: 528000\n",
      "loss:  4.48407764e-06\n",
      "training step: 529000\n",
      "loss:  4.46303738e-06\n",
      "training step: 530000\n",
      "loss:  0.00022525416\n",
      "training step: 531000\n",
      "loss:  4.43673616e-06\n",
      "training step: 532000\n",
      "loss:  4.41619204e-06\n",
      "training step: 533000\n",
      "loss:  4.41471275e-06\n",
      "training step: 534000\n",
      "loss:  4.3932996e-06\n",
      "training step: 535000\n",
      "loss:  4.38607049e-06\n",
      "training step: 536000\n",
      "loss:  4.36886739e-06\n",
      "training step: 537000\n",
      "loss:  4.36374694e-06\n",
      "training step: 538000\n",
      "loss:  4.34757e-06\n",
      "training step: 539000\n",
      "loss:  4.33990181e-06\n",
      "training step: 540000\n",
      "loss:  4.32354182e-06\n",
      "training step: 541000\n",
      "loss:  4.31824401e-06\n",
      "training step: 542000\n",
      "loss:  4.3028358e-06\n",
      "training step: 543000\n",
      "loss:  4.29506508e-06\n",
      "training step: 544000\n",
      "loss:  4.27289342e-06\n",
      "training step: 545000\n",
      "loss:  4.26469478e-06\n",
      "training step: 546000\n",
      "loss:  4.24864493e-06\n",
      "training step: 547000\n",
      "loss:  4.24181644e-06\n",
      "training step: 548000\n",
      "loss:  4.22019684e-06\n",
      "training step: 549000\n",
      "loss:  4.21126879e-06\n",
      "training step: 550000\n",
      "loss:  4.19772141e-06\n",
      "training step: 551000\n",
      "loss:  4.19073967e-06\n",
      "training step: 552000\n",
      "loss:  4.17358433e-06\n",
      "training step: 553000\n",
      "loss:  4.1869107e-06\n",
      "training step: 554000\n",
      "loss:  4.1559756e-06\n",
      "training step: 555000\n",
      "loss:  4.14699161e-06\n",
      "training step: 556000\n",
      "loss:  4.13022826e-06\n",
      "training step: 557000\n",
      "loss:  4.11078645e-06\n",
      "training step: 558000\n",
      "loss:  4.10938082e-06\n",
      "training step: 559000\n",
      "loss:  4.09100358e-06\n",
      "training step: 560000\n",
      "loss:  4.08124197e-06\n",
      "training step: 561000\n",
      "loss:  4.06944218e-06\n",
      "training step: 562000\n",
      "loss:  4.05476885e-06\n",
      "training step: 563000\n",
      "loss:  4.04699176e-06\n",
      "training step: 564000\n",
      "loss:  4.0325549e-06\n",
      "training step: 565000\n",
      "loss:  4.03424156e-06\n",
      "training step: 566000\n",
      "loss:  4.01805028e-06\n",
      "training step: 567000\n",
      "loss:  4.01084253e-06\n",
      "training step: 568000\n",
      "loss:  3.99674082e-06\n",
      "training step: 569000\n",
      "loss:  4.00077806e-06\n",
      "training step: 570000\n",
      "loss:  3.97578378e-06\n",
      "training step: 571000\n",
      "loss:  3.96095402e-06\n",
      "training step: 572000\n",
      "loss:  3.9510137e-06\n",
      "training step: 573000\n",
      "loss:  3.93752453e-06\n",
      "training step: 574000\n",
      "loss:  3.93689561e-06\n",
      "training step: 575000\n",
      "loss:  3.9232491e-06\n",
      "training step: 576000\n",
      "loss:  3.90807736e-06\n",
      "training step: 577000\n",
      "loss:  3.90433524e-06\n",
      "training step: 578000\n",
      "loss:  3.88974877e-06\n",
      "training step: 579000\n",
      "loss:  3.88646959e-06\n",
      "training step: 580000\n",
      "loss:  3.87145474e-06\n",
      "training step: 581000\n",
      "loss:  3.8618964e-06\n",
      "training step: 582000\n",
      "loss:  3.84840359e-06\n",
      "training step: 583000\n",
      "loss:  3.8456842e-06\n",
      "training step: 584000\n",
      "loss:  3.82982898e-06\n",
      "training step: 585000\n",
      "loss:  3.82883673e-06\n",
      "training step: 586000\n",
      "loss:  3.81143923e-06\n",
      "training step: 587000\n",
      "loss:  3.80258325e-06\n",
      "training step: 588000\n",
      "loss:  3.79026801e-06\n",
      "training step: 589000\n",
      "loss:  3.79124185e-06\n",
      "training step: 590000\n",
      "loss:  3.77069796e-06\n",
      "training step: 591000\n",
      "loss:  8.81182586e-06\n",
      "training step: 592000\n",
      "loss:  3.75172408e-06\n",
      "training step: 593000\n",
      "loss:  3.73810917e-06\n",
      "training step: 594000\n",
      "loss:  3.73619309e-06\n",
      "training step: 595000\n",
      "loss:  3.72439104e-06\n",
      "training step: 596000\n",
      "loss:  3.72320528e-06\n",
      "training step: 597000\n",
      "loss:  3.70388375e-06\n",
      "training step: 598000\n",
      "loss:  3.7007203e-06\n",
      "training step: 599000\n",
      "loss:  3.68830388e-06\n",
      "training step: 600000\n",
      "loss:  3.68527867e-06\n",
      "training step: 601000\n",
      "loss:  3.66821473e-06\n",
      "training step: 602000\n",
      "loss:  3.66755421e-06\n",
      "training step: 603000\n",
      "loss:  3.65123037e-06\n",
      "training step: 604000\n",
      "loss:  3.64715606e-06\n",
      "training step: 605000\n",
      "loss:  3.63869867e-06\n",
      "training step: 606000\n",
      "loss:  3.62753053e-06\n",
      "training step: 607000\n",
      "loss:  3.61721413e-06\n",
      "training step: 608000\n",
      "loss:  3.60556714e-06\n",
      "training step: 609000\n",
      "loss:  3.60425133e-06\n",
      "training step: 610000\n",
      "loss:  3.58816146e-06\n",
      "training step: 611000\n",
      "loss:  3.58060788e-06\n",
      "training step: 612000\n",
      "loss:  3.56918667e-06\n",
      "training step: 613000\n",
      "loss:  3.56504233e-06\n",
      "training step: 614000\n",
      "loss:  3.55119664e-06\n",
      "training step: 615000\n",
      "loss:  3.5502203e-06\n",
      "training step: 616000\n",
      "loss:  3.53846781e-06\n",
      "training step: 617000\n",
      "loss:  3.53358e-06\n",
      "training step: 618000\n",
      "loss:  3.52011716e-06\n",
      "training step: 619000\n",
      "loss:  3.51626318e-06\n",
      "training step: 620000\n",
      "loss:  3.5064113e-06\n",
      "training step: 621000\n",
      "loss:  3.49663287e-06\n",
      "training step: 622000\n",
      "loss:  3.48095887e-06\n",
      "training step: 623000\n",
      "loss:  3.47715513e-06\n",
      "training step: 624000\n",
      "loss:  3.46476213e-06\n",
      "training step: 625000\n",
      "loss:  3.47088962e-06\n",
      "training step: 626000\n",
      "loss:  3.45583e-06\n",
      "training step: 627000\n",
      "loss:  3.46160505e-06\n",
      "training step: 628000\n",
      "loss:  3.44588602e-06\n",
      "training step: 629000\n",
      "loss:  3.43808665e-06\n",
      "training step: 630000\n",
      "loss:  3.42432327e-06\n",
      "training step: 631000\n",
      "loss:  5.7405141e-06\n",
      "training step: 632000\n",
      "loss:  3.4124937e-06\n",
      "training step: 633000\n",
      "loss:  3.40315705e-06\n",
      "training step: 634000\n",
      "loss:  3.39337385e-06\n",
      "training step: 635000\n",
      "loss:  3.37717097e-06\n",
      "training step: 636000\n",
      "loss:  3.3772983e-06\n",
      "training step: 637000\n",
      "loss:  4.24346e-06\n",
      "training step: 638000\n",
      "loss:  3.35865e-06\n",
      "training step: 639000\n",
      "loss:  8.99223323e-06\n",
      "training step: 640000\n",
      "loss:  3.34359561e-06\n",
      "training step: 641000\n",
      "loss:  3.33478579e-06\n",
      "training step: 642000\n",
      "loss:  3.3284125e-06\n",
      "training step: 643000\n",
      "loss:  3.31372303e-06\n",
      "training step: 644000\n",
      "loss:  3.31484625e-06\n",
      "training step: 645000\n",
      "loss:  3.30205603e-06\n",
      "training step: 646000\n",
      "loss:  3.30030571e-06\n",
      "training step: 647000\n",
      "loss:  3.28695683e-06\n",
      "training step: 648000\n",
      "loss:  3.28105216e-06\n",
      "training step: 649000\n",
      "loss:  3.28810779e-06\n",
      "training step: 650000\n",
      "loss:  3.25992755e-06\n",
      "training step: 651000\n",
      "loss:  3.25674864e-06\n",
      "training step: 652000\n",
      "loss:  3.24529015e-06\n",
      "training step: 653000\n",
      "loss:  3.24149869e-06\n",
      "training step: 654000\n",
      "loss:  3.23070822e-06\n",
      "training step: 655000\n",
      "loss:  3.22620804e-06\n",
      "training step: 656000\n",
      "loss:  3.2140681e-06\n",
      "training step: 657000\n",
      "loss:  3.2063042e-06\n",
      "training step: 658000\n",
      "loss:  3.21360812e-06\n",
      "training step: 659000\n",
      "loss:  3.19461697e-06\n",
      "training step: 660000\n",
      "loss:  3.19404035e-06\n",
      "training step: 661000\n",
      "loss:  3.17834974e-06\n",
      "training step: 662000\n",
      "loss:  4.98394047e-06\n",
      "training step: 663000\n",
      "loss:  3.16846376e-06\n",
      "training step: 664000\n",
      "loss:  3.15557372e-06\n",
      "training step: 665000\n",
      "loss:  3.15192801e-06\n",
      "training step: 666000\n",
      "loss:  4.59569537e-06\n",
      "training step: 667000\n",
      "loss:  3.13850455e-06\n",
      "training step: 668000\n",
      "loss:  3.18740626e-06\n",
      "training step: 669000\n",
      "loss:  3.12173347e-06\n",
      "training step: 670000\n",
      "loss:  3.12341444e-06\n",
      "training step: 671000\n",
      "loss:  3.1125503e-06\n",
      "training step: 672000\n",
      "loss:  3.10559653e-06\n",
      "training step: 673000\n",
      "loss:  3.09535358e-06\n",
      "training step: 674000\n",
      "loss:  3.09554889e-06\n",
      "training step: 675000\n",
      "loss:  3.08423819e-06\n",
      "training step: 676000\n",
      "loss:  3.07709729e-06\n",
      "training step: 677000\n",
      "loss:  3.0715712e-06\n",
      "training step: 678000\n",
      "loss:  3.06765628e-06\n",
      "training step: 679000\n",
      "loss:  3.05983463e-06\n",
      "training step: 680000\n",
      "loss:  3.05565572e-06\n",
      "training step: 681000\n",
      "loss:  3.04505215e-06\n",
      "training step: 682000\n",
      "loss:  3.04315e-06\n",
      "training step: 683000\n",
      "loss:  8.24355302e-05\n",
      "training step: 684000\n",
      "loss:  3.02601143e-06\n",
      "training step: 685000\n",
      "loss:  3.04099785e-06\n",
      "training step: 686000\n",
      "loss:  3.01904606e-06\n",
      "training step: 687000\n",
      "loss:  3.00966735e-06\n",
      "training step: 688000\n",
      "loss:  3.00575857e-06\n",
      "training step: 689000\n",
      "loss:  2.995145e-06\n",
      "training step: 690000\n",
      "loss:  2.9922e-06\n",
      "training step: 691000\n",
      "loss:  2.98450072e-06\n",
      "training step: 692000\n",
      "loss:  2.9716557e-06\n",
      "training step: 693000\n",
      "loss:  2.96962889e-06\n",
      "training step: 694000\n",
      "loss:  2.96357916e-06\n",
      "training step: 695000\n",
      "loss:  2.95612108e-06\n",
      "training step: 696000\n",
      "loss:  2.94843312e-06\n",
      "training step: 697000\n",
      "loss:  2.94379311e-06\n",
      "training step: 698000\n",
      "loss:  2.93752851e-06\n",
      "training step: 699000\n",
      "loss:  2.93271069e-06\n",
      "training step: 700000\n",
      "loss:  2.92163145e-06\n",
      "training step: 701000\n",
      "loss:  2.92011214e-06\n",
      "training step: 702000\n",
      "loss:  2.91266588e-06\n",
      "training step: 703000\n",
      "loss:  2.90557182e-06\n",
      "training step: 704000\n",
      "loss:  2.89636455e-06\n",
      "training step: 705000\n",
      "loss:  2.89861578e-06\n",
      "training step: 706000\n",
      "loss:  2.88978504e-06\n",
      "training step: 707000\n",
      "loss:  2.89022751e-06\n",
      "training step: 708000\n",
      "loss:  2.8806935e-06\n",
      "training step: 709000\n",
      "loss:  2.87897274e-06\n",
      "training step: 710000\n",
      "loss:  2.86623822e-06\n",
      "training step: 711000\n",
      "loss:  2.87317334e-06\n",
      "training step: 712000\n",
      "loss:  2.85824513e-06\n",
      "training step: 713000\n",
      "loss:  2.84999396e-06\n",
      "training step: 714000\n",
      "loss:  2.84798443e-06\n",
      "training step: 715000\n",
      "loss:  2.84265434e-06\n",
      "training step: 716000\n",
      "loss:  2.83884515e-06\n",
      "training step: 717000\n",
      "loss:  2.82636938e-06\n",
      "training step: 718000\n",
      "loss:  2.83510349e-06\n",
      "training step: 719000\n",
      "loss:  2.82216729e-06\n",
      "training step: 720000\n",
      "loss:  2.81082498e-06\n",
      "training step: 721000\n",
      "loss:  2.80565405e-06\n",
      "training step: 722000\n",
      "loss:  2.80273548e-06\n",
      "training step: 723000\n",
      "loss:  2.7957758e-06\n",
      "training step: 724000\n",
      "loss:  2.79397136e-06\n",
      "training step: 725000\n",
      "loss:  2.78355856e-06\n",
      "training step: 726000\n",
      "loss:  2.78369612e-06\n",
      "training step: 727000\n",
      "loss:  2.77686104e-06\n",
      "training step: 728000\n",
      "loss:  2.77062418e-06\n",
      "training step: 729000\n",
      "loss:  2.76006199e-06\n",
      "training step: 730000\n",
      "loss:  2.76127503e-06\n",
      "training step: 731000\n",
      "loss:  2.75041339e-06\n",
      "training step: 732000\n",
      "loss:  2.74697777e-06\n",
      "training step: 733000\n",
      "loss:  2.73858745e-06\n",
      "training step: 734000\n",
      "loss:  2.72791249e-06\n",
      "training step: 735000\n",
      "loss:  2.74015656e-06\n",
      "training step: 736000\n",
      "loss:  2.72245052e-06\n",
      "training step: 737000\n",
      "loss:  2.7216372e-06\n",
      "training step: 738000\n",
      "loss:  2.71233148e-06\n",
      "training step: 739000\n",
      "loss:  2.70778264e-06\n",
      "training step: 740000\n",
      "loss:  2.70259738e-06\n",
      "training step: 741000\n",
      "loss:  2.69930547e-06\n",
      "training step: 742000\n",
      "loss:  2.69227485e-06\n",
      "training step: 743000\n",
      "loss:  2.71341946e-06\n",
      "training step: 744000\n",
      "loss:  2.68669828e-06\n",
      "training step: 745000\n",
      "loss:  2.67893301e-06\n",
      "training step: 746000\n",
      "loss:  2.69197426e-06\n",
      "training step: 747000\n",
      "loss:  2.66355096e-06\n",
      "training step: 748000\n",
      "loss:  2.66319262e-06\n",
      "training step: 749000\n",
      "loss:  2.65613858e-06\n",
      "training step: 750000\n",
      "loss:  2.65237372e-06\n",
      "training step: 751000\n",
      "loss:  2.64666232e-06\n",
      "training step: 752000\n",
      "loss:  2.64557411e-06\n",
      "training step: 753000\n",
      "loss:  2.6386e-06\n",
      "training step: 754000\n",
      "loss:  2.64052392e-06\n",
      "training step: 755000\n",
      "loss:  2.62944354e-06\n",
      "training step: 756000\n",
      "loss:  2.62962385e-06\n",
      "training step: 757000\n",
      "loss:  2.61807804e-06\n",
      "training step: 758000\n",
      "loss:  2.61310811e-06\n",
      "training step: 759000\n",
      "loss:  2.60655838e-06\n",
      "training step: 760000\n",
      "loss:  2.6044479e-06\n",
      "training step: 761000\n",
      "loss:  2.59675676e-06\n",
      "training step: 762000\n",
      "loss:  2.6041298e-06\n",
      "training step: 763000\n",
      "loss:  2.58943101e-06\n",
      "training step: 764000\n",
      "loss:  2.58739533e-06\n",
      "training step: 765000\n",
      "loss:  2.5773943e-06\n",
      "training step: 766000\n",
      "loss:  2.57466831e-06\n",
      "training step: 767000\n",
      "loss:  2.56502244e-06\n",
      "training step: 768000\n",
      "loss:  2.56343128e-06\n",
      "training step: 769000\n",
      "loss:  3.87983e-06\n",
      "training step: 770000\n",
      "loss:  2.5578338e-06\n",
      "training step: 771000\n",
      "loss:  2.55725786e-06\n",
      "training step: 772000\n",
      "loss:  2.54380734e-06\n",
      "training step: 773000\n",
      "loss:  2.54565634e-06\n",
      "training step: 774000\n",
      "loss:  2.53752455e-06\n",
      "training step: 775000\n",
      "loss:  2.53552844e-06\n",
      "training step: 776000\n",
      "loss:  2.52678615e-06\n",
      "training step: 777000\n",
      "loss:  2.58136197e-06\n",
      "training step: 778000\n",
      "loss:  2.52269501e-06\n",
      "training step: 779000\n",
      "loss:  2.51381971e-06\n",
      "training step: 780000\n",
      "loss:  2.51439747e-06\n",
      "training step: 781000\n",
      "loss:  2.50493417e-06\n",
      "training step: 782000\n",
      "loss:  2.50250355e-06\n",
      "training step: 783000\n",
      "loss:  2.49652066e-06\n",
      "training step: 784000\n",
      "loss:  2.49408572e-06\n",
      "training step: 785000\n",
      "loss:  2.48848096e-06\n",
      "training step: 786000\n",
      "loss:  2.48473862e-06\n",
      "training step: 787000\n",
      "loss:  2.47297749e-06\n",
      "training step: 788000\n",
      "loss:  2.47822732e-06\n",
      "training step: 789000\n",
      "loss:  2.46925606e-06\n",
      "training step: 790000\n",
      "loss:  3.25494534e-06\n",
      "training step: 791000\n",
      "loss:  2.46009472e-06\n",
      "training step: 792000\n",
      "loss:  2.46032096e-06\n",
      "training step: 793000\n",
      "loss:  2.45410092e-06\n",
      "training step: 794000\n",
      "loss:  2.62080607e-06\n",
      "training step: 795000\n",
      "loss:  2.44424086e-06\n",
      "training step: 796000\n",
      "loss:  2.44344506e-06\n",
      "training step: 797000\n",
      "loss:  2.43733507e-06\n",
      "training step: 798000\n",
      "loss:  8.201132e-05\n",
      "training step: 799000\n",
      "loss:  2.4278238e-06\n",
      "training step: 800000\n",
      "loss:  2.42829083e-06\n",
      "training step: 801000\n",
      "loss:  2.42277088e-06\n",
      "training step: 802000\n",
      "loss:  2.4124181e-06\n",
      "training step: 803000\n",
      "loss:  2.41082853e-06\n",
      "training step: 804000\n",
      "loss:  2.40716395e-06\n",
      "training step: 805000\n",
      "loss:  2.40675604e-06\n",
      "training step: 806000\n",
      "loss:  2.39797509e-06\n",
      "training step: 807000\n",
      "loss:  2.3988614e-06\n",
      "training step: 808000\n",
      "loss:  2.39127075e-06\n",
      "training step: 809000\n",
      "loss:  2.38816142e-06\n",
      "training step: 810000\n",
      "loss:  2.39041856e-06\n",
      "training step: 811000\n",
      "loss:  2.38135e-06\n",
      "training step: 812000\n",
      "loss:  2.37203813e-06\n",
      "training step: 813000\n",
      "loss:  2.37322797e-06\n",
      "training step: 814000\n",
      "loss:  2.36632945e-06\n",
      "training step: 815000\n",
      "loss:  2.36694609e-06\n",
      "training step: 816000\n",
      "loss:  2.36165465e-06\n",
      "training step: 817000\n",
      "loss:  2.36068172e-06\n",
      "training step: 818000\n",
      "loss:  2.3522573e-06\n",
      "training step: 819000\n",
      "loss:  2.35220023e-06\n",
      "training step: 820000\n",
      "loss:  2.34503636e-06\n",
      "training step: 821000\n",
      "loss:  2.34926119e-06\n",
      "training step: 822000\n",
      "loss:  2.34043227e-06\n",
      "training step: 823000\n",
      "loss:  2.33849937e-06\n",
      "training step: 824000\n",
      "loss:  2.33012202e-06\n",
      "training step: 825000\n",
      "loss:  2.32992465e-06\n",
      "training step: 826000\n",
      "loss:  2.32332582e-06\n",
      "training step: 827000\n",
      "loss:  2.32138655e-06\n",
      "training step: 828000\n",
      "loss:  2.31530885e-06\n",
      "training step: 829000\n",
      "loss:  2.36040114e-06\n",
      "training step: 830000\n",
      "loss:  2.31148124e-06\n",
      "training step: 831000\n",
      "loss:  2.31291756e-06\n",
      "training step: 832000\n",
      "loss:  2.29739567e-06\n",
      "training step: 833000\n",
      "loss:  7.010422e-06\n",
      "training step: 834000\n",
      "loss:  2.29349735e-06\n",
      "training step: 835000\n",
      "loss:  2.28716385e-06\n",
      "training step: 836000\n",
      "loss:  2.28719045e-06\n",
      "training step: 837000\n",
      "loss:  1.73711724e-05\n",
      "training step: 838000\n",
      "loss:  2.27814348e-06\n",
      "training step: 839000\n",
      "loss:  2.2746251e-06\n",
      "training step: 840000\n",
      "loss:  2.26863835e-06\n",
      "training step: 841000\n",
      "loss:  2.26804718e-06\n",
      "training step: 842000\n",
      "loss:  2.26203679e-06\n",
      "training step: 843000\n",
      "loss:  2.26916313e-06\n",
      "training step: 844000\n",
      "loss:  2.25491181e-06\n",
      "training step: 845000\n",
      "loss:  2.25832423e-06\n",
      "training step: 846000\n",
      "loss:  2.24582209e-06\n",
      "training step: 847000\n",
      "loss:  2.24584619e-06\n",
      "training step: 848000\n",
      "loss:  2.24148971e-06\n",
      "training step: 849000\n",
      "loss:  2.22901599e-06\n",
      "training step: 850000\n",
      "loss:  2.2365175e-06\n",
      "training step: 851000\n",
      "loss:  2.24169685e-06\n",
      "training step: 852000\n",
      "loss:  2.22639551e-06\n",
      "training step: 853000\n",
      "loss:  2.22289714e-06\n",
      "training step: 854000\n",
      "loss:  2.21702453e-06\n",
      "training step: 855000\n",
      "loss:  2.21902246e-06\n",
      "training step: 856000\n",
      "loss:  2.21181881e-06\n",
      "training step: 857000\n",
      "loss:  2.20889137e-06\n",
      "training step: 858000\n",
      "loss:  2.20657148e-06\n",
      "training step: 859000\n",
      "loss:  2.20286051e-06\n",
      "training step: 860000\n",
      "loss:  2.19553795e-06\n",
      "training step: 861000\n",
      "loss:  2.19937328e-06\n",
      "training step: 862000\n",
      "loss:  2.1904857e-06\n",
      "training step: 863000\n",
      "loss:  1.17339378e-05\n",
      "training step: 864000\n",
      "loss:  2.1852702e-06\n",
      "training step: 865000\n",
      "loss:  0.000228294797\n",
      "training step: 866000\n",
      "loss:  2.17654247e-06\n",
      "training step: 867000\n",
      "loss:  2.36966366e-06\n",
      "training step: 868000\n",
      "loss:  2.17197589e-06\n",
      "training step: 869000\n",
      "loss:  2.17120441e-06\n",
      "training step: 870000\n",
      "loss:  2.16144622e-06\n",
      "training step: 871000\n",
      "loss:  2.17662955e-06\n",
      "training step: 872000\n",
      "loss:  2.15922614e-06\n",
      "training step: 873000\n",
      "loss:  2.15204182e-06\n",
      "training step: 874000\n",
      "loss:  2.15372711e-06\n",
      "training step: 875000\n",
      "loss:  2.14942656e-06\n",
      "training step: 876000\n",
      "loss:  2.14443389e-06\n",
      "training step: 877000\n",
      "loss:  2.14028728e-06\n",
      "training step: 878000\n",
      "loss:  2.54388397e-06\n",
      "training step: 879000\n",
      "loss:  2.13111502e-06\n",
      "training step: 880000\n",
      "loss:  2.12920554e-06\n",
      "training step: 881000\n",
      "loss:  2.12175314e-06\n",
      "training step: 882000\n",
      "loss:  2.12730333e-06\n",
      "training step: 883000\n",
      "loss:  2.11724932e-06\n",
      "training step: 884000\n",
      "loss:  2.1175747e-06\n",
      "training step: 885000\n",
      "loss:  2.11011184e-06\n",
      "training step: 886000\n",
      "loss:  2.10871417e-06\n",
      "training step: 887000\n",
      "loss:  2.10228541e-06\n",
      "training step: 888000\n",
      "loss:  4.20137076e-06\n",
      "training step: 889000\n",
      "loss:  2.09719201e-06\n",
      "training step: 890000\n",
      "loss:  2.09260452e-06\n",
      "training step: 891000\n",
      "loss:  2.09137534e-06\n",
      "training step: 892000\n",
      "loss:  2.08333381e-06\n",
      "training step: 893000\n",
      "loss:  2.07818061e-06\n",
      "training step: 894000\n",
      "loss:  2.08306369e-06\n",
      "training step: 895000\n",
      "loss:  2.07491394e-06\n",
      "training step: 896000\n",
      "loss:  2.07205926e-06\n",
      "training step: 897000\n",
      "loss:  2.08043707e-06\n",
      "training step: 898000\n",
      "loss:  2.06813775e-06\n",
      "training step: 899000\n",
      "loss:  2.06635264e-06\n",
      "training step: 900000\n",
      "loss:  3.68513101e-05\n",
      "training step: 901000\n",
      "loss:  2.05544825e-06\n",
      "training step: 902000\n",
      "loss:  2.05107267e-06\n",
      "training step: 903000\n",
      "loss:  2.05268839e-06\n",
      "training step: 904000\n",
      "loss:  2.04599633e-06\n",
      "training step: 905000\n",
      "loss:  2.04699222e-06\n",
      "training step: 906000\n",
      "loss:  2.04176399e-06\n",
      "training step: 907000\n",
      "loss:  2.03951618e-06\n",
      "training step: 908000\n",
      "loss:  2.03392301e-06\n",
      "training step: 909000\n",
      "loss:  2.03247805e-06\n",
      "training step: 910000\n",
      "loss:  2.02726619e-06\n",
      "training step: 911000\n",
      "loss:  2.02440674e-06\n",
      "training step: 912000\n",
      "loss:  2.01838679e-06\n",
      "training step: 913000\n",
      "loss:  2.02000865e-06\n",
      "training step: 914000\n",
      "loss:  2.070954e-06\n",
      "training step: 915000\n",
      "loss:  2.01196917e-06\n",
      "training step: 916000\n",
      "loss:  2.01632133e-06\n",
      "training step: 917000\n",
      "loss:  2.00722616e-06\n",
      "training step: 918000\n",
      "loss:  2.00942213e-06\n",
      "training step: 919000\n",
      "loss:  2.37577456e-06\n",
      "training step: 920000\n",
      "loss:  1.99990131e-06\n",
      "training step: 921000\n",
      "loss:  1.99379565e-06\n",
      "training step: 922000\n",
      "loss:  1.99773808e-06\n",
      "training step: 923000\n",
      "loss:  1.99018882e-06\n",
      "training step: 924000\n",
      "loss:  2.50233552e-06\n",
      "training step: 925000\n",
      "loss:  1.9836782e-06\n",
      "training step: 926000\n",
      "loss:  1.9792742e-06\n",
      "training step: 927000\n",
      "loss:  1.97975123e-06\n",
      "training step: 928000\n",
      "loss:  1.97507529e-06\n",
      "training step: 929000\n",
      "loss:  1.97366035e-06\n",
      "training step: 930000\n",
      "loss:  1.96965857e-06\n",
      "training step: 931000\n",
      "loss:  1.9749782e-06\n",
      "training step: 932000\n",
      "loss:  1.9656502e-06\n",
      "training step: 933000\n",
      "loss:  1.96724272e-06\n",
      "training step: 934000\n",
      "loss:  1.96020665e-06\n",
      "training step: 935000\n",
      "loss:  1.95887628e-06\n",
      "training step: 936000\n",
      "loss:  1.95330927e-06\n",
      "training step: 937000\n",
      "loss:  1.95353709e-06\n",
      "training step: 938000\n",
      "loss:  1.94711447e-06\n",
      "training step: 939000\n",
      "loss:  1.94981226e-06\n",
      "training step: 940000\n",
      "loss:  1.94327413e-06\n",
      "training step: 941000\n",
      "loss:  1.93917094e-06\n",
      "training step: 942000\n",
      "loss:  1.94277823e-06\n",
      "training step: 943000\n",
      "loss:  1.93423625e-06\n",
      "training step: 944000\n",
      "loss:  1.92922744e-06\n",
      "training step: 945000\n",
      "loss:  1.93334199e-06\n",
      "training step: 946000\n",
      "loss:  1.92765492e-06\n",
      "training step: 947000\n",
      "loss:  1.92720245e-06\n",
      "training step: 948000\n",
      "loss:  1.92142579e-06\n",
      "training step: 949000\n",
      "loss:  1.92563266e-06\n",
      "training step: 950000\n",
      "loss:  1.91769323e-06\n",
      "training step: 951000\n",
      "loss:  1.9153706e-06\n",
      "training step: 952000\n",
      "loss:  1.91276308e-06\n",
      "training step: 953000\n",
      "loss:  1.9157817e-06\n",
      "training step: 954000\n",
      "loss:  1.90821561e-06\n",
      "training step: 955000\n",
      "loss:  1.90651747e-06\n",
      "training step: 956000\n",
      "loss:  1.33281028e-05\n",
      "training step: 957000\n",
      "loss:  1.89890557e-06\n",
      "training step: 958000\n",
      "loss:  1.90410969e-06\n",
      "training step: 959000\n",
      "loss:  1.89614036e-06\n",
      "training step: 960000\n",
      "loss:  1.89580589e-06\n",
      "training step: 961000\n",
      "loss:  1.88890419e-06\n",
      "training step: 962000\n",
      "loss:  1.88888339e-06\n",
      "training step: 963000\n",
      "loss:  1.8856598e-06\n",
      "training step: 964000\n",
      "loss:  1.88140541e-06\n",
      "training step: 965000\n",
      "loss:  1.88067918e-06\n",
      "training step: 966000\n",
      "loss:  1.87934688e-06\n",
      "training step: 967000\n",
      "loss:  1.87398462e-06\n",
      "training step: 968000\n",
      "loss:  1.8729188e-06\n",
      "training step: 969000\n",
      "loss:  1.86847751e-06\n",
      "training step: 970000\n",
      "loss:  1.86569287e-06\n",
      "training step: 971000\n",
      "loss:  1.86100726e-06\n",
      "training step: 972000\n",
      "loss:  1.86117165e-06\n",
      "training step: 973000\n",
      "loss:  1.86049249e-06\n",
      "training step: 974000\n",
      "loss:  1.8544639e-06\n",
      "training step: 975000\n",
      "loss:  1.85460533e-06\n",
      "training step: 976000\n",
      "loss:  1.84741168e-06\n",
      "training step: 977000\n",
      "loss:  1.85116676e-06\n",
      "training step: 978000\n",
      "loss:  1.84503654e-06\n",
      "training step: 979000\n",
      "loss:  1.84707824e-06\n",
      "training step: 980000\n",
      "loss:  1.84103374e-06\n",
      "training step: 981000\n",
      "loss:  1.83914199e-06\n",
      "training step: 982000\n",
      "loss:  1.83494024e-06\n",
      "training step: 983000\n",
      "loss:  1.83279246e-06\n",
      "training step: 984000\n",
      "loss:  1.8282966e-06\n",
      "training step: 985000\n",
      "loss:  1.83310055e-06\n",
      "training step: 986000\n",
      "loss:  2.06803838e-06\n",
      "training step: 987000\n",
      "loss:  1.82209033e-06\n",
      "training step: 988000\n",
      "loss:  1.81871394e-06\n",
      "training step: 989000\n",
      "loss:  1.8266395e-06\n",
      "training step: 990000\n",
      "loss:  1.81728262e-06\n",
      "training step: 991000\n",
      "loss:  1.81748499e-06\n",
      "training step: 992000\n",
      "loss:  1.81056646e-06\n",
      "training step: 993000\n",
      "loss:  1.81222993e-06\n",
      "training step: 994000\n",
      "loss:  1.80802613e-06\n",
      "training step: 995000\n",
      "loss:  1.81381563e-06\n",
      "training step: 996000\n",
      "loss:  1.80310133e-06\n",
      "training step: 997000\n",
      "loss:  1.79968254e-06\n",
      "training step: 998000\n",
      "loss:  1.79588756e-06\n",
      "training step: 999000\n",
      "loss:  1.79636459e-06\n",
      "training step: 1000000\n",
      "loss:  1.79664619e-06\n",
      "training step: 1001000\n",
      "loss:  1.78691516e-06\n",
      "training step: 1002000\n",
      "loss:  1.79144797e-06\n",
      "training step: 1003000\n",
      "loss:  1.78517928e-06\n",
      "training step: 1004000\n",
      "loss:  1.78526886e-06\n",
      "training step: 1005000\n",
      "loss:  1.78010532e-06\n",
      "training step: 1006000\n",
      "loss:  1.78196956e-06\n",
      "training step: 1007000\n",
      "loss:  1.77603931e-06\n",
      "training step: 1008000\n",
      "loss:  1.77609866e-06\n",
      "training step: 1009000\n",
      "loss:  3.46041634e-05\n",
      "training step: 1010000\n",
      "loss:  1.76951846e-06\n",
      "training step: 1011000\n",
      "loss:  1.76693595e-06\n",
      "training step: 1012000\n",
      "loss:  1.76018909e-06\n",
      "training step: 1013000\n",
      "loss:  1.76178798e-06\n",
      "training step: 1014000\n",
      "loss:  1.75762852e-06\n",
      "training step: 1015000\n",
      "loss:  1.75954642e-06\n",
      "training step: 1016000\n",
      "loss:  1.76550213e-06\n",
      "training step: 1017000\n",
      "loss:  1.75728769e-06\n",
      "training step: 1018000\n",
      "loss:  1.77089896e-06\n",
      "training step: 1019000\n",
      "loss:  1.74832519e-06\n",
      "training step: 1020000\n",
      "loss:  1.74945183e-06\n",
      "training step: 1021000\n",
      "loss:  1.74345655e-06\n",
      "training step: 1022000\n",
      "loss:  1.74394916e-06\n",
      "training step: 1023000\n",
      "loss:  1.73715978e-06\n",
      "training step: 1024000\n",
      "loss:  1.7384657e-06\n",
      "training step: 1025000\n",
      "loss:  1.73293961e-06\n",
      "training step: 1026000\n",
      "loss:  1.73379249e-06\n",
      "training step: 1027000\n",
      "loss:  1.73128888e-06\n",
      "training step: 1028000\n",
      "loss:  1.73368107e-06\n",
      "training step: 1029000\n",
      "loss:  1.7259465e-06\n",
      "training step: 1030000\n",
      "loss:  1.72573607e-06\n",
      "training step: 1031000\n",
      "loss:  1.71917497e-06\n",
      "training step: 1032000\n",
      "loss:  1.7245975e-06\n",
      "training step: 1033000\n",
      "loss:  1.7196528e-06\n",
      "training step: 1034000\n",
      "loss:  1.71841396e-06\n",
      "training step: 1035000\n",
      "loss:  1.71339445e-06\n",
      "training step: 1036000\n",
      "loss:  1.71439933e-06\n",
      "training step: 1037000\n",
      "loss:  1.70991382e-06\n",
      "training step: 1038000\n",
      "loss:  1.70626049e-06\n",
      "training step: 1039000\n",
      "loss:  1.70259602e-06\n",
      "training step: 1040000\n",
      "loss:  1.70812655e-06\n",
      "training step: 1041000\n",
      "loss:  1.70232943e-06\n",
      "training step: 1042000\n",
      "loss:  1.70182477e-06\n",
      "training step: 1043000\n",
      "loss:  1.69405087e-06\n",
      "training step: 1044000\n",
      "loss:  1.69152474e-06\n",
      "training step: 1045000\n",
      "loss:  1.69340535e-06\n",
      "training step: 1046000\n",
      "loss:  1.68783322e-06\n",
      "training step: 1047000\n",
      "loss:  1.68766837e-06\n",
      "training step: 1048000\n",
      "loss:  1.68216923e-06\n",
      "training step: 1049000\n",
      "loss:  1.68233191e-06\n",
      "training step: 1050000\n",
      "loss:  1.67662472e-06\n",
      "training step: 1051000\n",
      "loss:  1.68212091e-06\n",
      "training step: 1052000\n",
      "loss:  1.67514816e-06\n",
      "training step: 1053000\n",
      "loss:  1.67292308e-06\n",
      "training step: 1054000\n",
      "loss:  1.67664984e-06\n",
      "training step: 1055000\n",
      "loss:  1.66609732e-06\n",
      "training step: 1056000\n",
      "loss:  1.67043288e-06\n",
      "training step: 1057000\n",
      "loss:  1.66314e-06\n",
      "training step: 1058000\n",
      "loss:  1.67011603e-06\n",
      "training step: 1059000\n",
      "loss:  1.66221491e-06\n",
      "training step: 1060000\n",
      "loss:  1.65716176e-06\n",
      "training step: 1061000\n",
      "loss:  2.70577607e-06\n",
      "training step: 1062000\n",
      "loss:  1.65208178e-06\n",
      "training step: 1063000\n",
      "loss:  1.65553479e-06\n",
      "training step: 1064000\n",
      "loss:  1.64855771e-06\n",
      "training step: 1065000\n",
      "loss:  1.64856215e-06\n",
      "training step: 1066000\n",
      "loss:  1.64455264e-06\n",
      "training step: 1067000\n",
      "loss:  1.64316157e-06\n",
      "training step: 1068000\n",
      "loss:  1.64030519e-06\n",
      "training step: 1069000\n",
      "loss:  1.63641346e-06\n",
      "training step: 1070000\n",
      "loss:  1.6411143e-06\n",
      "training step: 1071000\n",
      "loss:  1.63463721e-06\n",
      "training step: 1072000\n",
      "loss:  1.63323159e-06\n",
      "training step: 1073000\n",
      "loss:  1.62868253e-06\n",
      "training step: 1074000\n",
      "loss:  1.62787433e-06\n",
      "training step: 1075000\n",
      "loss:  1.62708602e-06\n",
      "training step: 1076000\n",
      "loss:  1.62450021e-06\n",
      "training step: 1077000\n",
      "loss:  1.6233804e-06\n",
      "training step: 1078000\n",
      "loss:  1.61720618e-06\n",
      "training step: 1079000\n",
      "loss:  1.61781952e-06\n",
      "training step: 1080000\n",
      "loss:  1.61291155e-06\n",
      "training step: 1081000\n",
      "loss:  1.61430182e-06\n",
      "training step: 1082000\n",
      "loss:  1.60950879e-06\n",
      "training step: 1083000\n",
      "loss:  1.61248943e-06\n",
      "training step: 1084000\n",
      "loss:  1.6099699e-06\n",
      "training step: 1085000\n",
      "loss:  1.60346156e-06\n",
      "training step: 1086000\n",
      "loss:  1.60335424e-06\n",
      "training step: 1087000\n",
      "loss:  4.66966958e-06\n",
      "training step: 1088000\n",
      "loss:  1.6015581e-06\n",
      "training step: 1089000\n",
      "loss:  2.79403e-05\n",
      "training step: 1090000\n",
      "loss:  1.59730939e-06\n",
      "training step: 1091000\n",
      "loss:  1.60053798e-06\n",
      "training step: 1092000\n",
      "loss:  1.59013007e-06\n",
      "training step: 1093000\n",
      "loss:  1.59251761e-06\n",
      "training step: 1094000\n",
      "loss:  1.58799992e-06\n",
      "training step: 1095000\n",
      "loss:  1.58764692e-06\n",
      "training step: 1096000\n",
      "loss:  1.5850103e-06\n",
      "training step: 1097000\n",
      "loss:  1.58636396e-06\n",
      "training step: 1098000\n",
      "loss:  1.5906694e-06\n",
      "training step: 1099000\n",
      "loss:  1.57964109e-06\n",
      "training step: 1100000\n",
      "loss:  1.58134219e-06\n",
      "training step: 1101000\n",
      "loss:  1.5753335e-06\n",
      "training step: 1102000\n",
      "loss:  1.57416628e-06\n",
      "training step: 1103000\n",
      "loss:  1.57033446e-06\n",
      "training step: 1104000\n",
      "loss:  1.57180091e-06\n",
      "training step: 1105000\n",
      "loss:  1.56651993e-06\n",
      "training step: 1106000\n",
      "loss:  1.5680605e-06\n",
      "training step: 1107000\n",
      "loss:  1.56088436e-06\n",
      "training step: 1108000\n",
      "loss:  1.56308988e-06\n",
      "training step: 1109000\n",
      "loss:  1.56076646e-06\n",
      "training step: 1110000\n",
      "loss:  1.56412307e-06\n",
      "training step: 1111000\n",
      "loss:  1.55734961e-06\n",
      "training step: 1112000\n",
      "loss:  1.5600682e-06\n",
      "training step: 1113000\n",
      "loss:  1.55321027e-06\n",
      "training step: 1114000\n",
      "loss:  1.55516534e-06\n",
      "training step: 1115000\n",
      "loss:  1.55053897e-06\n",
      "training step: 1116000\n",
      "loss:  1.55839871e-06\n",
      "training step: 1117000\n",
      "loss:  1.54532654e-06\n",
      "training step: 1118000\n",
      "loss:  1.63770278e-06\n",
      "training step: 1119000\n",
      "loss:  1.54667885e-06\n",
      "training step: 1120000\n",
      "loss:  1.7941245e-06\n",
      "training step: 1121000\n",
      "loss:  1.54070256e-06\n",
      "training step: 1122000\n",
      "loss:  1.57622139e-06\n",
      "training step: 1123000\n",
      "loss:  1.53768804e-06\n",
      "training step: 1124000\n",
      "loss:  1.53070778e-06\n",
      "training step: 1125000\n",
      "loss:  1.53482927e-06\n",
      "training step: 1126000\n",
      "loss:  1.53115832e-06\n",
      "training step: 1127000\n",
      "loss:  1.52765301e-06\n",
      "training step: 1128000\n",
      "loss:  1.526441e-06\n",
      "training step: 1129000\n",
      "loss:  0.000523998751\n",
      "training step: 1130000\n",
      "loss:  1.52516145e-06\n",
      "training step: 1131000\n",
      "loss:  1.52717234e-06\n",
      "training step: 1132000\n",
      "loss:  1.51776646e-06\n",
      "training step: 1133000\n",
      "loss:  1.52119924e-06\n",
      "training step: 1134000\n",
      "loss:  1.51531719e-06\n",
      "training step: 1135000\n",
      "loss:  1.54817872e-06\n",
      "training step: 1136000\n",
      "loss:  1.51384654e-06\n",
      "training step: 1137000\n",
      "loss:  1.52158941e-06\n",
      "training step: 1138000\n",
      "loss:  1.51162419e-06\n",
      "training step: 1139000\n",
      "loss:  1.50804135e-06\n",
      "training step: 1140000\n",
      "loss:  1.50021924e-06\n",
      "training step: 1141000\n",
      "loss:  1.5074902e-06\n",
      "training step: 1142000\n",
      "loss:  2.6473881e-06\n",
      "training step: 1143000\n",
      "loss:  1.50281824e-06\n",
      "training step: 1144000\n",
      "loss:  1.49762775e-06\n",
      "training step: 1145000\n",
      "loss:  1.49757568e-06\n",
      "training step: 1146000\n",
      "loss:  1.49393475e-06\n",
      "training step: 1147000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 06:31:32.690112: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.49749019e-06\n",
      "training step: 1148000\n",
      "loss:  1.49140283e-06\n",
      "training step: 1149000\n",
      "loss:  1.49414984e-06\n",
      "training step: 1150000\n",
      "loss:  1.48980268e-06\n",
      "training step: 1151000\n",
      "loss:  1.48669653e-06\n",
      "training step: 1152000\n",
      "loss:  1.48069307e-06\n",
      "training step: 1153000\n",
      "loss:  1.48790627e-06\n",
      "training step: 1154000\n",
      "loss:  1.4820057e-06\n",
      "training step: 1155000\n",
      "loss:  1.48272034e-06\n",
      "training step: 1156000\n",
      "loss:  1.47807373e-06\n",
      "training step: 1157000\n",
      "loss:  1.52275368e-06\n",
      "training step: 1158000\n",
      "loss:  1.47668243e-06\n",
      "training step: 1159000\n",
      "loss:  1.47513117e-06\n",
      "training step: 1160000\n",
      "loss:  1.4691002e-06\n",
      "training step: 1161000\n",
      "loss:  1.46896707e-06\n",
      "training step: 1162000\n",
      "loss:  1.47975811e-06\n",
      "training step: 1163000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 06:37:13.182434: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.46573234e-06\n",
      "training step: 1164000\n",
      "loss:  1.46431444e-06\n",
      "training step: 1165000\n",
      "loss:  1.46230786e-06\n",
      "training step: 1166000\n",
      "loss:  1.46114235e-06\n",
      "training step: 1167000\n",
      "loss:  2.08759175e-05\n",
      "training step: 1168000\n",
      "loss:  1.46078435e-06\n",
      "training step: 1169000\n",
      "loss:  1.45729541e-06\n",
      "training step: 1170000\n",
      "loss:  1.4582663e-06\n",
      "training step: 1171000\n",
      "loss:  1.45114973e-06\n",
      "training step: 1172000\n",
      "loss:  1.45417152e-06\n",
      "training step: 1173000\n",
      "loss:  2.40811587e-05\n",
      "training step: 1174000\n",
      "loss:  1.44978503e-06\n",
      "training step: 1175000\n",
      "loss:  1.4466741e-06\n",
      "training step: 1176000\n",
      "loss:  1.4432776e-06\n",
      "training step: 1177000\n",
      "loss:  1.44636908e-06\n",
      "training step: 1178000\n",
      "loss:  1.44054275e-06\n",
      "training step: 1179000\n",
      "loss:  1.44211992e-06\n",
      "training step: 1180000\n",
      "loss:  1.43797877e-06\n",
      "training step: 1181000\n",
      "loss:  1.43985858e-06\n",
      "training step: 1182000\n",
      "loss:  1.4357845e-06\n",
      "training step: 1183000\n",
      "loss:  1.43509681e-06\n",
      "training step: 1184000\n",
      "loss:  1.44542582e-06\n",
      "training step: 1185000\n",
      "loss:  1.43114016e-06\n",
      "training step: 1186000\n",
      "loss:  1.42914666e-06\n",
      "training step: 1187000\n",
      "loss:  1.43062186e-06\n",
      "training step: 1188000\n",
      "loss:  1.42651129e-06\n",
      "training step: 1189000\n",
      "loss:  1.42908732e-06\n",
      "training step: 1190000\n",
      "loss:  1.42376302e-06\n",
      "training step: 1191000\n",
      "loss:  1.42173064e-06\n",
      "training step: 1192000\n",
      "loss:  1.42250076e-06\n",
      "training step: 1193000\n",
      "loss:  1.42011834e-06\n",
      "training step: 1194000\n",
      "loss:  1.41756175e-06\n",
      "training step: 1195000\n",
      "loss:  1.41907628e-06\n",
      "training step: 1196000\n",
      "loss:  1.41366922e-06\n",
      "training step: 1197000\n",
      "loss:  1.41566693e-06\n",
      "training step: 1198000\n",
      "loss:  1.41112048e-06\n",
      "training step: 1199000\n",
      "loss:  1.41037867e-06\n",
      "training step: 1200000\n",
      "loss:  1.41175894e-06\n",
      "training step: 1201000\n",
      "loss:  1.40868519e-06\n",
      "training step: 1202000\n",
      "loss:  1.40730026e-06\n",
      "training step: 1203000\n",
      "loss:  1.40477187e-06\n",
      "training step: 1204000\n",
      "loss:  1.41159342e-06\n",
      "training step: 1205000\n",
      "loss:  1.40247607e-06\n",
      "training step: 1206000\n",
      "loss:  1.40183442e-06\n",
      "training step: 1207000\n",
      "loss:  1.39947883e-06\n",
      "training step: 1208000\n",
      "loss:  1.39842166e-06\n",
      "training step: 1209000\n",
      "loss:  1.39397366e-06\n",
      "training step: 1210000\n",
      "loss:  1.39950191e-06\n",
      "training step: 1211000\n",
      "loss:  1.39344763e-06\n",
      "training step: 1212000\n",
      "loss:  1.39596705e-06\n",
      "training step: 1213000\n",
      "loss:  1.38662813e-06\n",
      "training step: 1214000\n",
      "loss:  1.3857757e-06\n",
      "training step: 1215000\n",
      "loss:  1.38624011e-06\n",
      "training step: 1216000\n",
      "loss:  1.38369592e-06\n",
      "training step: 1217000\n",
      "loss:  1.3849052e-06\n",
      "training step: 1218000\n",
      "loss:  1.38079599e-06\n",
      "training step: 1219000\n",
      "loss:  1.3812006e-06\n",
      "training step: 1220000\n",
      "loss:  1.37643053e-06\n",
      "training step: 1221000\n",
      "loss:  1.3791232e-06\n",
      "training step: 1222000\n",
      "loss:  1.37655843e-06\n",
      "training step: 1223000\n",
      "loss:  1.37769075e-06\n",
      "training step: 1224000\n",
      "loss:  1.37498296e-06\n",
      "training step: 1225000\n",
      "loss:  1.37645816e-06\n",
      "training step: 1226000\n",
      "loss:  1.3729973e-06\n",
      "training step: 1227000\n",
      "loss:  1.37805921e-06\n",
      "training step: 1228000\n",
      "loss:  1.37079212e-06\n",
      "training step: 1229000\n",
      "loss:  1.37047721e-06\n",
      "training step: 1230000\n",
      "loss:  1.3680359e-06\n",
      "training step: 1231000\n",
      "loss:  1.46858406e-06\n",
      "training step: 1232000\n",
      "loss:  1.36395067e-06\n",
      "training step: 1233000\n",
      "loss:  1.36668461e-06\n",
      "training step: 1234000\n",
      "loss:  1.36285462e-06\n",
      "training step: 1235000\n",
      "loss:  1.36061476e-06\n",
      "training step: 1236000\n",
      "loss:  1.35592131e-06\n",
      "training step: 1237000\n",
      "loss:  1.36209883e-06\n",
      "training step: 1238000\n",
      "loss:  1.35529774e-06\n",
      "training step: 1239000\n",
      "loss:  1.35385562e-06\n",
      "training step: 1240000\n",
      "loss:  1.34951517e-06\n",
      "training step: 1241000\n",
      "loss:  1.35306334e-06\n",
      "training step: 1242000\n",
      "loss:  1.35000391e-06\n",
      "training step: 1243000\n",
      "loss:  1.3516468e-06\n",
      "training step: 1244000\n",
      "loss:  1.34720369e-06\n",
      "training step: 1245000\n",
      "loss:  1.35352968e-06\n",
      "training step: 1246000\n",
      "loss:  1.34690265e-06\n",
      "training step: 1247000\n",
      "loss:  1.34347738e-06\n",
      "training step: 1248000\n",
      "loss:  1.34223205e-06\n",
      "training step: 1249000\n",
      "loss:  1.34819584e-06\n",
      "training step: 1250000\n",
      "loss:  1.34019672e-06\n",
      "training step: 1251000\n",
      "loss:  1.34116613e-06\n",
      "training step: 1252000\n",
      "loss:  1.33738263e-06\n",
      "training step: 1253000\n",
      "loss:  1.33986759e-06\n",
      "training step: 1254000\n",
      "loss:  1.33393814e-06\n",
      "training step: 1255000\n",
      "loss:  1.33117919e-06\n",
      "training step: 1256000\n",
      "loss:  1.32841581e-06\n",
      "training step: 1257000\n",
      "loss:  1.32860407e-06\n",
      "training step: 1258000\n",
      "loss:  1.33104515e-06\n",
      "training step: 1259000\n",
      "loss:  1.32792832e-06\n",
      "training step: 1260000\n",
      "loss:  1.32995797e-06\n",
      "training step: 1261000\n",
      "loss:  1.32560899e-06\n",
      "training step: 1262000\n",
      "loss:  1.32053731e-06\n",
      "training step: 1263000\n",
      "loss:  1.3224153e-06\n",
      "training step: 1264000\n",
      "loss:  1.32292712e-06\n",
      "training step: 1265000\n",
      "loss:  1.31988884e-06\n",
      "training step: 1266000\n",
      "loss:  1.31777085e-06\n",
      "training step: 1267000\n",
      "loss:  1.3152503e-06\n",
      "training step: 1268000\n",
      "loss:  1.31683305e-06\n",
      "training step: 1269000\n",
      "loss:  1.31359957e-06\n",
      "training step: 1270000\n",
      "loss:  1.31222396e-06\n",
      "training step: 1271000\n",
      "loss:  1.30895944e-06\n",
      "training step: 1272000\n",
      "loss:  1.31256581e-06\n",
      "training step: 1273000\n",
      "loss:  1.30929118e-06\n",
      "training step: 1274000\n",
      "loss:  1.30581486e-06\n",
      "training step: 1275000\n",
      "loss:  1.36347728e-06\n",
      "training step: 1276000\n",
      "loss:  1.30363412e-06\n",
      "training step: 1277000\n",
      "loss:  1.31553827e-06\n",
      "training step: 1278000\n",
      "loss:  1.30122044e-06\n",
      "training step: 1279000\n",
      "loss:  5.83542678e-05\n",
      "training step: 1280000\n",
      "loss:  1.29962245e-06\n",
      "training step: 1281000\n",
      "loss:  0.000536311883\n",
      "training step: 1282000\n",
      "loss:  1.29626403e-06\n",
      "training step: 1283000\n",
      "loss:  1.2954597e-06\n",
      "training step: 1284000\n",
      "loss:  1.29229181e-06\n",
      "training step: 1285000\n",
      "loss:  1.29068383e-06\n",
      "training step: 1286000\n",
      "loss:  1.28954912e-06\n",
      "training step: 1287000\n",
      "loss:  1.2877199e-06\n",
      "training step: 1288000\n",
      "loss:  1.28911302e-06\n",
      "training step: 1289000\n",
      "loss:  1.28375814e-06\n",
      "training step: 1290000\n",
      "loss:  1.28484896e-06\n",
      "training step: 1291000\n",
      "loss:  1.2813282e-06\n",
      "training step: 1292000\n",
      "loss:  1.2818183e-06\n",
      "training step: 1293000\n",
      "loss:  1.27938733e-06\n",
      "training step: 1294000\n",
      "loss:  1.28039221e-06\n",
      "training step: 1295000\n",
      "loss:  1.27474391e-06\n",
      "training step: 1296000\n",
      "loss:  1.27632006e-06\n",
      "training step: 1297000\n",
      "loss:  1.27309841e-06\n",
      "training step: 1298000\n",
      "loss:  1.27621922e-06\n",
      "training step: 1299000\n",
      "loss:  1.26932821e-06\n",
      "training step: 1300000\n",
      "loss:  1.26848522e-06\n",
      "training step: 1301000\n",
      "loss:  1.27033036e-06\n",
      "training step: 1302000\n",
      "loss:  1.26613372e-06\n",
      "training step: 1303000\n",
      "loss:  1.26727753e-06\n",
      "training step: 1304000\n",
      "loss:  1.26272732e-06\n",
      "training step: 1305000\n",
      "loss:  1.26641521e-06\n",
      "training step: 1306000\n",
      "loss:  1.26287807e-06\n",
      "training step: 1307000\n",
      "loss:  1.26296936e-06\n",
      "training step: 1308000\n",
      "loss:  1.25912015e-06\n",
      "training step: 1309000\n",
      "loss:  1.26157659e-06\n",
      "training step: 1310000\n",
      "loss:  1.25778183e-06\n",
      "training step: 1311000\n",
      "loss:  1.2567458e-06\n",
      "training step: 1312000\n",
      "loss:  1.25446377e-06\n",
      "training step: 1313000\n",
      "loss:  1.25700501e-06\n",
      "training step: 1314000\n",
      "loss:  1.25288284e-06\n",
      "training step: 1315000\n",
      "loss:  1.25331428e-06\n",
      "training step: 1316000\n",
      "loss:  1.24929181e-06\n",
      "training step: 1317000\n",
      "loss:  1.25213069e-06\n",
      "training step: 1318000\n",
      "loss:  1.24914777e-06\n",
      "training step: 1319000\n",
      "loss:  1.2445862e-06\n",
      "training step: 1320000\n",
      "loss:  1.32619527e-06\n",
      "training step: 1321000\n",
      "loss:  1.24176279e-06\n",
      "training step: 1322000\n",
      "loss:  1.2425852e-06\n",
      "training step: 1323000\n",
      "loss:  1.24039934e-06\n",
      "training step: 1324000\n",
      "loss:  1.24051292e-06\n",
      "training step: 1325000\n",
      "loss:  5.00197166e-06\n",
      "training step: 1326000\n",
      "loss:  1.23804705e-06\n",
      "training step: 1327000\n",
      "loss:  1.24080384e-06\n",
      "training step: 1328000\n",
      "loss:  1.23566849e-06\n",
      "training step: 1329000\n",
      "loss:  1.24047472e-06\n",
      "training step: 1330000\n",
      "loss:  1.2308293e-06\n",
      "training step: 1331000\n",
      "loss:  1.23292352e-06\n",
      "training step: 1332000\n",
      "loss:  1.22901963e-06\n",
      "training step: 1333000\n",
      "loss:  1.22944391e-06\n",
      "training step: 1334000\n",
      "loss:  1.22508368e-06\n",
      "training step: 1335000\n",
      "loss:  1.23078155e-06\n",
      "training step: 1336000\n",
      "loss:  1.22671918e-06\n",
      "training step: 1337000\n",
      "loss:  1.2233055e-06\n",
      "training step: 1338000\n",
      "loss:  1.22310678e-06\n",
      "training step: 1339000\n",
      "loss:  1.22366509e-06\n",
      "training step: 1340000\n",
      "loss:  1.22074266e-06\n",
      "training step: 1341000\n",
      "loss:  1.22959534e-06\n",
      "training step: 1342000\n",
      "loss:  1.22008009e-06\n",
      "training step: 1343000\n",
      "loss:  1.21864662e-06\n",
      "training step: 1344000\n",
      "loss:  1.21617416e-06\n",
      "training step: 1345000\n",
      "loss:  1.21591074e-06\n",
      "training step: 1346000\n",
      "loss:  1.21514245e-06\n",
      "training step: 1347000\n",
      "loss:  1.21399614e-06\n",
      "training step: 1348000\n",
      "loss:  1.21206119e-06\n",
      "training step: 1349000\n",
      "loss:  1.2094589e-06\n",
      "training step: 1350000\n",
      "loss:  1.20864911e-06\n",
      "training step: 1351000\n",
      "loss:  1.20898653e-06\n",
      "training step: 1352000\n",
      "loss:  1.20739753e-06\n",
      "training step: 1353000\n",
      "loss:  1.20932611e-06\n",
      "training step: 1354000\n",
      "loss:  1.20628545e-06\n",
      "training step: 1355000\n",
      "loss:  1.20472089e-06\n",
      "training step: 1356000\n",
      "loss:  1.20380059e-06\n",
      "training step: 1357000\n",
      "loss:  1.20550783e-06\n",
      "training step: 1358000\n",
      "loss:  1.19942206e-06\n",
      "training step: 1359000\n",
      "loss:  1.20105824e-06\n",
      "training step: 1360000\n",
      "loss:  1.21062089e-06\n",
      "training step: 1361000\n",
      "loss:  1.19812512e-06\n",
      "training step: 1362000\n",
      "loss:  1.19801973e-06\n",
      "training step: 1363000\n",
      "loss:  1.19399488e-06\n",
      "training step: 1364000\n",
      "loss:  1.19972242e-06\n",
      "training step: 1365000\n",
      "loss:  1.19317031e-06\n",
      "training step: 1366000\n",
      "loss:  1.19079118e-06\n",
      "training step: 1367000\n",
      "loss:  1.18955359e-06\n",
      "training step: 1368000\n",
      "loss:  1.18705589e-06\n",
      "training step: 1369000\n",
      "loss:  1.18808578e-06\n",
      "training step: 1370000\n",
      "loss:  1.18798778e-06\n",
      "training step: 1371000\n",
      "loss:  1.18603805e-06\n",
      "training step: 1372000\n",
      "loss:  1.18432888e-06\n",
      "training step: 1373000\n",
      "loss:  0.00023320668\n",
      "training step: 1374000\n",
      "loss:  1.18429227e-06\n",
      "training step: 1375000\n",
      "loss:  1.18103992e-06\n",
      "training step: 1376000\n",
      "loss:  1.40196892e-06\n",
      "training step: 1377000\n",
      "loss:  1.17817615e-06\n",
      "training step: 1378000\n",
      "loss:  1.1801684e-06\n",
      "training step: 1379000\n",
      "loss:  1.17490674e-06\n",
      "training step: 1380000\n",
      "loss:  1.17734214e-06\n",
      "training step: 1381000\n",
      "loss:  1.17379102e-06\n",
      "training step: 1382000\n",
      "loss:  1.17503271e-06\n",
      "training step: 1383000\n",
      "loss:  1.17194156e-06\n",
      "training step: 1384000\n",
      "loss:  1.1726047e-06\n",
      "training step: 1385000\n",
      "loss:  1.16839192e-06\n",
      "training step: 1386000\n",
      "loss:  1.17532829e-06\n",
      "training step: 1387000\n",
      "loss:  1.16932904e-06\n",
      "training step: 1388000\n",
      "loss:  1.17226762e-06\n",
      "training step: 1389000\n",
      "loss:  1.16667968e-06\n",
      "training step: 1390000\n",
      "loss:  1.16400429e-06\n",
      "training step: 1391000\n",
      "loss:  1.16442902e-06\n",
      "training step: 1392000\n",
      "loss:  1.16522085e-06\n",
      "training step: 1393000\n",
      "loss:  1.16195622e-06\n",
      "training step: 1394000\n",
      "loss:  1.15724424e-06\n",
      "training step: 1395000\n",
      "loss:  1.39548467e-06\n",
      "training step: 1396000\n",
      "loss:  1.15424211e-06\n",
      "training step: 1397000\n",
      "loss:  1.15708531e-06\n",
      "training step: 1398000\n",
      "loss:  1.1550236e-06\n",
      "training step: 1399000\n",
      "loss:  1.15543548e-06\n",
      "training step: 1400000\n",
      "loss:  1.15224395e-06\n",
      "training step: 1401000\n",
      "loss:  1.15112914e-06\n",
      "training step: 1402000\n",
      "loss:  1.1508215e-06\n",
      "training step: 1403000\n",
      "loss:  1.14842169e-06\n",
      "training step: 1404000\n",
      "loss:  1.15102603e-06\n",
      "training step: 1405000\n",
      "loss:  1.15490604e-06\n",
      "training step: 1406000\n",
      "loss:  1.34612333e-06\n",
      "training step: 1407000\n",
      "loss:  1.14360148e-06\n",
      "training step: 1408000\n",
      "loss:  1.14513773e-06\n",
      "training step: 1409000\n",
      "loss:  1.14347063e-06\n",
      "training step: 1410000\n",
      "loss:  1.14081092e-06\n",
      "training step: 1411000\n",
      "loss:  1.1428134e-06\n",
      "training step: 1412000\n",
      "loss:  1.15315333e-06\n",
      "training step: 1413000\n",
      "loss:  1.13822807e-06\n",
      "training step: 1414000\n",
      "loss:  1.14097338e-06\n",
      "training step: 1415000\n",
      "loss:  1.13467945e-06\n",
      "training step: 1416000\n",
      "loss:  1.13741805e-06\n",
      "training step: 1417000\n",
      "loss:  1.13505655e-06\n",
      "training step: 1418000\n",
      "loss:  1.13222575e-06\n",
      "training step: 1419000\n",
      "loss:  1.13192925e-06\n",
      "training step: 1420000\n",
      "loss:  1.13103965e-06\n",
      "training step: 1421000\n",
      "loss:  1.13070678e-06\n",
      "training step: 1422000\n",
      "loss:  1.12654573e-06\n",
      "training step: 1423000\n",
      "loss:  1.12903967e-06\n",
      "training step: 1424000\n",
      "loss:  1.12540522e-06\n",
      "training step: 1425000\n",
      "loss:  1.12464761e-06\n",
      "training step: 1426000\n",
      "loss:  1.12400812e-06\n",
      "training step: 1427000\n",
      "loss:  1.1208349e-06\n",
      "training step: 1428000\n",
      "loss:  1.12199484e-06\n",
      "training step: 1429000\n",
      "loss:  1.12493797e-06\n",
      "training step: 1430000\n",
      "loss:  1.12085888e-06\n",
      "training step: 1431000\n",
      "loss:  1.1198731e-06\n",
      "training step: 1432000\n",
      "loss:  1.11739644e-06\n",
      "training step: 1433000\n",
      "loss:  1.1211074e-06\n",
      "training step: 1434000\n",
      "loss:  1.11653117e-06\n",
      "training step: 1435000\n",
      "loss:  1.11655686e-06\n",
      "training step: 1436000\n",
      "loss:  1.11584e-06\n",
      "training step: 1437000\n",
      "loss:  1.11016777e-06\n",
      "training step: 1438000\n",
      "loss:  1.10970359e-06\n",
      "training step: 1439000\n",
      "loss:  1.11672387e-06\n",
      "training step: 1440000\n",
      "loss:  1.11279712e-06\n",
      "training step: 1441000\n",
      "loss:  1.11012662e-06\n",
      "training step: 1442000\n",
      "loss:  1.10668543e-06\n",
      "training step: 1443000\n",
      "loss:  1.10613405e-06\n",
      "training step: 1444000\n",
      "loss:  1.1054924e-06\n",
      "training step: 1445000\n",
      "loss:  1.10389681e-06\n",
      "training step: 1446000\n",
      "loss:  1.10454846e-06\n",
      "training step: 1447000\n",
      "loss:  1.85167414e-06\n",
      "training step: 1448000\n",
      "loss:  1.10026917e-06\n",
      "training step: 1449000\n",
      "loss:  1.10129577e-06\n",
      "training step: 1450000\n",
      "loss:  1.0993042e-06\n",
      "training step: 1451000\n",
      "loss:  5.51806761e-06\n",
      "training step: 1452000\n",
      "loss:  1.09668611e-06\n",
      "training step: 1453000\n",
      "loss:  1.09599603e-06\n",
      "training step: 1454000\n",
      "loss:  1.09269251e-06\n",
      "training step: 1455000\n",
      "loss:  1.09576433e-06\n",
      "training step: 1456000\n",
      "loss:  2.12996474e-06\n",
      "training step: 1457000\n",
      "loss:  1.0926932e-06\n",
      "training step: 1458000\n",
      "loss:  1.09089251e-06\n",
      "training step: 1459000\n",
      "loss:  1.1714269e-06\n",
      "training step: 1460000\n",
      "loss:  1.09028269e-06\n",
      "training step: 1461000\n",
      "loss:  1.08994595e-06\n",
      "training step: 1462000\n",
      "loss:  1.08732729e-06\n",
      "training step: 1463000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29788/2301405257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m 'seed':None, 'debug_traj': True}\n\u001b[1;32m      4\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_homo_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_c_homo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/e/work/ML_MO_COUPLE/nn_frame.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                         \u001b[0mis_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;31m# save model every selected steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "setting = {'activation':'tanh', 'nn_shape':(256,256,256), 'batch_size':1681, 'training_steps':50000000,\\\n",
    "'learning_rate': 0.000001, 'decay_rate':0.95, 'decay_per_steps':1000, 'save_step':1000, 'drop_rate':0, 'save_path':'./save',\\\n",
    "'seed':None, 'debug_traj': True}\n",
    "NN = nn.NN(setting_dict=setting)\n",
    "NN.train(train_homo_pairs,train_c_homo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143ab74-322f-4b26-9ee5-7f7d433b2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "print(np.mean((NN.model(train_homo_pairs, training=False)-c_homo)/c_homo))\n",
    "x = np.linspace(0, 4, 41)\n",
    "y = np.linspace(0, 4, 41)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = NN.model(train_homo_pairs, training=False).numpy().reshape((41,41))\n",
    "Z1 = c_homo.reshape((41,41))\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "ax.contourf(x,y, np.exp(-Z))\n",
    "ax.set_title('pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeac264-1b61-4a8a-bb30-b368f5551f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c3106-5eca-4313-b014-b33b23c558f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homo[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ace6c2-030a-4f0d-aef2-2440da10b435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
